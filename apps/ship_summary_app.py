# ship_summary_app.py â€”â€” BCF å‘è´§æ±‡æ€»ï¼ˆæŒ‰ä»“åº“ï¼Œå«ä¸¤é¡µç­¾ï¼‰
import streamlit as st
import pandas as pd
import gspread
from google.oauth2.service_account import Credentials
from gspread.exceptions import SpreadsheetNotFound
from datetime import datetime, timedelta

# ====== é…ç½® ======
SCOPES = ["https://www.googleapis.com/auth/spreadsheets", "https://www.googleapis.com/auth/drive"]
SHEET_ARRIVALS_NAME = "åˆ°ä»“æ•°æ®è¡¨"      # éœ€å«ï¼šè¿å•å· / ä»“åº“ä»£ç  / ç®±æ•° / ä½“ç§¯ / æ”¶è´¹é‡
SHEET_SHIP_DETAIL   = "bolè‡ªææ˜ç»†"     # éœ€å«ï¼šè¿å•å· / åˆ†æ‘Šè´¹ç”¨(æˆ–æè´§è´¹ç”¨) / ETA(åˆ°BCF)
SHEET_WB_SUMMARY_NAME = "è¿å•å…¨é“¾è·¯æ±‡æ€»"  # â˜… å°†æ­¤æ”¹æˆä½ çš„æ€»è¡¨åï¼ˆTab2ä½¿ç”¨ï¼‰

# ====== æˆæƒ ======
def get_gspread_client():
    if "gcp_service_account" in st.secrets:
        sa_info = st.secrets["gcp_service_account"]
        creds = Credentials.from_service_account_info(sa_info, scopes=SCOPES)
    else:
        creds = Credentials.from_service_account_file("service_accounts.json", scopes=SCOPES)
    return gspread.authorize(creds)

client = get_gspread_client()

# ====== å·¥å…· ======
def _norm_cols(cols):
    return [c.replace("\u00A0", "").replace("\n", "").strip() for c in cols]

def _to_num(s):
    return pd.to_numeric(s, errors="coerce")

def _parse_date_series(s):
    """æ”¯æŒ Excel åºåˆ—å·æˆ–å¸¸è§æ—¥æœŸå­—ç¬¦ä¸² -> pandas.Timestamp"""
    def excel_serial_to_dt(v):
        try:
            f = float(v)
            return datetime(1899, 12, 30) + timedelta(days=f)
        except Exception:
            return pd.NaT
    s1 = s.apply(excel_serial_to_dt)
    s2 = pd.to_datetime(s, errors="coerce")
    return s1.combine_first(s2)

def _pick(cols, candidates):
    for c in candidates:
        if c in cols:
            return c
    return None

# ====== è¯»å–ï¼šåˆ°ä»“æ•°æ®è¡¨ ======
@st.cache_data(ttl=60)
def load_arrivals_df():
    ws = client.open(SHEET_ARRIVALS_NAME).sheet1
    data = ws.get_all_values(value_render_option="UNFORMATTED_VALUE", date_time_render_option="SERIAL_NUMBER")
    if not data:
        return pd.DataFrame()
    header = _norm_cols(data[0])
    df = pd.DataFrame(data[1:], columns=header)

    col_wb  = _pick(df.columns, ["è¿å•å·","Waybill","å•å·"])
    col_wh  = _pick(df.columns, ["ä»“åº“ä»£ç ","ä»“åº“","åº“"])
    col_box = _pick(df.columns, ["ç®±æ•°","ç®±å­æ•°é‡","ç®±å­æ•°"])
    col_cbm = _pick(df.columns, ["ä½“ç§¯","CBM","ä½“ç§¯CBM"])
    col_wt  = _pick(df.columns, ["æ”¶è´¹é‡","è®¡è´¹é‡","é‡é‡","æ”¶è´¹é‡KG","è®¡è´¹é‡KG"])

    if col_wb is None: df["è¿å•å·"] = pd.NA; col_wb = "è¿å•å·"
    if col_wh is None: df["ä»“åº“ä»£ç "] = pd.NA; col_wh = "ä»“åº“ä»£ç "
    if col_box is None: df["ç®±æ•°"] = pd.NA; col_box = "ç®±æ•°"
    if col_cbm is None: df["ä½“ç§¯"] = pd.NA; col_cbm = "ä½“ç§¯"
    if col_wt is None: df["æ”¶è´¹é‡"] = pd.NA; col_wt = "æ”¶è´¹é‡"

    df[col_box] = _to_num(df[col_box])
    df[col_cbm] = _to_num(df[col_cbm])
    df[col_wt]  = _to_num(df[col_wt])
    df[col_wb] = df[col_wb].astype(str).str.strip()

    df = df.drop_duplicates(subset=[col_wb])
    return df[[col_wb,col_wh,col_box,col_cbm,col_wt]].rename(columns={
        col_wb:"è¿å•å·", col_wh:"ä»“åº“ä»£ç ", col_box:"ç®±æ•°", col_cbm:"ä½“ç§¯", col_wt:"æ”¶è´¹é‡"
    })

# ====== è¯»å–ï¼šbolè‡ªææ˜ç»†ï¼ˆæè´§è´¹ç”¨ & æè´§æ—¥æœŸï¼‰ ======
@st.cache_data(ttl=60)
def load_ship_detail_df():
    try:
        ws = client.open(SHEET_SHIP_DETAIL).sheet1
    except SpreadsheetNotFound:
        return pd.DataFrame()
    data = ws.get_all_values(value_render_option="UNFORMATTED_VALUE", date_time_render_option="SERIAL_NUMBER")
    if not data:
        return pd.DataFrame()
    header = _norm_cols(data[0])
    df = pd.DataFrame(data[1:], columns=header)

    # åˆ—åå…¼å®¹
    if "è¿å•å·" not in df.columns: df["è¿å•å·"] = pd.NA
    fee_col = "åˆ†æ‘Šè´¹ç”¨" if "åˆ†æ‘Šè´¹ç”¨" in df.columns else ("æè´§è´¹ç”¨" if "æè´§è´¹ç”¨" in df.columns else None)
    if fee_col is None:
        df["åˆ†æ‘Šè´¹ç”¨"] = pd.NA
        fee_col = "åˆ†æ‘Šè´¹ç”¨"
    if "ETA(åˆ°BCF)" not in df.columns:
        df["ETA(åˆ°BCF)"] = pd.NA

    df["è¿å•å·"] = df["è¿å•å·"].astype(str).str.strip()
    df["æè´§è´¹ç”¨"] = _to_num(df[fee_col])
    df["æè´§æ—¥æœŸ"] = _parse_date_series(df["ETA(åˆ°BCF)"])

    if "ä»“åº“ä»£ç " in df.columns:
        return df[["è¿å•å·","æè´§è´¹ç”¨","æè´§æ—¥æœŸ","ä»“åº“ä»£ç "]]
    return df[["è¿å•å·","æè´§è´¹ç”¨","æè´§æ—¥æœŸ"]]

# ====== è¯»å–ï¼šè¿å•å…¨é“¾è·¯æ±‡æ€»ï¼ˆå‘è´§ä¿¡æ¯ï¼‰ ======
@st.cache_data(ttl=60)
def load_wb_summary_df():
    """ä»ã€Šè¿å•å…¨é“¾è·¯æ±‡æ€»ã€‹è¯»å–å‘è´§ä¾§æ‰€éœ€åˆ—ï¼Œå¹¶åšå®½æ¾åˆ—åå…¼å®¹ä¸ç±»å‹æ¸…æ´—"""
    try:
        ws = client.open(SHEET_WB_SUMMARY_NAME).sheet1
    except SpreadsheetNotFound:
        return pd.DataFrame()
    vals = ws.get_all_values(
        value_render_option="UNFORMATTED_VALUE",
        date_time_render_option="SERIAL_NUMBER",
    )
    if not vals:
        return pd.DataFrame()
    header = _norm_cols(vals[0])
    df = pd.DataFrame(vals[1:], columns=vals[0])

    col_wb   = _pick(df.columns, ["è¿å•å·","Waybill"])
    col_wh   = _pick(df.columns, ["ä»“åº“ä»£ç ","ä»“åº“"])
    col_fee  = _pick(df.columns, ["å‘èµ°è´¹ç”¨","å‘è´§è´¹ç”¨","å‡ºä»“è´¹ç”¨","å‘è½¦è´¹ç”¨"])
    col_ship = _pick(df.columns, ["å‘èµ°æ—¥æœŸ","å‘è´§æ—¥æœŸ","å‡ºä»“æ—¥æœŸ"])
    col_arrb = _pick(df.columns, ["åˆ°BCFæ—¥æœŸ","åˆ°BCFæ—¥","BCFæ—¥æœŸ"])
    col_arrw = _pick(df.columns, ["åˆ°ä»“æ—¥æœŸ","åˆ°ä»“æ—¥","åˆ°ä»“(wh)"])
    col_box  = _pick(df.columns, ["ç®±æ•°","ç®±å­æ•°"])
    col_cbm  = _pick(df.columns, ["ä½“ç§¯","CBM","ä½“ç§¯CBM"])
    col_wt   = _pick(df.columns, ["æ”¶è´¹é‡","è®¡è´¹é‡","é‡é‡","æ”¶è´¹é‡KG","è®¡è´¹é‡KG"])

    for need, col in [("è¿å•å·", col_wb), ("ä»“åº“ä»£ç ", col_wh)]:
        if col is None: df[need] = ""
    df2 = df.rename(columns={
        (col_wb or "è¿å•å·"): "è¿å•å·",
        (col_wh or "ä»“åº“ä»£ç "): "ä»“åº“ä»£ç ",
    })

    # è´¹ç”¨
    if col_fee is None: df2["å‘èµ°è´¹ç”¨"] = pd.NA
    else: df2["å‘èµ°è´¹ç”¨"] = _to_num(df[col_fee])

    # æ—¥æœŸ
    def _parse_col(cname):
        if cname and cname in df.columns:
            return _parse_date_series(df[cname])
        return pd.Series([pd.NaT]*len(df2))
    df2["åˆ°BCFæ—¥æœŸ"] = _parse_col(col_arrb)
    df2["å‘èµ°æ—¥æœŸ"] = _parse_col(col_ship)
    df2["åˆ°ä»“æ—¥æœŸ"] = _parse_col(col_arrw)

    # æ•°é‡å‹ï¼ˆè‹¥æ€»è¡¨ç¼ºå¤±ï¼Œå…ˆç•™ç©ºï¼Œç¨åä»ã€Šåˆ°ä»“æ•°æ®è¡¨ã€‹è¡¥é½ï¼‰
    if col_box: df2["ç®±æ•°"] = _to_num(df[col_box])
    else:       df2["ç®±æ•°"] = pd.NA
    if col_cbm: df2["ä½“ç§¯"] = _to_num(df[col_cbm])
    else:       df2["ä½“ç§¯"] = pd.NA
    if col_wt:  df2["æ”¶è´¹é‡"] = _to_num(df[col_wt])
    else:       df2["æ”¶è´¹é‡"] = pd.NA

    # è§„èŒƒ
    df2["è¿å•å·"] = df2["è¿å•å·"].astype(str).str.strip()
    df2["ä»“åº“ä»£ç "] = df2["ä»“åº“ä»£ç "].astype(str).str.strip()
    df2 = df2[df2["è¿å•å·"] != ""].drop_duplicates(subset=["è¿å•å·"])
    return df2[["è¿å•å·","ä»“åº“ä»£ç ","ç®±æ•°","ä½“ç§¯","æ”¶è´¹é‡","å‘èµ°è´¹ç”¨","åˆ°BCFæ—¥æœŸ","å‘èµ°æ—¥æœŸ","åˆ°ä»“æ—¥æœŸ"]]

# ====== UI ======
st.set_page_config(page_title="ğŸ“¦ BCF å‘è´§æ±‡æ€»ï¼ˆæŒ‰ä»“åº“ï¼‰", layout="wide")
st.title("ğŸ“¦ BCF å‘è´§æ±‡æ€»ï¼ˆæŒ‰ä»“åº“ï¼‰")

tab1, tab2 = st.tabs(["æè´§ä¿¡æ¯ï¼ˆæŒ‰ä»“åº“ï¼‰", "å‘è´§ä¿¡æ¯ï¼ˆæŒ‰ä»“åº“ï¼‰"])

# ---------------- Tab1ï¼šæè´§ä¿¡æ¯ï¼ˆæŒ‰ä»“åº“ï¼‰ ----------------
with tab1:
    left, right = st.columns([1,6])
    with left:
        if st.button("ğŸ”„ åˆ·æ–°ç¼“å­˜", key="btn_refresh_pickup"):
            st.cache_data.clear()
            st.rerun()

    arrivals = load_arrivals_df()
    ship     = load_ship_detail_df()

    if arrivals.empty or ship.empty:
        st.warning("æœªè¯»å–åˆ°æœ‰æ•ˆæ•°æ®ã€‚è¯·ç¡®è®¤ã€Œåˆ°ä»“æ•°æ®è¡¨ã€ä¸ã€Œbolè‡ªææ˜ç»†ã€å­˜åœ¨ä¸”åŒ…å«å¿…éœ€åˆ—ã€‚")
        st.stop()

    # æ—¶é—´ç­›é€‰ï¼ˆæè´§æ—¥æœŸ=ETA(åˆ°BCF)ï¼‰
    valid_dates = ship["æè´§æ—¥æœŸ"].dropna(
    if valid_dates.empty:
        st.info("æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„ã€æè´§æ—¥æœŸã€ï¼Œå°†å±•ç¤ºå…¨éƒ¨è®°å½•ã€‚")
        ship_f = ship.copy()
    else:
        min_d, max_d = valid_dates.min().date(), valid_dates.max().date()
        default_start = max(max_d - timedelta(days=14), min_d)
        start_date, end_date = st.date_input(
            "é€‰æ‹©æ—¶é—´èŒƒå›´ï¼ˆæè´§æ—¥æœŸï¼‰",
            value=(default_start, max_d),
            min_value=min_d, max_value=max_d,
            key="date_pickup"
        )
        ship_f = ship[ship["æè´§æ—¥æœŸ"].between(pd.to_datetime(start_date), pd.to_datetime(end_date))].copy()

    if ship_f.empty:
        st.warning("æ—¶é—´ç­›é€‰åæ— æ•°æ®ã€‚")
        st.stop()

    # åˆå¹¶è´¹ç”¨åˆ°åˆ°ä»“ä¿¡æ¯
    merged = pd.merge(
        ship_f, arrivals,
        on="è¿å•å·", how="left"
    )
    # ä»“åº“å…œåº•
    if "ä»“åº“ä»£ç _x" in merged.columns and "ä»“åº“ä»£ç _y" in merged.columns:
        merged["ä»“åº“ä»£ç "] = merged["ä»“åº“ä»£ç _y"].fillna(merged["ä»“åº“ä»£ç _x"])
        merged.drop(columns=["ä»“åº“ä»£ç _x","ä»“åº“ä»£ç _y"], inplace=True)
    elif "ä»“åº“ä»£ç " not in merged.columns and "ä»“åº“ä»£ç " in ship_f.columns:
        merged["ä»“åº“ä»£ç "] = ship_f["ä»“åº“ä»£ç "]

    # æ•°å€¼å…œåº•
    for c in ["ç®±æ•°","ä½“ç§¯","æ”¶è´¹é‡","æè´§è´¹ç”¨"]:
        if c not in merged.columns: merged[c] = pd.NA
        merged[c] = _to_num(merged[c])

    # ä»“åº“è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
    wh_list = merged["ä»“åº“ä»£ç "].dropna().unique().tolist()
    wh_pick = st.multiselect("ç­›é€‰ä»“åº“ï¼ˆå¯å¤šé€‰ï¼Œç•™ç©º=å…¨éƒ¨ï¼‰", options=sorted(wh_list), key="wh_pickup")
    if wh_pick:
        merged = merged[merged["ä»“åº“ä»£ç "].isin(wh_pick)]
    if merged.empty:
        st.warning("ç­›é€‰åæ— æ•°æ®ã€‚")
        st.stop()

    # æ±‡æ€»ï¼ˆæŒ‰ä»“åº“ï¼‰
    grp = merged.groupby("ä»“åº“ä»£ç ", dropna=False).agg(
        ç®±æ•°åˆè®¡=("ç®±æ•°","sum"),
        ä½“ç§¯åˆè®¡=("ä½“ç§¯","sum"),
        æ”¶è´¹é‡åˆè®¡KG=("æ”¶è´¹é‡","sum"),
        æè´§è´¹ç”¨åˆè®¡=("æè´§è´¹ç”¨","sum"),
    ).reset_index()
    grp["æè´§è´¹ç”¨/KG"] = grp.apply(
        lambda r: (r["æè´§è´¹ç”¨åˆè®¡"] / r["æ”¶è´¹é‡åˆè®¡KG"])
        if pd.notna(r["æ”¶è´¹é‡åˆè®¡KG"]) and r["æ”¶è´¹é‡åˆè®¡KG"]>0 else pd.NA,
        axis=1
    )

    # Grand Total
    grand = pd.DataFrame({
        "ä»“åº“ä»£ç ": ["Grand Total"],
        "ç®±æ•°åˆè®¡": [grp["ç®±æ•°åˆè®¡"].sum(skipna=True)],
        "ä½“ç§¯åˆè®¡": [grp["ä½“ç§¯åˆè®¡"].sum(skipna=True)],
        "æ”¶è´¹é‡åˆè®¡KG": [grp["æ”¶è´¹é‡åˆè®¡KG"].sum(skipna=True)],
        "æè´§è´¹ç”¨åˆè®¡": [grp["æè´§è´¹ç”¨åˆè®¡"].sum(skipna=True)],
    })
    grand["æè´§è´¹ç”¨/KG"] = (
        grand["æè´§è´¹ç”¨åˆè®¡"] / grand["æ”¶è´¹é‡åˆè®¡KG"]
        if grand.loc[0,"æ”¶è´¹é‡åˆè®¡KG"] and grand.loc[0,"æ”¶è´¹é‡åˆè®¡KG"]>0 else pd.NA
    )

    st.markdown("### ğŸ“Š æ±‡æ€»ç»“æœï¼ˆæŒ‰ä»“åº“ï½œæè´§ï¼‰")
    show_df = pd.concat([grp, grand], ignore_index=True)

    def _fmt2(x): 
        return "" if pd.isna(x) else f"{x:,.2f}"
    fmt_df = show_df.copy()
    for c in ["ç®±æ•°åˆè®¡","ä½“ç§¯åˆè®¡","æ”¶è´¹é‡åˆè®¡KG","æè´§è´¹ç”¨åˆè®¡","æè´§è´¹ç”¨/KG"]:
        if c in fmt_df.columns: fmt_df[c] = fmt_df[c].map(_fmt2)

    st.dataframe(fmt_df, use_container_width=True, height=420)

    with st.expander("ğŸ” æŸ¥çœ‹ç”¨äºæ±‡æ€»çš„æ˜ç»†ï¼ˆæè´§ï¼‰"):
        cols = ["ä»“åº“ä»£ç ","è¿å•å·","ç®±æ•°","ä½“ç§¯","æ”¶è´¹é‡","æè´§è´¹ç”¨","æè´§æ—¥æœŸ"]
        exist_cols = [c for c in cols if c in merged.columns]
        st.dataframe(
            merged[exist_cols].sort_values(["ä»“åº“ä»£ç ","æè´§æ—¥æœŸ","è¿å•å·"], na_position="last"),
            use_container_width=True, height=360
        )

    csv = show_df.to_csv(index=False).encode("utf-8-sig")
    st.download_button("â¬‡ï¸ ä¸‹è½½æ±‡æ€» CSVï¼ˆæè´§ï¼‰", data=csv, file_name="bcf_warehouse_pickup_summary.csv", mime="text/csv")

# ---------------- Tab2ï¼šå‘è´§ä¿¡æ¯ï¼ˆæŒ‰ä»“åº“ï¼‰ ----------------
with tab2:
    left, right = st.columns([1,6])
    with left:
        if st.button("ğŸ”„ åˆ·æ–°ç¼“å­˜", key="btn_refresh_ship"):
            st.cache_data.clear()
            st.rerun()

    wb_sum = load_wb_summary_df()
    arrivals2 = load_arrivals_df()

    if wb_sum.empty:
        st.warning(f"æœªèƒ½ä»ã€{SHEET_WB_SUMMARY_NAME}ã€è¯»å–åˆ°æ•°æ®æˆ–ç¼ºå°‘å…³é”®åˆ—ã€‚")
        st.stop()

    # ç”¨ã€Šåˆ°ä»“æ•°æ®è¡¨ã€‹è¡¥é½ ç®±æ•°/ä½“ç§¯/æ”¶è´¹é‡ï¼ˆæ€»è¡¨ç¼ºçš„æƒ…å†µä¸‹ï¼‰
    if not arrivals2.empty:
        w = arrivals2[["è¿å•å·","ä»“åº“ä»£ç ","ç®±æ•°","ä½“ç§¯","æ”¶è´¹é‡"]].copy()
        w = w.drop_duplicates(subset=["è¿å•å·"])
        m = wb_sum.merge(w, on=["è¿å•å·","ä»“åº“ä»£ç "], how="left", suffixes=("","_arr"))
        for c in ["ç®±æ•°","ä½“ç§¯","æ”¶è´¹é‡"]:
            if c in m.columns and f"{c}_arr" in m.columns:
                m[c] = m[c].combine_first(m[f"{c}_arr"])
        wb_sum = m[[ "è¿å•å·","ä»“åº“ä»£ç ","ç®±æ•°","ä½“ç§¯","æ”¶è´¹é‡","å‘èµ°è´¹ç”¨","åˆ°BCFæ—¥æœŸ","å‘èµ°æ—¥æœŸ","åˆ°ä»“æ—¥æœŸ" ]]

    # æ—¶é—´ç­›é€‰ï¼ˆä»¥â€œå‘èµ°æ—¥æœŸâ€ä¸ºé”šï¼Œæ›´è´´åˆå‘è´§ä¾§è¿è¥å‘¨æœŸï¼‰
    valid_ship = wb_sum["å‘èµ°æ—¥æœŸ"].dropna()
    if valid_ship.empty:
        st.info("æ²¡æœ‰å¯ç”¨çš„ã€å‘èµ°æ—¥æœŸã€ï¼Œå°†å±•ç¤ºå…¨éƒ¨è®°å½•ã€‚")
        wb_f = wb_sum.copy()
    else:
        min_d, max_d = valid_ship.min().date(), valid_ship.max().date()
        default_start = max(max_d - timedelta(days=14), min_d)
        start_date, end_date = st.date_input(
            "é€‰æ‹©æ—¶é—´èŒƒå›´ï¼ˆå‘èµ°æ—¥æœŸï¼‰",
            value=(default_start, max_d),
            min_value=min_d, max_value=max_d,
            key="date_ship"
        )
        wb_f = wb_sum[wb_sum["å‘èµ°æ—¥æœŸ"].between(pd.to_datetime(start_date), pd.to_datetime(end_date))].copy()

    if wb_f.empty:
        st.warning("æ—¶é—´ç­›é€‰åæ— æ•°æ®ã€‚")
        st.stop()

    # ä»“åº“è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
    wh_list2 = wb_f["ä»“åº“ä»£ç "].dropna().unique().tolist()
    wh_pick2 = st.multiselect("ç­›é€‰ä»“åº“ï¼ˆå¯å¤šé€‰ï¼Œç•™ç©º=å…¨éƒ¨ï¼‰", options=sorted(wh_list2), key="wh_ship")
    if wh_pick2:
        wb_f = wb_f[wb_f["ä»“åº“ä»£ç "].isin(wh_pick2)]
    if wb_f.empty:
        st.warning("ç­›é€‰åæ— æ•°æ®ã€‚")
        st.stop()

    # æ•°å€¼å…œåº•
    for c in ["ç®±æ•°","ä½“ç§¯","æ”¶è´¹é‡","å‘èµ°è´¹ç”¨"]:
        wb_f[c] = _to_num(wb_f[c])

    # æ—¶æ•ˆï¼ˆé€å•ï¼‰
    wb_f["_å‘è´§æ—¶æ•ˆå¤©"] = (wb_f["å‘èµ°æ—¥æœŸ"] - wb_f["åˆ°BCFæ—¥æœŸ"]).dt.days     # å‘èµ° - åˆ°BCF
    wb_f["_å¦¥æŠ•æ—¶æ•ˆå¤©"] = (wb_f["åˆ°ä»“æ—¥æœŸ"] - wb_f["å‘èµ°æ—¥æœŸ"]).dt.days     # åˆ°ä»“ - å‘èµ°

    # æŒ‰ä»“åº“æ±‡æ€»
    grp_ship = wb_f.groupby("ä»“åº“ä»£ç ", dropna=False).agg(
        ç®±æ•°åˆè®¡=("ç®±æ•°","sum"),
        ä½“ç§¯åˆè®¡=("ä½“ç§¯","sum"),
        æ”¶è´¹é‡åˆè®¡KG=("æ”¶è´¹é‡","sum"),
        å‘è´§è´¹ç”¨åˆè®¡=("å‘èµ°è´¹ç”¨","sum"),
        å‘è´§æ—¶æ•ˆå¤©=("_å‘è´§æ—¶æ•ˆå¤©","mean"),
        å¦¥æŠ•æ—¶æ•ˆå¤©=("_å¦¥æŠ•æ—¶æ•ˆå¤©","mean"),
        å•æ®æ•°=("è¿å•å·","count"),
    ).reset_index()

    # å‘è´§è´¹ç”¨/KG
    grp_ship["å‘è´§è´¹ç”¨/KG"] = grp_ship.apply(
        lambda r: (r["å‘è´§è´¹ç”¨åˆè®¡"]/r["æ”¶è´¹é‡åˆè®¡KG"]) if pd.notna(r["æ”¶è´¹é‡åˆè®¡KG"]) and r["æ”¶è´¹é‡åˆè®¡KG"]>0 else pd.NA,
        axis=1
    )

    # Grand Totalï¼ˆæ—¶æ•ˆæŒ‰å…¨é‡é€å•å¹³å‡ï¼‰
    grand_ship = pd.DataFrame({
        "ä»“åº“ä»£ç ": ["Grand Total"],
        "ç®±æ•°åˆè®¡": [grp_ship["ç®±æ•°åˆè®¡"].sum(skipna=True)],
        "ä½“ç§¯åˆè®¡": [grp_ship["ä½“ç§¯åˆè®¡"].sum(skipna=True)],
        "æ”¶è´¹é‡åˆè®¡KG": [grp_ship["æ”¶è´¹é‡åˆè®¡KG"].sum(skipna=True)],
        "å‘è´§è´¹ç”¨åˆè®¡": [grp_ship["å‘è´§è´¹ç”¨åˆè®¡"].sum(skipna=True)],
        "å‘è´§æ—¶æ•ˆå¤©": [wb_f["_å‘è´§æ—¶æ•ˆå¤©"].mean(skipna=True)],
        "å¦¥æŠ•æ—¶æ•ˆå¤©": [wb_f["_å¦¥æŠ•æ—¶æ•ˆå¤©"].mean(skipna=True)],
        "å•æ®æ•°": [wb_f["è¿å•å·"].count()],
    })
    grand_ship["å‘è´§è´¹ç”¨/KG"] = (
        grand_ship["å‘è´§è´¹ç”¨åˆè®¡"]/grand_ship["æ”¶è´¹é‡åˆè®¡KG"]
        if grand_ship.loc[0,"æ”¶è´¹é‡åˆè®¡KG"] and grand_ship.loc[0,"æ”¶è´¹é‡åˆè®¡KG"]>0 else pd.NA
    )

    st.markdown("### ğŸšš å‘è´§ä¿¡æ¯æ±‡æ€»ï¼ˆæŒ‰ä»“åº“ï¼‰")
    show_ship = pd.concat([grp_ship, grand_ship], ignore_index=True)
        # è°ƒæ•´åˆ—é¡ºåºï¼šæŠŠã€Œå‘è´§æ—¶æ•ˆå¤©ã€ã€Œå¦¥æŠ•æ—¶æ•ˆå¤©ã€æ”¾åœ¨æœ€åä¸¤åˆ—
    desired_order = [
        "ä»“åº“ä»£ç ", "ç®±æ•°åˆè®¡", "ä½“ç§¯åˆè®¡", "æ”¶è´¹é‡åˆè®¡KG",
        "å‘è´§è´¹ç”¨åˆè®¡", "å•æ®æ•°", "å‘è´§è´¹ç”¨/KG",
        "å‘è´§æ—¶æ•ˆå¤©", "å¦¥æŠ•æ—¶æ•ˆå¤©"
    ]
    present = [c for c in desired_order if c in show_ship.columns]
    others  = [c for c in show_ship.columns if c not in present]
    show_ship = show_ship[present + others]

    def _fmt2(x): 
        return "" if pd.isna(x) else f"{x:,.2f}"
    fmt_cols = ["ç®±æ•°åˆè®¡","ä½“ç§¯åˆè®¡","æ”¶è´¹é‡åˆè®¡KG","å‘è´§è´¹ç”¨åˆè®¡","å‘è´§è´¹ç”¨/KG","å‘è´§æ—¶æ•ˆå¤©","å¦¥æŠ•æ—¶æ•ˆå¤©"]
    fmt_ship = show_ship.copy()
    for c in fmt_cols:
        if c in fmt_ship.columns: fmt_ship[c] = fmt_ship[c].map(_fmt2)

    st.dataframe(fmt_ship, use_container_width=True, height=420)

    with st.expander("ğŸ” æŸ¥çœ‹ç”¨äºæ±‡æ€»çš„æ˜ç»†ï¼ˆå‘è´§ï¼Œå«æ—¶æ•ˆå¤©æ•°ï¼‰"):
        detail = wb_f.copy()
        detail["å‘è´§æ—¶æ•ˆå¤©"] = detail["_å‘è´§æ—¶æ•ˆå¤©"]
        detail["å¦¥æŠ•æ—¶æ•ˆå¤©"] = detail["_å¦¥æŠ•æ—¶æ•ˆå¤©"]
        st.dataframe(
            detail[["ä»“åº“ä»£ç ","è¿å•å·","ç®±æ•°","ä½“ç§¯","æ”¶è´¹é‡","å‘èµ°è´¹ç”¨","åˆ°BCFæ—¥æœŸ","å‘èµ°æ—¥æœŸ","åˆ°ä»“æ—¥æœŸ","å‘è´§æ—¶æ•ˆå¤©","å¦¥æŠ•æ—¶æ•ˆå¤©"]]
                .sort_values(["ä»“åº“ä»£ç ","å‘èµ°æ—¥æœŸ","è¿å•å·"], na_position="last"),
            use_container_width=True, height=360
        )

    csv2 = show_ship.to_csv(index=False).encode("utf-8-sig")
    st.download_button("â¬‡ï¸ ä¸‹è½½æ±‡æ€» CSVï¼ˆå‘è´§ï¼‰", data=csv2, file_name="bcf_warehouse_ship_summary.csv", mime="text/csv")
