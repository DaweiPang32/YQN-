# recv_app.py  â€”â€” æ”¶è´§æ‰˜ç›˜ç»‘å®šï¼ˆä¸»æ•°æ®æºï¼šbolè‡ªææ˜ç»† + åˆ°ä»“æ•°æ®è¡¨(ç®±æ•°/ä»“åº“ä»£ç )ï¼‰
import streamlit as st
import pandas as pd
import gspread
from google.oauth2.service_account import Credentials
from gspread.exceptions import SpreadsheetNotFound, APIError
from datetime import datetime, timedelta, date
from zoneinfo import ZoneInfo
import time
import re
import zlib

# ========= Google æˆæƒ =========
SCOPES = ["https://www.googleapis.com/auth/spreadsheets", "https://www.googleapis.com/auth/drive"]

def get_gspread_client():
    # 1) Cloudï¼šä¼˜å…ˆä» st.secrets è¯»å–ï¼ˆStreamlit Cloud é…ç½®çš„æœºå¯†ï¼‰
    if "gcp_service_account" in st.secrets:
        sa_info = st.secrets["gcp_service_account"]  # ä¸€ä¸ª dict
        creds = Credentials.from_service_account_info(sa_info, scopes=SCOPES)
        return gspread.authorize(creds)
    # 2) æœ¬åœ°ï¼šå…¼å®¹ JSON æ–‡ä»¶
    else:
        creds = Credentials.from_service_account_file("service_accounts.json", scopes=SCOPES)
        return gspread.authorize(creds)

client = get_gspread_client()

# ========= è¡¨åé…ç½® =========
SHEET_ARRIVALS_NAME   = "åˆ°ä»“æ•°æ®è¡¨"
SHEET_SHIP_DETAIL     = "bolè‡ªææ˜ç»†"     # å‘è´§appè¿½åŠ çš„æºï¼Œä½œä¸ºæ”¶è´§å±•ç¤ºä¸»æ•°æ®
SHEET_PALLET_DETAIL   = "æ‰˜ç›˜æ˜ç»†è¡¨"       # æ”¶è´§ç«¯ä¸Šä¼ ç›®æ ‡è¡¨ï¼ˆè¿½åŠ ï¼‰

# ========= å”¯ä¸€IDæ³¨å†Œè¡¨é…ç½®ï¼ˆç”¨äºç»å¯¹å”¯ä¸€çš„æ‰˜ç›˜å·ï¼‰=========
SHEET_PALLET_REGISTRY_TITLE = "æ‰˜ç›˜å·æ³¨å†Œè¡¨"  # å»ºè®®å›ºå®šæ”¾åˆ° st.secrets["pallet_registry_key"]

# ========= å·¥å…·ï¼šæŒ‡æ•°é€€é¿é‡è¯•ï¼ˆä¸“æ²» 429/5xxï¼‰=========
def _retry(fn, *args, _retries=5, _base=0.6, _factor=1.8, _max_sleep=6.0, **kwargs):
    for i in range(_retries):
        try:
            return fn(*args, **kwargs)
        except APIError as e:
            code = getattr(e, "response", None).status_code if getattr(e, "response", None) else None
            if code in (429, 500, 502, 503, 504):
                time.sleep(min(_base * (_factor ** i), _max_sleep))
                continue
            raise

# ========= å°å·¥å…· =========
def excel_serial_to_date(val):
    """æŠŠ Excel æ•°å­—æ—¥æœŸ(å¦‚ 45857) è½¬ä¸º datetimeï¼›éæ³•è¿”å› NaT"""
    try:
        f = float(val)
        return datetime(1899, 12, 30) + timedelta(days=f)
    except Exception:
        return pd.NaT

ALPHABET = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'

def _to_base36(n: int) -> str:
    if n == 0:
        return '0'
    s = []
    while n:
        n, r = divmod(n, 36)
        s.append(ALPHABET[r])
    return ''.join(reversed(s))

@st.cache_resource(ttl=24*3600)
def get_ws(sheet_title: str, secret_key_name: str | None = None):
    """
    ä¼˜å…ˆç”¨ key æ‰“å¼€é¿å…é‡åæŸ¥è¯¢ï¼›è¿”å› sheet1 å¥æŸ„ï¼ˆé•¿æœŸç¼“å­˜ï¼‰ã€‚
    åœ¨ st.secrets é‡Œå¯é…ç½®ï¼šship_detail_key / arrivals_key / pallet_detail_key / pallet_registry_key
    """
    key = ""
    if secret_key_name:
        try:
            key = st.secrets.get(secret_key_name, "").strip()
        except Exception:
            key = ""

    if key:
        ss = _retry(client.open_by_key, key)
    else:
        ss = _retry(client.open, sheet_title)
    return ss.sheet1

def get_pallet_registry_ws():
    """
    è¿”å›â€˜æ‰˜ç›˜å·æ³¨å†Œè¡¨â€™çš„ sheet1ã€‚ä¸å­˜åœ¨åˆ™åˆ›å»ºå¹¶å†™è¡¨å¤´ã€‚
    ä¼˜å…ˆç”¨ key æ‰“å¼€ï¼ˆæ”¾åœ¨ st.secrets["pallet_registry_key"]ï¼‰ï¼Œé¿å…é‡åå¸¦æ¥çš„æ­§ä¹‰ã€‚
    """
    key = ""
    try:
        key = st.secrets.get("pallet_registry_key", "").strip()
    except Exception:
        key = ""
    try:
        if key:
            ss = _retry(client.open_by_key, key)
        else:
            ss = _retry(client.open, SHEET_PALLET_REGISTRY_TITLE)
    except SpreadsheetNotFound:
        ss = _retry(client.create, SHEET_PALLET_REGISTRY_TITLE)
        # åˆ›å»ºåå¯åœ¨ Google Sheets ä¸­æ‰‹åŠ¨å…±äº«ç»™å…¶ä»–éœ€è¦å†™å…¥çš„æœåŠ¡è´¦å·
    ws = ss.sheet1
    # å¦‚æœæ˜¯ä¸€ä¸ªå…¨æ–°è¡¨ï¼Œå†™å…¥è¡¨å¤´
    if not _retry(ws.get_all_values):
        _retry(ws.update, [["ts_iso", "warehouse", "note"]])
    return ws

def allocate_unique_seq(warehouse: str | None) -> int:
    """
    é€šè¿‡å‘æ³¨å†Œè¡¨ append ä¸€è¡Œæ¥è·å–ä¸€ä¸ªå”¯ä¸€çš„è¡Œå·ã€‚
    Google Sheets çš„ append æ˜¯åŸå­è¿½åŠ ï¼šå¹¶å‘æ—¶æ¯æ¬¡éƒ½ä¼šæ‹¿åˆ°ä¸åŒçš„è¡Œå·ã€‚
    """
    ws = get_pallet_registry_ws()
    resp = _retry(
        ws.append_row,
        [datetime.utcnow().isoformat(), (warehouse or "").upper(), ""],
        value_input_option="RAW",
        insert_data_option="INSERT_ROWS",
        table_range="A1",
        include_values_in_response=True,
    )
    updated_range = (resp or {}).get("updates", {}).get("updatedRange", "")
    # å½¢å¦‚ "Sheet1!A42:C42" â†’ æå– 42
    m = re.search(r"![A-Z]+(\d+):", updated_range)
    if m:
        return int(m.group(1))
    # å…œåº•ï¼ˆæå°‘å‘ç”Ÿï¼‰ï¼šç”¨å½“å‰å·²ç”¨æ•°æ®è¡Œæ•°ä½œä¸ºåºåˆ—
    try:
        used = len(_retry(ws.get_all_values))
        return max(used, 2)  # è‡³å°‘ä»ç¬¬2è¡Œèµ·ï¼ˆç¬¬1è¡Œä¸ºè¡¨å¤´ï¼‰
    except Exception:
        return int(datetime.utcnow().timestamp())

def generate_pallet_id(warehouse: str | None = None) -> str:
    """
    PYYMMDD-WHH-SEQ36-C
    - YYMMDDï¼šå½“å‰æ—¥æœŸ
    - WHH   ï¼šä»“åº“å‰ä¸‰ä½ï¼ˆä¸è¶³è¡¥ UNKï¼‰
    - SEQ36 ï¼šæ³¨å†Œè¡¨è¡Œå·çš„ base36ï¼ˆå®šé•¿6ä½ï¼‰
    - C     ï¼šCRC32 æ ¡éªŒä½ï¼ˆå•å­—ç¬¦ï¼‰
    """
    wh = (str(warehouse) if warehouse else "UNK").upper()[:3] or "UNK"
    ts = datetime.now().strftime("%y%m%d")

    try:
        seq = allocate_unique_seq(wh)
    except Exception:
        # æ³¨å†Œè¡¨ä¸´æ—¶å¼‚å¸¸æ—¶ï¼Œé€€åŒ–åˆ°æ—¶é—´æˆ³æ–¹æ¡ˆï¼ˆä»ç„¶æä½æ¦‚ç‡é‡å¤ï¼‰
        seq = int(datetime.utcnow().timestamp() * 10_000)

    seq36 = _to_base36(seq).rjust(6, '0')
    core = f"P{ts}-{wh}-{seq36}"
    check = ALPHABET[zlib.crc32(core.encode()) % 36]
    return f"{core}-{check}"

# ========= ç¼“å­˜è¯»å– =========
@st.cache_data(ttl=300)  # 5 åˆ†é’Ÿï¼Œæ˜¾è‘—é™ä½æ¯åˆ†é’Ÿè¯»é‡
def load_ship_detail_df():
    """
    è¯»å– bolè‡ªææ˜ç»†ï¼ˆå‘è´§æ˜ç»†ï¼‰ï¼Œä½œä¸ºæ”¶è´§å±•ç¤ºçš„ä¸»æ•°æ®æºã€‚
    åªä¿ç•™ï¼šè¿å•å· / å®¢æˆ·å•å· / ETA(åˆ°BCF)ã€‚æ—¥æœŸå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åºåˆ—å·ï¼Œè¿™é‡Œç»Ÿä¸€è§£æä¸º datetimeã€‚
    """
    try:
        ws = get_ws(SHEET_SHIP_DETAIL, "ship_detail_key")
    except SpreadsheetNotFound:
        return pd.DataFrame()

    vals = _retry(ws.get_all_values,
                  value_render_option="UNFORMATTED_VALUE",
                  date_time_render_option="SERIAL_NUMBER")
    if not vals:
        return pd.DataFrame()

    header = vals[0]
    rows   = vals[1:]
    df = pd.DataFrame(rows, columns=header)

    # å…œåº•éœ€è¦åˆ—
    for col in ["è¿å•å·", "å®¢æˆ·å•å·", "ETA(åˆ°BCF)"]:
        if col not in df.columns:
            df[col] = pd.NA

    df["è¿å•å·"] = df["è¿å•å·"].astype(str).str.strip()
    df = df[df["è¿å•å·"] != ""]

    # ETA è§£æï¼šå°è¯•åºåˆ—å·ï¼Œå† to_datetime
    parsed_serial = df["ETA(åˆ°BCF)"].apply(excel_serial_to_date)
    fallback      = pd.to_datetime(df["ETA(åˆ°BCF)"], errors="coerce")
    df["ETA(åˆ°BCF)"] = parsed_serial.combine_first(fallback)

    # è‹¥åŒä¸€è¿å•å‡ºç°å¤šè¡Œï¼ˆå‘è´§ç«¯å¯èƒ½å¤šæ¬¡è¿½åŠ ï¼‰ï¼Œä¿ç•™æœ€åä¸€æ¡
    if not df.empty:
        df = df.groupby("è¿å•å·", as_index=False).last()

    return df[["è¿å•å·", "å®¢æˆ·å•å·", "ETA(åˆ°BCF)"]]

@st.cache_data(ttl=300)
def load_arrivals_df():
    """
    è¯»å– åˆ°ä»“æ•°æ®è¡¨ï¼›ä»…ä¿ç•™ï¼šè¿å•å· / ä»“åº“ä»£ç  / ç®±æ•°ã€‚
    """
    ws = get_ws(SHEET_ARRIVALS_NAME, "arrivals_key")
    data = _retry(ws.get_all_values)
    if not data:
        return pd.DataFrame()

    header = [h.replace("\u00A0", " ").replace("\n", "").replace(" ", "") for h in data[0]]
    df = pd.DataFrame(data[1:], columns=header)

    for need in ["è¿å•å·", "ä»“åº“ä»£ç ", "ç®±æ•°"]:
        if need not in df.columns:
            df[need] = pd.NA

    df["è¿å•å·"] = df["è¿å•å·"].astype(str).str.strip()
    df = df.drop_duplicates(subset=["è¿å•å·"])
    # ç®±æ•°è½¬æ•°å€¼ï¼ˆå¯èƒ½ä»éœ€äººå·¥è°ƒæ•´ï¼‰
    df["ç®±æ•°"] = pd.to_numeric(df["ç®±æ•°"], errors="coerce")

    return df[["è¿å•å·", "ä»“åº“ä»£ç ", "ç®±æ•°"]]

def load_uploaded_allocations(warehouse: str) -> dict:
    """
    ä»ã€Šæ‰˜ç›˜æ˜ç»†è¡¨ã€‹ä¸­æ±‡æ€»ï¼šåŒä»“åº“ä¸‹æ¯ä¸ªè¿å•å·å·²ä¸Šä¼ çš„â€œç®±æ•°â€æ€»å’Œã€‚
    è¿”å› {è¿å•å·: å·²ä¸Šä¼ ç®±æ•°}
    """
    try:
        sheet = get_ws(SHEET_PALLET_DETAIL, "pallet_detail_key")
    except SpreadsheetNotFound:
        return {}

    values = _retry(sheet.get_all_values)
    if not values:
        return {}

    header = values[0]
    rows = values[1:]

    def col_idx(name: str, default=None):
        try:
            return header.index(name)
        except ValueError:
            return default

    idx_wh = col_idx("ä»“åº“ä»£ç ")
    idx_wb = col_idx("è¿å•å·")
    idx_qty = col_idx("ç®±æ•°")

    if idx_wb is None or idx_qty is None:
        return {}

    agg = {}
    for r in rows:
        if idx_wh is not None and len(r) > idx_wh:
            if str(r[idx_wh]).strip() != str(warehouse).strip():
                continue
        if len(r) <= idx_wb or len(r) <= idx_qty:
            continue
        wb = str(r[idx_wb]).strip()
        if not wb:
            continue
        qty = pd.to_numeric(r[idx_qty], errors="coerce")
        if pd.isna(qty):
            qty = 0
        agg[wb] = agg.get(wb, 0) + int(qty)
    return agg

# ========= é¡µé¢è®¾ç½® =========
st.set_page_config(page_title="ç‰©æµæ”¶è´§å¹³å°ï¼ˆåŸºäºå‘è´§æ˜ç»†ï¼‰", layout="wide")
st.title("ğŸ“¦ BCF æ”¶è´§æ‰˜ç›˜ç»‘å®šï¼ˆæ•°æ®æºï¼šbolè‡ªææ˜ç»† + åˆ°ä»“ç®±æ•°ï¼‰")

# ========= åˆ·æ–°ç¼“å­˜ï¼ˆè½¯åˆ·æ–°ï¼Œä»…æ¸…æ•°æ®åŠ è½½å‡½æ•°ï¼‰ =========
tools_l, _ = st.columns([1,6])
with tools_l:
    if st.button("ğŸ”„ ä»…åˆ·æ–°æ•°æ®è¡¨ç¼“å­˜"):
        load_ship_detail_df.clear()
        load_arrivals_df.clear()
        st.rerun()
# ========= åˆå§‹åŒ–çŠ¶æ€ =========
if "all_pallets" not in st.session_state:
    st.session_state["all_pallets"] = []
if "pallet_detail_records" not in st.session_state:
    st.session_state["pallet_detail_records"] = []

# ========= æ•°æ®åŠ è½½ï¼ˆæ•è·429å‹å¥½æç¤ºï¼‰ =========
try:
    ship_df    = load_ship_detail_df()   # è¿å•å· / å®¢æˆ·å•å· / ETA(åˆ°BCF)
    arrivals   = load_arrivals_df()      # è¿å•å· / ä»“åº“ä»£ç  / ç®±æ•°
except APIError as e:
    code = getattr(e, "response", None).status_code if getattr(e, "response", None) else None
    if code == 429:
        st.error("Google Sheets è¯»å–é¢‘ç‡è¶…é™ï¼ˆ429ï¼‰ã€‚è¯·ç¨åå†è¯•ï¼Œæˆ–å‡å°‘æ‰¹é‡æ“ä½œé¢‘æ¬¡ã€‚")
        st.stop()
    else:
        raise

if ship_df.empty and arrivals.empty:
    st.warning("æ²¡æœ‰ä» Google Sheets è¯»å–åˆ°æ•°æ®ï¼Œè¯·æ£€æŸ¥è¡¨å/æƒé™ã€‚")
    st.stop()

# ========= åˆå¹¶ï¼ˆä»¥ bolè‡ªææ˜ç»† ä¸ºä¸»ï¼Œå·¦è¿åˆ°ä»“æ•°æ®è¡¨çš„ ä»“åº“ä»£ç  / ç®±æ•°ï¼‰=========
merged_df = ship_df.merge(arrivals, on="è¿å•å·", how="left")
# ç¡®ä¿ ETA(åˆ°BCF) ä¸º datetime
merged_df["ETA(åˆ°BCF)"] = pd.to_datetime(merged_df["ETA(åˆ°BCF)"], errors="coerce")

# ===== æ—¥æœŸç­›é€‰ï¼ˆæŒ‰ ETA(åˆ°BCF)ï¼‰=====
valid_dates = merged_df["ETA(åˆ°BCF)"].dropna()
if valid_dates.empty:
    st.warning("å½“å‰æ•°æ®ä¸­æ²¡æœ‰å¯è§£æçš„ ETA(åˆ°BCF)ã€‚è¯·æ£€æŸ¥æºè¡¨æˆ–åˆ·æ–°ç¼“å­˜ã€‚")
    st.stop()

min_d = valid_dates.min().date()
max_d = valid_dates.max().date()
default_start = max(max_d - timedelta(days=14), min_d)

st.markdown("### ğŸ” æŒ‰ ETA(åˆ°BCF) æ—¥æœŸç­›é€‰")
start_date, end_date = st.date_input(
    "é€‰æ‹©æ—¥æœŸèŒƒå›´ï¼ˆåŒ…å«ç«¯ç‚¹ï¼‰",
    value=(default_start, max_d),
    min_value=min_d,
    max_value=max_d
)

mask_date = merged_df["ETA(åˆ°BCF)"].between(pd.to_datetime(start_date), pd.to_datetime(end_date))
merged_df_by_date = merged_df[mask_date].copy()

# ===== ä»“åº“ç­›é€‰ï¼ˆåŸºäºæ—¥æœŸè¿‡æ»¤åçš„ç»“æœï¼‰=====
warehouse_options = merged_df_by_date["ä»“åº“ä»£ç "].dropna().unique()
if len(warehouse_options) == 0:
    st.warning("å½“å‰æ—¥æœŸèŒƒå›´å†…æ²¡æœ‰ä»“åº“æ•°æ®ï¼Œè¯·è°ƒæ•´æ—¥æœŸèŒƒå›´ã€‚")
    st.stop()

warehouse = st.selectbox("é€‰æ‹©ä»“åº“ä»£ç ï¼š", warehouse_options)

# ===== å±•ç¤ºåˆå¹¶ç»“æœï¼ˆå·²æŒ‰æ—¥æœŸä¸ä»“åº“è¿‡æ»¤ï¼‰=====
display_cols = ["ä»“åº“ä»£ç ", "è¿å•å·", "å®¢æˆ·å•å·", "ETA(åˆ°BCF)", "ç®±æ•°"]
use_cols = [c for c in display_cols if c in merged_df_by_date.columns]
filtered_df = merged_df_by_date[merged_df_by_date["ä»“åº“ä»£ç "] == warehouse]
filtered_df = filtered_df[use_cols].sort_values(by=["ETA(åˆ°BCF)", "è¿å•å·"], na_position="last")

st.markdown("### ğŸ“‹ å·²åˆ° BCF çš„å¾…æ”¶è´§è¿å•ï¼ˆå·²æŒ‰æ—¥æœŸä¸ä»“åº“è¿‡æ»¤ï¼‰")
st.dataframe(filtered_df, use_container_width=True, height=320)

# ========== æ‰˜ç›˜ç»‘å®šé€»è¾‘ ==========
st.markdown("### ğŸ§° æ‰˜ç›˜æ“ä½œ")

# å·¥å…·æ ï¼šå•ä¸ªæ–°å»ºã€æ‰¹é‡æ•°é‡ã€æ‰¹é‡æ–°å»º
col1, col2, col3, _sp = st.columns([1, 1, 1, 6])

with col1:
    st.write(" ")
    if st.button("â• æ–°å»ºæ‰˜ç›˜", key="create_one_pallet", use_container_width=True):
        new_pallet = generate_pallet_id(warehouse)
        tries = 0
        while new_pallet in st.session_state["all_pallets"] and tries < 5:
            new_pallet = generate_pallet_id(warehouse)
            tries += 1
        st.session_state["all_pallets"].append(new_pallet)
        st.success(f"å·²æ–°å»ºæ‰˜ç›˜ï¼š{new_pallet}")

with col2:
    bulk_n = st.number_input(
        "æ‰¹é‡æ•°é‡",
        min_value=1, max_value=200, step=1, value=5,
        key="bulk_new_pallets_count"
    )

with col3:
    st.write(" ")
    if st.button("ğŸ§© æ‰¹é‡æ–°å»ºæ‰˜ç›˜", key="create_bulk_pallets", use_container_width=True):
        created = []
        existing = set(st.session_state["all_pallets"])
        for _ in range(int(bulk_n)):
            p = generate_pallet_id(warehouse)
            tries = 0
            while (p in existing or p in created) and tries < 8:
                p = generate_pallet_id(warehouse)
                tries += 1
            created.append(p)
        st.session_state["all_pallets"].extend(created)
        st.success(f"âœ… æ‰¹é‡æ–°å»ºå®Œæˆï¼Œå…± {len(created)} ä¸ªï¼š{', '.join(created[:5])}{' ...' if len(created)>5 else ''}")

# æ¯ä¸ªæ‰˜ç›˜çš„æ“ä½œåŒºï¼ˆç”¨ form é˜²æŠ–ï¼Œå‡å°‘ rerun å¯¼è‡´çš„è¯»å–å‹åŠ›ï¼‰
for pallet_id in list(st.session_state["all_pallets"]):
    with st.expander(f"ğŸ“¦ æ‰˜ç›˜ {pallet_id} æ“ä½œåŒº", expanded=True):
        form_key = f"form_{pallet_id}"
        with st.form(form_key, clear_on_submit=False):
            st.markdown(f"ğŸšš å½“å‰æ‰˜ç›˜å·ï¼š**{pallet_id}**")
            waybills = filtered_df["è¿å•å·"].dropna().unique()

            st.markdown("#### ğŸ“¦ æ‰˜ç›˜æ•´ä½“å°ºå¯¸ï¼ˆç»Ÿä¸€å¡«å†™ä¸€æ¬¡ï¼‰")
            pallet_cols = st.columns(4)
            with pallet_cols[0]:
                weight = st.number_input("æ‰˜ç›˜é‡é‡", min_value=0.0, key=f"weight_{pallet_id}")
            with pallet_cols[1]:
                length = st.number_input("æ‰˜ç›˜é•¿", min_value=0.0, key=f"length_{pallet_id}")
            with pallet_cols[2]:
                width = st.number_input("æ‰˜ç›˜å®½",  min_value=0.0, key=f"width_{pallet_id}")
            with pallet_cols[3]:
                height = st.number_input("æ‰˜ç›˜é«˜",  min_value=0.0, key=f"height_{pallet_id}")

            # ===== å½•å…¥è¿å•ï¼ˆä¸¤ç§æ–¹å¼ï¼‰=====
            st.markdown("#### ğŸ“¦ è¿å•æ˜ç»†ï¼ˆé€‰æ‹©ä¸€ç§æ–¹å¼å½•å…¥ï¼‰")
            tab_paste, tab_manual = st.tabs(["ğŸ§· ç²˜è´´è¿å•åˆ—è¡¨ï¼ˆæ¨èï¼‰", "ğŸ–±ï¸ é€æ¡é€‰æ‹©"])

            # === å…¬å…±ï¼šåˆ°ä»“æ€»ç®±æ•°æ˜ å°„ï¼ˆé»˜è®¤å€¼ç”¨å®ƒï¼‰ ===
            allowed_map = (
                filtered_df.assign(ç®±æ•°=pd.to_numeric(filtered_df["ç®±æ•°"], errors="coerce"))
                          .groupby("è¿å•å·", as_index=True)["ç®±æ•°"].max()
                          .to_dict()
            )

            # ä¾›â€œæ‰‹åŠ¨é€‰æ‹©â€æ–¹å¼æš‚å­˜
            entries = []

            # ===== æ–¹å¼ä¸€ï¼šç²˜è´´è¿å•å· =====
            with tab_paste:
                st.caption("ä» Excel å¤åˆ¶æ•´åˆ—è¿å•å·ï¼Œç›´æ¥ç²˜è´´åˆ°ä¸‹é¢ï¼ˆæ”¯æŒæ¢è¡Œ/é€—å·/åˆ¶è¡¨ç¬¦ï¼‰ï¼Œä¼šè‡ªåŠ¨å»é‡å¹¶è¿‡æ»¤ä¸åœ¨å½“å‰ä»“/æ—¥æœŸèŒƒå›´å†…çš„è¿å•ã€‚")
                pasted = st.text_area(
                    "ç²˜è´´è¿å•å·",
                    key=f"pasted_wb_{pallet_id}",
                    height=120,
                    help="ç¤ºä¾‹ï¼š\nUSSH2025...\nUSSH2025...\næˆ–ç”¨é€—å·/åˆ¶è¡¨ç¬¦åˆ†éš”"
                )
                if st.form_submit_button("ğŸ” è§£æè¿å•", use_container_width=True):
                    raw_tokens = re.split(r"[,\s\t\r\n]+", pasted.strip())
                    tokens = [t.strip() for t in raw_tokens if t.strip()]
                    valid_set = set(filtered_df["è¿å•å·"].dropna().astype(str))

                    valid_list, seen = [], set()
                    for t in tokens:
                        if t in valid_set and t not in seen:
                            valid_list.append(t); seen.add(t)
                    invalid_list = [t for t in tokens if t not in valid_set]

                    # é»˜è®¤ç®±æ•° = åˆ°ä»“â€œç®±æ•°â€ï¼›å¯ç¼–è¾‘ï¼›ä¸æ˜¾ç¤ºâ€œå¯åˆ†é…å‰©ä½™â€
                    init_rows = []
                    for t in valid_list:
                        allowed_qty = int(pd.to_numeric(allowed_map.get(t, 0), errors="coerce") or 0)
                        init_rows.append({
                            "è¿å•å·": t,
                            "ç®±æ•°": allowed_qty if allowed_qty > 0 else 1,
                            "åˆ é™¤": False
                        })

                    df_init = pd.DataFrame(init_rows)
                    st.session_state[f"wb_rows_{pallet_id}"] = df_init

                    if invalid_list:
                        st.warning(f"å·²å¿½ç•¥ {len(invalid_list)} ä¸ªæœªåœ¨å½“å‰ä»“/æ—¥æœŸèŒƒå›´å†…çš„è¿å•ï¼š{', '.join(invalid_list[:5])}{' ...' if len(invalid_list)>5 else ''}")
                    if not valid_list:
                        st.info("æœªè§£æåˆ°æœ‰æ•ˆçš„è¿å•å·ï¼Œè¯·æ£€æŸ¥ç²˜è´´å†…å®¹æˆ–æ—¥æœŸ/ä»“åº“ç­›é€‰ã€‚")

                # æ¸²æŸ“å¯ç¼–è¾‘è¡¨æ ¼
                df_rows = st.session_state.get(f"wb_rows_{pallet_id}")
                if df_rows is not None and not df_rows.empty:
                    edited_df = st.data_editor(
                        df_rows,
                        key=f"wb_editor_{pallet_id}",
                        use_container_width=True,
                        height=260,
                        num_rows="dynamic",
                        column_config={
                            "è¿å•å·": st.column_config.TextColumn(disabled=True),
                            "ç®±æ•°": st.column_config.NumberColumn(step=1, min_value=1),  # å¯æ”¹
                            "åˆ é™¤": st.column_config.CheckboxColumn("åˆ é™¤"),
                        },
                    )
                    st.session_state[f"wb_rows_{pallet_id}"] = edited_df

            # ===== æ–¹å¼äºŒï¼šé€æ¡é€‰æ‹©ï¼ˆä¿ç•™ï¼‰=====
            with tab_manual:

                num_entries = st.number_input(
                    f"æ·»åŠ è¿å•æ•°é‡ - æ‰˜ç›˜ {pallet_id}",
                    min_value=1, step=1, value=1, key=f"num_{pallet_id}"
                )
                for i in range(num_entries):
                    cols = st.columns([3, 1])
                    with cols[0]:
                        wb = st.selectbox(f"è¿å•å· {i+1}", waybills, key=f"wb_{pallet_id}_{i}")
                    with cols[1]:
                        qty = st.number_input("ç®±æ•°", min_value=1, key=f"qty_{pallet_id}_{i}")
                    entries.append((wb, qty))

            # ===== è¡¨å•æäº¤ï¼šç¡®è®¤ç»‘å®šï¼ˆä¼˜å…ˆè¯»å–ç²˜è´´è¡¨æ ¼ï¼›å¦åˆ™ç”¨æ‰‹åŠ¨é€‰æ‹©ï¼‰=====
            submitted = st.form_submit_button(f"ğŸš€ ç¡®è®¤ç»‘å®šæ‰˜ç›˜ {pallet_id}", use_container_width=True)

        # è¡¨å•å¤–å¤„ç†æäº¤ç»“æœï¼ˆé¿å…é‡å¤æ¸²æŸ“ï¼‰
        if submitted:
            # æ„é€ åˆå¹¶åçš„ {è¿å•: æ•°é‡}
            grouped_entries = {}
            pasted_df = st.session_state.get(f"wb_rows_{pallet_id}")
            if pasted_df is not None and not pasted_df.empty:
                df_use = pasted_df[pasted_df.get("åˆ é™¤", False) == False].copy()
                for _, r in df_use.iterrows():
                    wb = str(r.get("è¿å•å·", "")).strip()
                    qty = int(pd.to_numeric(r.get("ç®±æ•°", 0), errors="coerce") or 0)
                    if not wb or qty <= 0:
                        continue
                    grouped_entries[wb] = grouped_entries.get(wb, 0) + qty
            else:
                for wb, qty in entries:
                    wb = str(wb).strip()
                    grouped_entries[wb] = grouped_entries.get(wb, 0) + int(qty)

            # æ ¡éªŒï¼šè¯»å–â€œå·²åˆ†é…â€ï¼ˆå·²ä¸Šä¼  + æœ¬åœ°ï¼‰
            allocated_uploaded = load_uploaded_allocations(warehouse)
            allocated_local = {}
            for r in st.session_state.get("pallet_detail_records", []):
                if r.get("ä»“åº“ä»£ç ") != warehouse:
                    continue
                wb2 = str(r.get("è¿å•å·", "")).strip()
                if not wb2:
                    continue
                allocated_local[wb2] = allocated_local.get(wb2, 0) + int(pd.to_numeric(r.get("ç®±æ•°", 0), errors="coerce") or 0)

            allocated_map = {}
            for wb_, v in allocated_uploaded.items():
                allocated_map[wb_] = allocated_map.get(wb_, 0) + int(v)
            for wb_, v in allocated_local.items():
                allocated_map[wb_] = allocated_map.get(wb_, 0) + int(v)

            # allowed_map å¤ç”¨ form å†…åŒæ ·å£å¾„ï¼ˆåˆ°ä»“æ€»ç®±æ•°ï¼‰
            allowed_map = (
                filtered_df.assign(ç®±æ•°=pd.to_numeric(filtered_df["ç®±æ•°"], errors="coerce"))
                          .groupby("è¿å•å·", as_index=True)["ç®±æ•°"].max()
                          .to_dict()
            )

            violations, missing_info = [], []
            for wb, add_qty in grouped_entries.items():
                allowed = allowed_map.get(wb, None)
                if allowed is None or pd.isna(allowed):
                    missing_info.append(wb)
                    continue
                already = int(allocated_map.get(wb, 0))
                total_after = already + int(add_qty)
                if total_after > int(allowed):
                    violations.append({
                        "è¿å•å·": wb,
                        "åˆ°ä»“ç®±æ•°": int(allowed),
                        "å·²åˆ†é…(å·²ä¸Šä¼ +æœ¬åœ°)": int(already),
                        "æœ¬æ¬¡è¾“å…¥": int(add_qty),
                        "è¶…å‡º": int(total_after - int(allowed)),
                    })

            if missing_info:
                st.warning("ä»¥ä¸‹è¿å•åœ¨ã€åˆ°ä»“æ•°æ®è¡¨ã€æœªæ‰¾åˆ°æœ‰æ•ˆç®±æ•°ï¼Œè·³è¿‡æ ¡éªŒï¼š{}".format(", ".join(missing_info)))

            if violations:
                st.error("âŒ æœ‰è¿å•æœ¬æ¬¡è¾“å…¥ç®±æ•°è¶…å‡ºã€åˆ°ä»“æ•°æ®è¡¨ã€æ€»ç®±æ•°ï¼Œè¯·è°ƒæ•´åå†æäº¤ã€‚")
                st.dataframe(pd.DataFrame(violations), use_container_width=True)
            else:
                # å¹¶æ¿åˆ¤å®šï¼šæŒ‰â€œä¸åŒè¿å•æ•°â€
                is_merged = len([wb for wb, q in grouped_entries.items() if q > 0]) > 1
                detail_type = "å¹¶æ¿æ‰˜ç›˜" if is_merged else "æ™®é€šæ‰˜ç›˜"

                # å†™å…¥æœ¬åœ°æš‚å­˜ï¼ˆåŒä¸€è¿å•åªå†™ä¸€è¡Œï¼‰
                for wb, qty in grouped_entries.items():
                    if qty <= 0:
                        continue
                    row = filtered_df[filtered_df["è¿å•å·"] == wb].iloc[0]
                    record = {
                        "æ‰˜ç›˜å·": pallet_id,
                        "ä»“åº“ä»£ç ": warehouse,
                        "è¿å•å·": wb,
                        "å®¢æˆ·å•å·": row.get("å®¢æˆ·å•å·", ""),
                        "ç®±æ•°": int(qty),
                        "é‡é‡": weight,
                        "é•¿": length,
                        "å®½": width,
                        "é«˜": height,
                        "ETA(åˆ°BCF)": row.get("ETA(åˆ°BCF)", ""),
                        "ç±»å‹": detail_type
                    }
                    st.session_state["pallet_detail_records"].append(record)

                st.success(f"âœ… æ‰˜ç›˜ {pallet_id} ç»‘å®šå®Œæˆï¼ˆ{detail_type}ï¼‰")
                st.session_state["all_pallets"].remove(pallet_id)

# ======= SUBMIT æŒ‰é’®æ”¾å¤§åŠ ç²—é«˜äº®æ ·å¼ï¼ˆå…¨å±€ï¼‰ =======
st.markdown("""
    <style>
    div.stButton > button[kind="secondary"] {
        font-size: 28px !important;
        font-weight: 900 !important;
        padding: 0.8em 1.6em !important;
        background-color: #ff9800 !important;
        color: white !important;
        border-radius: 10px !important;
        border: 3px solid #e65100 !important;
    }
    </style>
""", unsafe_allow_html=True)

# ========== å±•ç¤ºä¸ç¼–è¾‘æ‰˜ç›˜æ˜ç»†ï¼ˆæœ¬åœ°å†…å­˜ï¼Œå¯åˆ é™¤/è‡ªåŠ¨ä¿å­˜ç¼–è¾‘ï¼‰==========
if st.session_state["pallet_detail_records"]:
    st.markdown("### ğŸ“¦ å½“å‰æ‰˜ç›˜æ˜ç»†è®°å½•ï¼ˆä¸Šä¼ å‰å¯ç¼–è¾‘/åˆ é™¤ï¼‰")

    df_preview = pd.DataFrame(st.session_state["pallet_detail_records"]).copy()

    # æƒ¯ç”¨åˆ—é¡ºåº
    base_cols = ["æ‰˜ç›˜å·", "ä»“åº“ä»£ç ", "è¿å•å·", "å®¢æˆ·å•å·",
                 "ç®±æ•°", "é‡é‡", "é•¿", "å®½", "é«˜", "ETA(åˆ°BCF)", "ç±»å‹"]
    for col in base_cols:
        if col not in df_preview.columns:
            df_preview[col] = ""

    df_preview = df_preview[base_cols]

    # æŠŠâ€œåˆ é™¤â€æ”¾åˆ°æœ€åä¸€åˆ—
    if "åˆ é™¤" not in df_preview.columns:
        df_preview["åˆ é™¤"] = False
    else:
        df_preview["åˆ é™¤"] = df_preview["åˆ é™¤"].astype(bool)

    edited_df = st.data_editor(
        df_preview,
        key="preview_editor",
        num_rows="fixed",
        use_container_width=True,
        height=360,
        column_config={
            "æ‰˜ç›˜å·": st.column_config.TextColumn(disabled=True),
            "ä»“åº“ä»£ç ": st.column_config.TextColumn(disabled=True),
            "è¿å•å·": st.column_config.TextColumn(disabled=True),
            "å®¢æˆ·å•å·": st.column_config.TextColumn(),
            "ç®±æ•°": st.column_config.NumberColumn(step=1, min_value=1),
            "é‡é‡": st.column_config.NumberColumn(),
            "é•¿": st.column_config.NumberColumn(),
            "å®½": st.column_config.NumberColumn(),
            "é«˜": st.column_config.NumberColumn(),
            "ETA(åˆ°BCF)": st.column_config.DatetimeColumn(),
            "ç±»å‹": st.column_config.TextColumn(disabled=True),
            "åˆ é™¤": st.column_config.CheckboxColumn("åˆ é™¤"),
        },
    )

    # è‡ªåŠ¨ä¿å­˜ç¼–è¾‘
    updated_records = edited_df.drop(columns=["åˆ é™¤"], errors="ignore").to_dict(orient="records")
    st.session_state["pallet_detail_records"] = updated_records

    # åˆ é™¤æŒ‰é’®
    cdel, _, _ = st.columns([1, 1, 6])
    with cdel:
        if st.button("ğŸ—‘ï¸ åˆ é™¤æ‰€é€‰"):
            to_delete_idx = edited_df.index[edited_df["åˆ é™¤"] == True].tolist()
            if to_delete_idx:
                kept = [r for i, r in enumerate(updated_records) if i not in to_delete_idx]
                st.session_state["pallet_detail_records"] = kept
                st.success(f"å·²åˆ é™¤ {len(to_delete_idx)} æ¡è®°å½•")
                st.rerun()
            else:
                st.info("æœªå‹¾é€‰è¦åˆ é™¤çš„è®°å½•ã€‚")


    # ========== ä¸Šä¼ æ‰˜ç›˜æ˜ç»†åˆ° Google Sheets ==========
    c1, c2, _ = st.columns([2, 2, 6])
    with c1:
        clear_after = st.checkbox("ä¸Šä¼ åæ¸…ç©ºæœ¬åœ°è®°å½•", value=True)
    with c2:
        if st.button("ğŸ“¤ SUBMIT"):
            df_upload = pd.DataFrame(st.session_state["pallet_detail_records"]).copy()

            # ç»Ÿä¸€åˆ—åï¼šå››ä¸ªå°ºå¯¸åˆ—æ”¹åï¼ˆä½ åŸæœ‰é€»è¾‘ï¼‰
            rename_map = {"é‡é‡": "æ‰˜ç›˜é‡é‡", "é•¿": "æ‰˜ç›˜é•¿", "å®½": "æ‰˜ç›˜å®½", "é«˜": "æ‰˜ç›˜é«˜"}
            df_upload.rename(columns=rename_map, inplace=True)

            # ==== æ–°å¢ï¼šæäº¤æ—¶åˆ»ï¼ˆä»¥æ´›æ‰çŸ¶æœ¬åœ°æ—¶é—´ï¼‰ ====
            now_la = datetime.now(ZoneInfo("America/Los_Angeles"))
            df_upload["æ‰˜ç›˜åˆ›å»ºæ—¥æœŸ"] = now_la.strftime("%Y-%m-%d")
            df_upload["æ‰˜ç›˜åˆ›å»ºæ—¶é—´"] = now_la.strftime("%H:%M:%S")

            # æ—¥æœŸåˆ—è½¬å­—ç¬¦ä¸²ï¼ˆå« ETA åˆ—ï¼‰
            dt_cols = df_upload.select_dtypes(include=["datetime64[ns]", "datetime64[ns, UTC]"]).columns.tolist()
            if "ETA(åˆ°BCF)" in df_upload.columns and df_upload["ETA(åˆ°BCF)"].dtype == object:
                df_upload["ETA(åˆ°BCF)"] = pd.to_datetime(df_upload["ETA(åˆ°BCF)"], errors="coerce")
                if "ETA(åˆ°BCF)" not in dt_cols:
                    dt_cols.append("ETA(åˆ°BCF)")
            for c in dt_cols:
                df_upload[c] = df_upload[c].dt.strftime("%Y-%m-%d").fillna("")

            if "ç®±æ•°" in df_upload.columns:
                df_upload["ç®±æ•°"] = pd.to_numeric(df_upload["ç®±æ•°"], errors="coerce").fillna(0).astype(int)

            # è¿½åŠ ä¸Šä¼ ï¼ˆä¸€æ¬¡æ€§ append å¤šè¡Œï¼Œè®¡ä¸€æ¬¡å†™è¯·æ±‚ï¼‰
            try:
                ssheet = get_ws(SHEET_PALLET_DETAIL, "pallet_detail_key")
            except SpreadsheetNotFound:
                # è‹¥ç›®æ ‡è¡¨ä¸å­˜åœ¨åˆ™åˆ›å»º
                ss = _retry(client.create, SHEET_PALLET_DETAIL)
                ssheet = ss.sheet1

            existing = _retry(ssheet.get_all_values)
            if not existing:
                # è¡¨ä¸ºç©ºï¼šç›´æ¥ç”¨å½“å‰ df çš„åˆ—ä½œä¸ºæ–°è¡¨å¤´ï¼ˆåŒ…å«æ–°åŠ çš„ä¸¤åˆ—ï¼‰
                header = df_upload.columns.tolist()
                rows = df_upload.fillna("").values.tolist()
                _retry(ssheet.update, [header] + rows)
            else:
                # è¡¨å·²å­˜åœ¨ï¼šå¦‚ç¼ºå°‘æ–°åˆ—ï¼Œåˆ™æ‰©å±•è¡¨å¤´åˆ°æœ«å°¾
                existing_header = existing[0]

                # åˆå¹¶è¡¨å¤´ï¼ˆä¿ç•™åŸæœ‰é¡ºåºï¼Œåœ¨æœ«å°¾è¡¥é½ df_upload ä¸­çš„æ–°å¢åˆ—ï¼‰
                merged_header = existing_header[:]
                for col in df_upload.columns:
                    if col not in merged_header:
                        merged_header.append(col)

                # è‹¥ header æœ‰å˜åŒ–ï¼Œå…ˆæ›´æ–°ç¬¬ 1 è¡Œçš„è¡¨å¤´åˆ° merged_header
                if merged_header != existing_header:
                    # åªæ›´æ–°è¡¨å¤´è¡Œï¼›A1 æ ä½æ›´æ–°ä¸ºæ›´é•¿çš„è¡¨å¤´æ˜¯å®‰å…¨çš„
                    _retry(ssheet.update, "1:1", [merged_header])

                # æŒ‰ merged_header é¡ºåºç»„ç»‡è¦è¿½åŠ çš„è¡Œï¼›ä¸å­˜åœ¨çš„åˆ—è¡¥ç©º
                tmp = df_upload.copy()
                for col in merged_header:
                    if col not in tmp.columns:
                        tmp[col] = ""
                rows = tmp.reindex(columns=merged_header).fillna("").values.tolist()

                _retry(ssheet.append_rows, rows, value_input_option="USER_ENTERED")

            st.success(f"âœ… å·²è¿½åŠ ä¸Šä¼  {len(df_upload)} æ¡æ‰˜ç›˜æ˜ç»†åˆ°ã€Œ{SHEET_PALLET_DETAIL}ã€")

            if clear_after:
                st.session_state["pallet_detail_records"] = []
                st.rerun()
