# recv_app.py  â€”â€” æ”¶è´§æ‰˜ç›˜ç»‘å®šï¼ˆä¸»æ•°æ®æºï¼šbolè‡ªææ˜ç»† + åˆ°ä»“æ•°æ®è¡¨(ç®±æ•°/ä»“åº“ä»£ç )ï¼‰
import streamlit as st
import pandas as pd
import gspread
from google.oauth2.service_account import Credentials
from gspread.exceptions import SpreadsheetNotFound
from datetime import datetime, timedelta, date
import random
import string
from uuid import uuid4


SCOPES = ["https://www.googleapis.com/auth/spreadsheets", "https://www.googleapis.com/auth/drive"]

def get_gspread_client():
    # 1) Cloudï¼šä¼˜å…ˆä» st.secrets è¯»å–ï¼ˆStreamlit Cloud é…ç½®çš„æœºå¯†ï¼‰
    if "gcp_service_account" in st.secrets:
        sa_info = st.secrets["gcp_service_account"]  # è¿™æ˜¯ä¸€ä¸ª dictï¼ˆæˆ‘ä»¬ç¨ååœ¨ Cloud é‡Œé…ç½®ï¼‰
        creds = Credentials.from_service_account_info(sa_info, scopes=SCOPES)
        return gspread.authorize(creds)
    # 2) æœ¬åœ°ï¼šå…¼å®¹ä½ åŸæ¥çš„ JSON æ–‡ä»¶
    else:
        creds = Credentials.from_service_account_file("service_accounts.json", scopes=SCOPES)
        return gspread.authorize(creds)

client = get_gspread_client()


# ========= è¡¨åé…ç½® =========
SHEET_ARRIVALS_NAME   = "åˆ°ä»“æ•°æ®è¡¨"
SHEET_SHIP_DETAIL     = "bolè‡ªææ˜ç»†"    # å‘è´§appè¿½åŠ çš„æºï¼Œä½œä¸ºæ”¶è´§å±•ç¤ºä¸»æ•°æ®
SHEET_PALLET_DETAIL   = "æ‰˜ç›˜æ˜ç»†è¡¨"      # æ”¶è´§ç«¯ä¸Šä¼ ç›®æ ‡è¡¨ï¼ˆè¿½åŠ ï¼‰

# ========= å·¥å…·å‡½æ•° =========
def excel_serial_to_date(val):
    """æŠŠ Excel æ•°å­—æ—¥æœŸ(å¦‚ 45857) è½¬ä¸º datetimeï¼›éæ³•è¿”å› NaT"""
    try:
        f = float(val)
        return datetime(1899, 12, 30) + timedelta(days=f)
    except Exception:
        return pd.NaT

def generate_pallet_id():
    return "P" + ''.join(random.choices(string.digits, k=3))

# ========= ç¼“å­˜è¯»å– =========
@st.cache_data(ttl=60)
def load_ship_detail_df():
    """
    è¯»å– bolè‡ªææ˜ç»†ï¼ˆå‘è´§æ˜ç»†ï¼‰ï¼Œä½œä¸ºæ”¶è´§å±•ç¤ºçš„ä¸»æ•°æ®æºã€‚
    åªä¿ç•™ï¼šè¿å•å· / å®¢æˆ·å•å· / ETA(åˆ°BCF)ã€‚æ—¥æœŸå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åºåˆ—å·ï¼Œè¿™é‡Œç»Ÿä¸€è§£æä¸º datetimeã€‚
    """
    try:
        ws = client.open(SHEET_SHIP_DETAIL).sheet1
    except SpreadsheetNotFound:
        return pd.DataFrame()

    vals = ws.get_all_values(value_render_option="UNFORMATTED_VALUE",
                             date_time_render_option="SERIAL_NUMBER")
    if not vals:
        return pd.DataFrame()

    header = vals[0]
    rows   = vals[1:]
    df = pd.DataFrame(rows, columns=header)

    # å…œåº•éœ€è¦åˆ—
    for col in ["è¿å•å·", "å®¢æˆ·å•å·", "ETA(åˆ°BCF)"]:
        if col not in df.columns:
            df[col] = pd.NA

    df["è¿å•å·"] = df["è¿å•å·"].astype(str).str.strip()
    df = df[df["è¿å•å·"] != ""]

    # ETA è§£æï¼šå°è¯•åºåˆ—å·ï¼Œå† to_datetime
    parsed_serial = df["ETA(åˆ°BCF)"].apply(excel_serial_to_date)
    fallback      = pd.to_datetime(df["ETA(åˆ°BCF)"], errors="coerce")
    df["ETA(åˆ°BCF)"] = parsed_serial.combine_first(fallback)

    # è‹¥åŒä¸€è¿å•å‡ºç°å¤šè¡Œï¼ˆå‘è´§ç«¯å¯èƒ½å¤šæ¬¡è¿½åŠ ï¼‰ï¼Œä¿ç•™æœ€åä¸€æ¡
    if not df.empty:
        df = df.groupby("è¿å•å·", as_index=False).last()

    return df[["è¿å•å·", "å®¢æˆ·å•å·", "ETA(åˆ°BCF)"]]

@st.cache_data(ttl=60)
def load_arrivals_df():
    """
    è¯»å– åˆ°ä»“æ•°æ®è¡¨ï¼›ä»…ä¿ç•™ï¼šè¿å•å· / ä»“åº“ä»£ç  / ç®±æ•°ã€‚
    """
    ws = client.open(SHEET_ARRIVALS_NAME).sheet1
    data = ws.get_all_values()
    if not data:
        return pd.DataFrame()

    header = [h.replace("\u00A0", " ").replace("\n", "").replace(" ", "") for h in data[0]]
    df = pd.DataFrame(data[1:], columns=header)

    for need in ["è¿å•å·", "ä»“åº“ä»£ç ", "ç®±æ•°"]:
        if need not in df.columns:
            df[need] = pd.NA

    df["è¿å•å·"] = df["è¿å•å·"].astype(str).str.strip()
    df = df.drop_duplicates(subset=["è¿å•å·"])
    # ç®±æ•°è½¬æ•°å€¼ï¼ˆå¯èƒ½ä»éœ€äººå·¥è°ƒæ•´ï¼‰
    df["ç®±æ•°"] = pd.to_numeric(df["ç®±æ•°"], errors="coerce")

    return df[["è¿å•å·", "ä»“åº“ä»£ç ", "ç®±æ•°"]]

# ========= é¡µé¢è®¾ç½® =========
st.set_page_config(page_title="ç‰©æµæ”¶è´§å¹³å°ï¼ˆåŸºäºå‘è´§æ˜ç»†ï¼‰", layout="wide")

st.title("ğŸ“¦ BCF æ”¶è´§æ‰˜ç›˜ç»‘å®šï¼ˆæ•°æ®æºï¼šbolè‡ªææ˜ç»† + åˆ°ä»“ç®±æ•°ï¼‰")

# ========= åˆ·æ–°ç¼“å­˜ =========
tools_l, _ = st.columns([1,6])
with tools_l:
    if st.button("ğŸ”„ åˆ·æ–°æ•°æ®ç¼“å­˜"):
        st.cache_data.clear()
        st.rerun()

# ========= åˆå§‹åŒ–çŠ¶æ€ =========
if "all_pallets" not in st.session_state:
    st.session_state["all_pallets"] = []
if "pallet_detail_records" not in st.session_state:
    st.session_state["pallet_detail_records"] = []

# ========= æ•°æ®åŠ è½½ =========
ship_df    = load_ship_detail_df()   # è¿å•å· / å®¢æˆ·å•å· / ETA(åˆ°BCF)
arrivals   = load_arrivals_df()      # è¿å•å· / ä»“åº“ä»£ç  / ç®±æ•°

if ship_df.empty and arrivals.empty:
    st.warning("æ²¡æœ‰ä» Google Sheets è¯»å–åˆ°æ•°æ®ï¼Œè¯·æ£€æŸ¥è¡¨å/æƒé™ã€‚")
    st.stop()

# ========= åˆå¹¶ï¼ˆä»¥ bolè‡ªææ˜ç»† ä¸ºä¸»ï¼Œå·¦è¿åˆ°ä»“æ•°æ®è¡¨çš„ ä»“åº“ä»£ç  / ç®±æ•°ï¼‰=========
merged_df = ship_df.merge(arrivals, on="è¿å•å·", how="left")
# ç¡®ä¿ ETA(åˆ°BCF) ä¸º datetime
merged_df["ETA(åˆ°BCF)"] = pd.to_datetime(merged_df["ETA(åˆ°BCF)"], errors="coerce")

# ===== æ—¥æœŸç­›é€‰ï¼ˆæŒ‰ ETA(åˆ°BCF)ï¼‰=====
valid_dates = merged_df["ETA(åˆ°BCF)"].dropna()
if valid_dates.empty:
    st.warning("å½“å‰æ•°æ®ä¸­æ²¡æœ‰å¯è§£æçš„ ETA(åˆ°BCF)ã€‚è¯·æ£€æŸ¥æºè¡¨æˆ–åˆ·æ–°ç¼“å­˜ã€‚")
    st.stop()

min_d = valid_dates.min().date()
max_d = valid_dates.max().date()
default_start = max(max_d - timedelta(days=14), min_d)

st.markdown("### ğŸ” æŒ‰ ETA(åˆ°BCF) æ—¥æœŸç­›é€‰")
start_date, end_date = st.date_input(
    "é€‰æ‹©æ—¥æœŸèŒƒå›´ï¼ˆåŒ…å«ç«¯ç‚¹ï¼‰",
    value=(default_start, max_d),
    min_value=min_d,
    max_value=max_d
)

mask_date = merged_df["ETA(åˆ°BCF)"].between(pd.to_datetime(start_date), pd.to_datetime(end_date))
merged_df_by_date = merged_df[mask_date].copy()

# ===== ä»“åº“ç­›é€‰ï¼ˆåŸºäºæ—¥æœŸè¿‡æ»¤åçš„ç»“æœï¼‰=====
warehouse_options = merged_df_by_date["ä»“åº“ä»£ç "].dropna().unique()
if len(warehouse_options) == 0:
    st.warning("å½“å‰æ—¥æœŸèŒƒå›´å†…æ²¡æœ‰ä»“åº“æ•°æ®ï¼Œè¯·è°ƒæ•´æ—¥æœŸèŒƒå›´ã€‚")
    st.stop()

warehouse = st.selectbox("é€‰æ‹©ä»“åº“ä»£ç ï¼š", warehouse_options)

# ===== å±•ç¤ºåˆå¹¶ç»“æœï¼ˆå·²æŒ‰æ—¥æœŸä¸ä»“åº“è¿‡æ»¤ï¼‰=====
display_cols = ["ä»“åº“ä»£ç ", "è¿å•å·", "å®¢æˆ·å•å·", "ETA(åˆ°BCF)", "ç®±æ•°"]
use_cols = [c for c in display_cols if c in merged_df_by_date.columns]
filtered_df = merged_df_by_date[merged_df_by_date["ä»“åº“ä»£ç "] == warehouse]
filtered_df = filtered_df[use_cols].sort_values(by=["ETA(åˆ°BCF)", "è¿å•å·"], na_position="last")

st.markdown("### ğŸ“‹ å·²åˆ° BCF çš„å¾…æ”¶è´§è¿å•ï¼ˆå·²æŒ‰æ—¥æœŸä¸ä»“åº“è¿‡æ»¤ï¼‰")
st.dataframe(filtered_df, use_container_width=True, height=320)

# ========== æ‰˜ç›˜ç»‘å®šé€»è¾‘ ==========
st.markdown("### ğŸ§° æ‰˜ç›˜æ“ä½œ")
if st.button("â• æ–°å»ºæ‰˜ç›˜"):
    new_pallet = generate_pallet_id()
    while new_pallet in st.session_state["all_pallets"]:
        new_pallet = generate_pallet_id()
    st.session_state["all_pallets"].append(new_pallet)

for pallet_id in list(st.session_state["all_pallets"]):
    with st.expander(f"ğŸ“¦ æ‰˜ç›˜ {pallet_id} æ“ä½œåŒº", expanded=True):
        st.markdown(f"ğŸšš å½“å‰æ‰˜ç›˜å·ï¼š**{pallet_id}**")
        waybills = filtered_df["è¿å•å·"].dropna().unique()

        num_entries = st.number_input(
            f"æ·»åŠ è¿å•æ•°é‡ - æ‰˜ç›˜ {pallet_id}",
            min_value=1, step=1, value=1, key=f"num_{pallet_id}"
        )

        st.markdown("#### ğŸ“¦ æ‰˜ç›˜æ•´ä½“å°ºå¯¸ï¼ˆç»Ÿä¸€å¡«å†™ä¸€æ¬¡ï¼‰")
        pallet_cols = st.columns(4)
        with pallet_cols[0]:
            weight = st.number_input("æ‰˜ç›˜é‡é‡", min_value=0.0, key=f"weight_{pallet_id}")
        with pallet_cols[1]:
            length = st.number_input("æ‰˜ç›˜é•¿", min_value=0.0, key=f"length_{pallet_id}")
        with pallet_cols[2]:
            width = st.number_input("æ‰˜ç›˜å®½",  min_value=0.0, key=f"width_{pallet_id}")
        with pallet_cols[3]:
            height = st.number_input("æ‰˜ç›˜é«˜",  min_value=0.0, key=f"height_{pallet_id}")

        st.markdown("#### ğŸ“¦ è¿å•æ˜ç»†ï¼ˆæ¯å•å•ç‹¬å¡«å†™ç®±æ•°ï¼‰")
        entries = []
        for i in range(num_entries):
            cols = st.columns([3, 1])
            with cols[0]:
                wb = st.selectbox(f"è¿å•å· {i+1}", waybills, key=f"wb_{pallet_id}_{i}")
            with cols[1]:
                qty = st.number_input("ç®±æ•°", min_value=1, key=f"qty_{pallet_id}_{i}")
            entries.append((wb, qty))

        if st.button(f"ğŸš€ ç¡®è®¤ç»‘å®šæ‰˜ç›˜ {pallet_id}"):
            is_merged = len(entries) > 1
            detail_type = "å¹¶æ¿æ‰˜ç›˜" if is_merged else "æ™®é€šæ‰˜ç›˜"

            for wb, qty in entries:
                row = filtered_df[filtered_df["è¿å•å·"] == wb].iloc[0]
                rid = f"{pallet_id}-{uuid4().hex[:6]}"
                record = {
                    "æ‰˜ç›˜å·": pallet_id,
                    "ä»“åº“ä»£ç ": warehouse,
                    "è¿å•å·": wb,
                    "å®¢æˆ·å•å·": row.get("å®¢æˆ·å•å·", ""),
                    "ç®±æ•°": qty,
                    "é‡é‡": weight,
                    "é•¿": length,
                    "å®½": width,
                    "é«˜": height,
                    "ETA(åˆ°BCF)": row.get("ETA(åˆ°BCF)", ""),
                    "ç±»å‹": detail_type
                }
                st.session_state["pallet_detail_records"].append(record)

            st.success(f"âœ… æ‰˜ç›˜ {pallet_id} ç»‘å®šå®Œæˆï¼ˆ{detail_type}ï¼‰")
            st.session_state["all_pallets"].remove(pallet_id)

# ======= SUBMIT æŒ‰é’®æ”¾å¤§åŠ ç²—é«˜äº®æ ·å¼ =======
st.markdown("""
    <style>
    /* é’ˆå¯¹ä¸Šä¼ åŒºçš„ SUBMIT æŒ‰é’®æ”¾å¤§ + é«˜äº® */
    div.stButton > button[kind="secondary"] {
        font-size: 28px !important;      /* å­—ä½“å¾ˆå¤§ */
        font-weight: 900 !important;     /* åŠ ç²— */
        padding: 0.8em 1.6em !important; /* å†…è¾¹è·å¤§ä¸€ç‚¹ */
        background-color: #ff9800 !important; /* é†’ç›®æ©™è‰²èƒŒæ™¯ */
        color: white !important;         /* ç™½è‰²æ–‡å­— */
        border-radius: 10px !important;  /* åœ†è§’ */
        border: 3px solid #e65100 !important; /* æ·±è‰²è¾¹æ¡† */
    }
    </style>
""", unsafe_allow_html=True)


# ========== å±•ç¤ºä¸ç¼–è¾‘æ‰˜ç›˜æ˜ç»†ï¼ˆæœ¬åœ°å†…å­˜ï¼Œå¯åˆ é™¤/è‡ªåŠ¨ä¿å­˜ç¼–è¾‘ï¼‰==========
if st.session_state["pallet_detail_records"]:
    st.markdown("### ğŸ“¦ å½“å‰æ‰˜ç›˜æ˜ç»†è®°å½•ï¼ˆä¸Šä¼ å‰å¯ç¼–è¾‘/åˆ é™¤ï¼‰")

    df_preview = pd.DataFrame(st.session_state["pallet_detail_records"]).copy()

    # æƒ¯ç”¨åˆ—é¡ºåº
    base_cols = ["æ‰˜ç›˜å·", "ä»“åº“ä»£ç ", "è¿å•å·", "å®¢æˆ·å•å·",
                 "ç®±æ•°", "é‡é‡", "é•¿", "å®½", "é«˜", "ETA(åˆ°BCF)", "ç±»å‹"]
    for col in base_cols:
        if col not in df_preview.columns:
            df_preview[col] = ""

    df_preview = df_preview[base_cols]

    # æŠŠâ€œåˆ é™¤â€æ”¾åˆ°æœ€åä¸€åˆ—
    if "åˆ é™¤" not in df_preview.columns:
        df_preview["åˆ é™¤"] = False
    else:
        df_preview["åˆ é™¤"] = df_preview["åˆ é™¤"].astype(bool)

    edited_df = st.data_editor(
        df_preview,
        key="preview_editor",
        num_rows="fixed",
        use_container_width=True,
        height=360,
        column_config={
            "æ‰˜ç›˜å·": st.column_config.TextColumn(disabled=True),
            "ä»“åº“ä»£ç ": st.column_config.TextColumn(disabled=True),
            "è¿å•å·": st.column_config.TextColumn(disabled=True),
            "å®¢æˆ·å•å·": st.column_config.TextColumn(),
            "ç®±æ•°": st.column_config.NumberColumn(step=1, min_value=1),
            "é‡é‡": st.column_config.NumberColumn(),
            "é•¿": st.column_config.NumberColumn(),
            "å®½": st.column_config.NumberColumn(),
            "é«˜": st.column_config.NumberColumn(),
            "ETA(åˆ°BCF)": st.column_config.DatetimeColumn(),
            "ç±»å‹": st.column_config.TextColumn(disabled=True),
            "åˆ é™¤": st.column_config.CheckboxColumn("åˆ é™¤"),
        },
    )

    # è‡ªåŠ¨ä¿å­˜ç¼–è¾‘
    updated_records = edited_df.drop(columns=["åˆ é™¤"], errors="ignore").to_dict(orient="records")
    st.session_state["pallet_detail_records"] = updated_records

    # åˆ é™¤æŒ‰é’®
    cdel, _, _ = st.columns([1, 1, 6])
    with cdel:
        if st.button("ğŸ—‘ï¸ åˆ é™¤æ‰€é€‰"):
            to_delete_idx = edited_df.index[edited_df["åˆ é™¤"] == True].tolist()
            if to_delete_idx:
                kept = [r for i, r in enumerate(updated_records) if i not in to_delete_idx]
                st.session_state["pallet_detail_records"] = kept
                st.success(f"å·²åˆ é™¤ {len(to_delete_idx)} æ¡è®°å½•")
                st.rerun()
            else:
                st.info("æœªå‹¾é€‰è¦åˆ é™¤çš„è®°å½•ã€‚")

    st.markdown("---")

    # ========== ä¸Šä¼ æ‰˜ç›˜æ˜ç»†åˆ° Google Sheets ==========
    c1, c2, _ = st.columns([2, 2, 6])
    with c1:
        clear_after = st.checkbox("ä¸Šä¼ åæ¸…ç©ºæœ¬åœ°è®°å½•", value=True)
    with c2:
        if st.button("ğŸ“¤ SUBMIT"):
            df_upload = pd.DataFrame(st.session_state["pallet_detail_records"]).copy()

            rename_map = {"é‡é‡": "æ‰˜ç›˜é‡é‡", "é•¿": "æ‰˜ç›˜é•¿", "å®½": "æ‰˜ç›˜å®½", "é«˜": "æ‰˜ç›˜é«˜"}
            df_upload.rename(columns=rename_map, inplace=True)

            # æ—¥æœŸåˆ—è½¬å­—ç¬¦ä¸²
            dt_cols = df_upload.select_dtypes(include=["datetime64[ns]", "datetime64[ns, UTC]"]).columns.tolist()
            if "ETA(åˆ°BCF)" in df_upload.columns and df_upload["ETA(åˆ°BCF)"].dtype == object:
                df_upload["ETA(åˆ°BCF)"] = pd.to_datetime(df_upload["ETA(åˆ°BCF)"], errors="coerce")
                if "ETA(åˆ°BCF)" not in dt_cols:
                    dt_cols.append("ETA(åˆ°BCF)")
            for c in dt_cols:
                df_upload[c] = df_upload[c].dt.strftime("%Y-%m-%d").fillna("")

            if "ç®±æ•°" in df_upload.columns:
                df_upload["ç®±æ•°"] = pd.to_numeric(df_upload["ç®±æ•°"], errors="coerce").fillna(0).astype(int)

            try:
                ss = client.open(SHEET_PALLET_DETAIL)
                sheet = ss.sheet1
            except SpreadsheetNotFound:
                ss = client.create(SHEET_PALLET_DETAIL)
                sheet = ss.sheet1

            existing = sheet.get_all_values()
            if not existing:
                header = df_upload.columns.tolist()
                rows = df_upload.fillna("").values.tolist()
                sheet.update([header] + rows)
            else:
                existing_header = existing[0]
                tmp = df_upload.copy()
                for col in existing_header:
                    if col not in tmp.columns:
                        tmp[col] = ""
                rows = tmp.reindex(columns=existing_header).fillna("").values.tolist()
                sheet.append_rows(rows, value_input_option="USER_ENTERED")

            st.success(f"âœ… å·²è¿½åŠ ä¸Šä¼  {len(df_upload)} æ¡æ‰˜ç›˜æ˜ç»†åˆ°ã€Œ{SHEET_PALLET_DETAIL}ã€")

            if clear_after:
                st.session_state["pallet_detail_records"] = []
                st.rerun()
