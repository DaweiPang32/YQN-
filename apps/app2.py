# ğŸšš å‘è´§è°ƒåº¦ï¼ˆä¸¥æ ¼ USSH è¿å•å· + å®¢æˆ·å•å·ä»…æ¥è‡ªã€åˆ°ä»“æ•°æ®è¡¨ã€ï¼‰

import streamlit as st
import pandas as pd
import numpy as np
import gspread
from google.oauth2.service_account import Credentials
from gspread.exceptions import SpreadsheetNotFound, APIError
from googleapiclient.discovery import build
from streamlit.errors import StreamlitAPIException
from googleapiclient.errors import HttpError
from datetime import datetime, timedelta, date
import calendar
import re
import random, time
import math

# ========= æˆæƒèŒƒå›´ =========
SCOPES = ["https://www.googleapis.com/auth/spreadsheets", "https://www.googleapis.com/auth/drive"]

if not st.session_state.get("_page_configured", False):
    try:
        st.set_page_config(page_title="å‘è´§è°ƒåº¦", layout="wide")
    except StreamlitAPIException:
        pass
    st.session_state["_page_configured"] = True

# ========= é¢„ç¼–è¯‘æ­£åˆ™ =========
_RE_PARENS = re.compile(r"[\(\ï¼ˆ][\s\S]*?[\)\ï¼‰]", re.DOTALL)
_RE_SPLIT = re.compile(r"[,\ï¼Œ;\ï¼›ã€\|\/\s]+")
_RE_NUM = re.compile(r'[-+]?\d+(?:\.\d+)?')
_IP_TOKEN   = re.compile(r"\bIP\d+\b", flags=re.IGNORECASE)
_SEP_INNER  = re.compile(r"[,\ï¼Œ;\ï¼›\|/]+")
# âœ… ä¸¥æ ¼åªè®¤ USSH + 12 ä½æ•°å­—ï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
_WB_USSH_REGEX = re.compile(r"\bUSSH\d{12}\b", flags=re.IGNORECASE)

def _extract_wb_ushh_only(mixed: str) -> list[str]:
    """ä»…è¯†åˆ« USSH + 12ä½æ•°å­—ä¸ºè¿å•å·ï¼›ç»Ÿä¸€è¾“å‡ºä¸ºå¤§å†™å»é‡ä¿åºã€‚"""
    if _is_blank(mixed):
        return []
    hits = _WB_USSH_REGEX.findall(str(mixed))
    out, seen = [], set()
    for t in hits:
        t_norm = t.upper()
        if t_norm not in seen:
            seen.add(t_norm)
            out.append(t_norm)
    return out

def _has_multi_ip_in_parens(text: str) -> bool:
    if _is_blank(text):
        return False
    s = str(text)
    def _count_in_pair(open_ch, close_ch, src):
        depth, buf, hits = 0, [], 0
        for ch in src:
            if ch == open_ch:
                if depth == 0:
                    buf = []
                depth += 1
            elif ch == close_ch and depth > 0:
                depth -= 1
                if depth == 0:
                    inner = "".join(buf)
                    if len(_IP_TOKEN.findall(inner)) >= 2:
                        hits += 1
                    buf = []
            else:
                if depth > 0:
                    buf.append(ch)
        return hits
    return (_count_in_pair("(", ")", s) > 0) or (_count_in_pair("ï¼ˆ", "ï¼‰", s) > 0)

def _normalize_ip_list_in_parens(text: str) -> str:
    """ä»…å½“æ‹¬å·é‡Œå‡ºç° â‰¥2 ä¸ª IPxxxx æ—¶ï¼Œå°†åˆ†éš”ç¬¦ç»Ÿä¸€ä¸ºç©ºæ ¼ï¼›å¦åˆ™ä¿æŒåŸæ ·ã€‚"""
    if _is_blank(text):
        return ""
    s = str(text)
    if not _has_multi_ip_in_parens(s):
        return s
    def _do_for_pair(open_ch, close_ch, src):
        out, buf, depth = [], [], 0
        for ch in src:
            if ch == open_ch:
                if depth == 0:
                    buf = []
                depth += 1
                out.append(ch)
            elif ch == close_ch and depth > 0:
                depth -= 1
                inner = "".join(buf)
                if len(_IP_TOKEN.findall(inner)) >= 2:
                    inner = _SEP_INNER.sub(" ", inner)
                    inner = re.sub(r"\s{2,}", " ", inner).strip()
                out.append(inner)
                out.append(ch)
                buf = []
            else:
                if depth > 0:
                    buf.append(ch)
                else:
                    out.append(ch)
        return "".join(out)
    s = _do_for_pair("(", ")", s)
    s = _do_for_pair("ï¼ˆ", "ï¼‰", s)
    return s

def _split_tokens(s: str) -> list[str]:
    if not isinstance(s, str):
        s = str(s) if s is not None else ""
    return [t for t in _RE_SPLIT.split(s) if t]

def _remove_parens_iter(s: str) -> str:
    if not isinstance(s, str) or not s:
        return ""
    prev = None
    out = s
    while prev != out:
        prev = out
        out = re.sub(r"\([^()]*\)", "", out)
        out = re.sub(r"ï¼ˆ[^ï¼ˆï¼‰]*ï¼‰", "", out)
    return out

def _first_balanced_paren_content(s: str) -> str | None:
    if not isinstance(s, str) or not s:
        return None
    start = s.find("(")
    if start != -1:
        depth = 0
        for i in range(start, len(s)):
            ch = s[i]
            if ch == "(":
                depth += 1
            elif ch == ")":
                depth -= 1
                if depth == 0:
                    return s[start+1:i].strip()
    start = s.find("ï¼ˆ")
    if start != -1:
        depth = 0
        for i in range(start, len(s)):
            ch = s[i]
            if ch == "ï¼ˆ":
                depth += 1
            elif ch == "ï¼‰":
                depth -= 1
                if depth == 0:
                    return s[start+1:i].strip()
    return None

# ========= å®¢æˆ·ç«¯å¤ç”¨ =========
@st.cache_resource
def get_clients():
    if "gcp_service_account" in st.secrets:
        sa_info = st.secrets["gcp_service_account"]
        creds = Credentials.from_service_account_info(sa_info, scopes=SCOPES)
    else:
        creds = Credentials.from_service_account_file("service_accounts.json", scopes=SCOPES)
    gc = gspread.authorize(creds)
    svc = build("sheets", "v4", credentials=creds, cache_discovery=False)
    return gc, svc

def get_gspread_client():
    gc, _ = get_clients()
    return gc

client, sheets_service = get_clients()

# ========= è¡¨åé…ç½® =========
SHEET_ARRIVALS_NAME   = "åˆ°ä»“æ•°æ®è¡¨"
SHEET_PALLET_DETAIL   = "æ‰˜ç›˜æ˜ç»†è¡¨"
SHEET_SHIP_TRACKING   = "å‘è´§è¿½è¸ª"
SHEET_BOL_DETAIL      = "bolè‡ªææ˜ç»†"
SHEET_WB_SUMMARY      = "è¿å•å…¨é“¾è·¯æ±‡æ€»"

# ========= é€šç”¨å·¥å…· =========
def _norm_header(cols):
    return [c.replace("\u00A0"," ").replace("\n","").strip().replace(" ","") for c in cols]

def _to_num(x):
    try: return float(str(x).strip())
    except: return None

def _to_num_safe(x):
    try:
        s = str(x).strip().replace(",","")
        s = re.sub(r"[^\d\.\-]", "", s)
        return float(s)
    except:
        return None

def _is_blank(v):
    try:
        if v is None: return True
        if pd.isna(v): return True
        if isinstance(v, str) and v.strip()=="": return True
        return False
    except Exception:
        try: return bool(pd.isna(v))
        except Exception: return False

def _pack_ranges_for_col(ws_title: str, col_idx_1based: int, rowvals: list[tuple[int, list]]):
    updates = []
    if not rowvals:
        return updates
    s = p = rowvals[0][0]
    buf = [rowvals[0][1]]
    for r, v in rowvals[1:]:
        if r == p + 1:
            p = r
            buf.append(v)
        else:
            a1s = gspread.utils.rowcol_to_a1(s, col_idx_1based)
            a1e = gspread.utils.rowcol_to_a1(p, col_idx_1based)
            updates.append({"range": f"{ws_title}!{a1s}:{a1e}", "values": buf})
            s = p = r
            buf = [v]
    a1s = gspread.utils.rowcol_to_a1(s, col_idx_1based)
    a1e = gspread.utils.rowcol_to_a1(p, col_idx_1based)
    updates.append({"range": f"{ws_title}!{a1s}:{a1e}", "values": buf})
    return updates

# ==== é€€é¿é‡è¯•è¯»å– ====
def _safe_get_all_values(ws, value_render_option="UNFORMATTED_VALUE", date_time_render_option="SERIAL_NUMBER"):
    backoffs = [0.5, 1.0, 2.0, 4.0, 8.0]
    for i, delay in enumerate([0.0] + backoffs):
        if delay:
            time.sleep(delay + random.random()*0.2)
        try:
            return ws.get_all_values(
                value_render_option=value_render_option,
                date_time_render_option=date_time_render_option
            )
        except APIError as e:
            msg = str(e)
            if ("Quota exceeded" in msg) or ("Read requests per minute" in msg) or ("429" in msg):
                if i < len(backoffs):
                    continue
            raise

def _with_backoff(fn, *args, **kwargs):
    delays = [0.3, 0.6, 1.2, 2.4, 4.8, 6.0]
    last_err = None
    for i, d in enumerate([0.0] + delays):
        if d > 0:
            time.sleep(d + random.random() * 0.2)
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            msg = str(e)
            if ("429" in msg) or ("Quota" in msg) or ("quota" in msg):
                last_err = e
                continue
            raise
    if last_err:
        raise last_err

def _bust(name: str):
    key = f"_bust_{name}"
    st.session_state[key] = int(st.session_state.get(key, 0)) + 1
    return st.session_state[key]

def _get_bust(name: str) -> int:
    return int(st.session_state.get(f"_bust_{name}", 0))

def _norm_waybill_str(v):
    if _is_blank(v): return ""
    s = str(v).strip()
    if s.endswith(".0"): s = s[:-2]
    try:
        f = float(s)
        if abs(f - round(f)) < 1e-9: s = str(int(round(f)))
    except: pass
    return s

_BASE = datetime(1899, 12, 30)

def _coerce_excel_serial_sum(v):
    try:
        if isinstance(v, (int, float)) and not pd.isna(v):
            return float(v)
    except Exception:
        pass
    if isinstance(v, str):
        s = v.strip()
        s = re.sub(r'[\u00A0\u2000-\u200B]', ' ', s)
        s = s.replace(',', '.')
        nums = _RE_NUM.findall(s)
        total, ok = 0.0, False
        for n in nums:
            try:
                total += float(n); ok = True
            except Exception: pass
        if ok: return total
    if isinstance(v, (list, tuple)):
        total, ok = 0.0, False
        for x in v:
            if x is None or (isinstance(x, float) and pd.isna(x)):
                continue
            try:
                xs = str(x).strip().replace(',', '.')
                total += float(xs); ok = True
            except Exception: pass
        if ok: return total
    return None

def _parse_sheet_value_to_date(v):
    if _is_blank(v): return None
    if isinstance(v, str):
        s = v.strip()
        if any(tok in s for tok in ["-", "/", "å¹´", "æœˆ", "æ—¥", ":"]):
            dt = pd.to_datetime(s, errors="coerce")
            if pd.notna(dt): return dt.date()
    serial = _coerce_excel_serial_sum(v)
    if serial is not None:
        try:
            dt = _BASE + timedelta(days=float(serial))
            return dt.date()
        except Exception: pass
    try:
        dt = pd.to_datetime(v, errors="coerce")
        if pd.isna(dt): return None
        return dt.date()
    except Exception:
        return None

def _excel_serial_to_dt(v):
    if _is_blank(v): return None
    if isinstance(v, str):
        s = v.strip()
        if any(tok in s for tok in ["-", "/", "å¹´", "æœˆ", "æ—¥", ":"]):
            ts = pd.to_datetime(s, errors="coerce")
            if pd.notna(ts): return ts.to_pydatetime()
    serial = _coerce_excel_serial_sum(v)
    if serial is not None:
        try:
            return _BASE + timedelta(days=float(serial))
        except Exception: pass
    try:
        ts = pd.to_datetime(v, errors="coerce")
        if pd.isna(ts): return None
        return ts.to_pydatetime()
    except Exception:
        return None

def _fmt_date(d: date, out_fmt="%Y-%m-%d"):
    return d.strftime(out_fmt) if isinstance(d, date) else ""

def _parse_time_window_days(win: str):
    if not isinstance(win, str): return (None, None)
    s = win.strip()
    if "-" not in s: return (None, None)
    a, b = s.split("-", 1)
    try:
        sa, sb = int(a), int(b)
        if 1 <= sa <= 31 and 1 <= sb <= 31 and sa <= sb:
            return (sa, sb)
    except: pass
    return (None, None)

def _clamp_dom(year, month, dom):
    last = calendar.monthrange(year, month)[1]
    dom = max(1, min(last, dom))
    return date(year, month, dom)

def _promise_diff_days_str(win: str, anchor: date | None):
    sa, sb = _parse_time_window_days(win)
    if sa is None: return ""
    today = date.today()
    if anchor is None: anchor = today
    y, m = anchor.year, anchor.month
    start_d = _clamp_dom(y, m, sa)
    end_d   = _clamp_dom(y, m, sb)
    x = (start_d - today).days
    y2 = (end_d   - today).days
    return f"{x}-{y2}"

def _fmt_time_from_any(v, out_fmt="%H:%M"):
    dt = _excel_serial_to_dt(v)
    return dt.strftime(out_fmt) if isinstance(dt, datetime) else ""

def _split_dt_to_date_time_str(date_raw, time_raw):
    d_dt = _excel_serial_to_dt(date_raw)
    t_dt = _excel_serial_to_dt(time_raw)
    if isinstance(d_dt, datetime):
        date_str = d_dt.date().strftime("%Y-%m-%d")
    elif isinstance(t_dt, datetime):
        date_str = date.today().strftime("%Y-%m-%d")
    else:
        date_str = ""
    time_str = ""
    if isinstance(t_dt, datetime):
        time_str = t_dt.strftime("%H:%M")
    elif isinstance(d_dt, datetime):
        time_str = d_dt.strftime("%H:%M")
    return date_str, time_str

def _split_waybill_list(s):
    if _is_blank(s): return []
    parts = re.split(r"[,\ï¼Œ;\ï¼›ã€\|\/\s]+", str(s))
    return [_norm_waybill_str(p) for p in parts if _norm_waybill_str(p)]

def _first_nonblank_str(s):
    for x in s:
        if not _is_blank(x):
            return str(x).strip()
    return ""

# ========= è½»é‡ç­¾å =========
@st.cache_data(ttl=10)
def _sheet_row_sig(sheet_name: str, _bust=0) -> tuple[int, int]:
    try:
        ws = client.open(sheet_name).sheet1
    except SpreadsheetNotFound:
        return (0, 0)
    vals = _safe_get_all_values(ws, "UNFORMATTED_VALUE", "SERIAL_NUMBER")
    if not vals:
        return (0, 0)
    rows = len(vals)
    cols = max((len(r) for r in vals), default=0)
    return (rows, cols)

@st.cache_data(ttl=300)
def load_bol_pickup_map(_bust=0) -> dict:
    try:
        ws = client.open(SHEET_BOL_DETAIL).sheet1
    except SpreadsheetNotFound:
        return {}
    vals = _safe_get_all_values(ws, "UNFORMATTED_VALUE", "SERIAL_NUMBER")
    if not vals:
        return {}
    header = _norm_header(vals[0])
    df = pd.DataFrame(vals[1:], columns=header) if len(vals) > 1 else pd.DataFrame(columns=header)
    col_wb = next((c for c in ["è¿å•å·","waybill","Waybill"] if c in df.columns), None)
    col_pk = next((c for c in ["è‡ªæä»“åº“","è‡ªæä»“","pickup","Pickup"] if c in df.columns), None)
    if not col_wb or not col_pk:
        return {}
    df[col_wb] = df[col_wb].apply(_norm_waybill_str)
    df[col_pk] = df[col_pk].astype(str).str.strip()
    df = df[(df[col_wb] != "") & (df[col_pk] != "")]
    df = df.drop_duplicates(subset=[col_wb], keep="last")
    return dict(zip(df[col_wb], df[col_pk]))

@st.cache_data(ttl=30)
def load_arrivals_df(_bust=0) -> pd.DataFrame:
    """
    è¾“å‡ºåˆ—ï¼š
      è¿å•å·ã€ä»“åº“ä»£ç ã€æ”¶è´¹é‡ã€ä½“ç§¯ã€ETA/ATAã€ETD/ATDã€å¯¹å®¢æ‰¿è¯ºé€ä»“æ—¶é—´ã€é¢„è®¡åˆ°ä»“æ—¶é—´ï¼ˆæ—¥ï¼‰ã€å®¢æˆ·å•å·ã€_ETAATA_date
    """
    try:
        ws = client.open(SHEET_ARRIVALS_NAME).sheet1
    except SpreadsheetNotFound:
        return pd.DataFrame(columns=[
            "è¿å•å·","ä»“åº“ä»£ç ","æ”¶è´¹é‡","ä½“ç§¯",
            "ETA/ATA","ETD/ATD","å¯¹å®¢æ‰¿è¯ºé€ä»“æ—¶é—´","é¢„è®¡åˆ°ä»“æ—¶é—´ï¼ˆæ—¥ï¼‰",
            "å®¢æˆ·å•å·","_ETAATA_date"
        ])
    vals = _safe_get_all_values(ws, "UNFORMATTED_VALUE", "SERIAL_NUMBER")
    if not vals or not vals[0]:
        return pd.DataFrame(columns=[
            "è¿å•å·","ä»“åº“ä»£ç ","æ”¶è´¹é‡","ä½“ç§¯",
            "ETA/ATA","ETD/ATD","å¯¹å®¢æ‰¿è¯ºé€ä»“æ—¶é—´","é¢„è®¡åˆ°ä»“æ—¶é—´ï¼ˆæ—¥ï¼‰",
            "å®¢æˆ·å•å·","_ETAATA_date"
        ])
    header = _norm_header(vals[0])
    df = pd.DataFrame(vals[1:], columns=header)

    if "è¿å•å·" not in df.columns: df["è¿å•å·"] = pd.NA
    if "ä»“åº“ä»£ç " not in df.columns: df["ä»“åº“ä»£ç "] = pd.NA
    if "æ”¶è´¹é‡" not in df.columns: df["æ”¶è´¹é‡"] = pd.NA

    vol_col = next((c for c in ["ä½“ç§¯","CBM","ä½“ç§¯m3","ä½“ç§¯(m3)","ä½“ç§¯ï¼ˆm3ï¼‰"] if c in df.columns), None)
    if vol_col is None:
        df["ä½“ç§¯"] = pd.NA
    else:
        df["ä½“ç§¯"] = pd.to_numeric(df[vol_col], errors="coerce")

    etaata_src = None
    for cand in ["ETA/ATA","ETAATA"]:
        if cand in df.columns:
            etaata_src = cand; break

    if "ETD/ATD" not in df.columns: df["ETD/ATD"] = pd.NA
    if "å¯¹å®¢æ‰¿è¯ºé€ä»“æ—¶é—´" not in df.columns: df["å¯¹å®¢æ‰¿è¯ºé€ä»“æ—¶é—´"] = pd.NA

    eta_wh_col = next((c for c in ["é¢„è®¡åˆ°ä»“æ—¶é—´ï¼ˆæ—¥ï¼‰","é¢„è®¡åˆ°ä»“æ—¶é—´(æ—¥)","é¢„è®¡åˆ°ä»“æ—¶é—´æ—¥"] if c in df.columns), None)
    if eta_wh_col is None:
        df["é¢„è®¡åˆ°ä»“æ—¶é—´ï¼ˆæ—¥ï¼‰"] = pd.NA
        eta_wh_col = "é¢„è®¡åˆ°ä»“æ—¶é—´ï¼ˆæ—¥ï¼‰"

    # âœ… ç¡®ä¿å®¢æˆ·å•å·åˆ—å­˜åœ¨
    if "å®¢æˆ·å•å·" not in df.columns:
        df["å®¢æˆ·å•å·"] = pd.NA

    # è§„èŒƒåŒ–
    df["è¿å•å·"] = df["è¿å•å·"].apply(_norm_waybill_str)
    df["ä»“åº“ä»£ç "] = df["ä»“åº“ä»£ç "].astype(str).str.strip()
    df["æ”¶è´¹é‡"] = pd.to_numeric(df["æ”¶è´¹é‡"], errors="coerce")

    if etaata_src is not None:
        df["_ETAATA_date"] = df[etaata_src].apply(_parse_sheet_value_to_date)
        df["ETA/ATA"] = df["_ETAATA_date"].apply(_fmt_date).replace("", pd.NA)
    else:
        df["_ETAATA_date"] = pd.NA
        df["ETA/ATA"] = pd.NA

    df["_ETD_ATD_date"] = df["ETD/ATD"].apply(_parse_sheet_value_to_date)
    df["ETD/ATD"] = df["_ETD_ATD_date"].apply(_fmt_date).replace("", pd.NA)

    df["_ETA_WH_date"] = df[eta_wh_col].apply(_parse_sheet_value_to_date)
    df["é¢„è®¡åˆ°ä»“æ—¶é—´ï¼ˆæ—¥ï¼‰"] = df["_ETA_WH_date"].apply(_fmt_date).replace("", pd.NA)

    df = df.drop_duplicates(subset=["è¿å•å·"], keep="last")

    keep = ["ä»“åº“ä»£ç ","è¿å•å·","æ”¶è´¹é‡","ä½“ç§¯",
            "ETA/ATA","ETD/ATD","å¯¹å®¢æ‰¿è¯ºé€ä»“æ—¶é—´","é¢„è®¡åˆ°ä»“æ—¶é—´ï¼ˆæ—¥ï¼‰",
            "å®¢æˆ·å•å·",
            "_ETAATA_date"]
    for c in keep:
        if c not in df.columns:
            df[c] = pd.NA
    return df[keep]

@st.cache_data(ttl=300)
def load_waybill_summary_df(_bust=0):
    try:
        ws = client.open(SHEET_WB_SUMMARY).sheet1
    except SpreadsheetNotFound:
        st.error(f"æ‰¾ä¸åˆ°å·¥ä½œè¡¨ã€Œ{SHEET_WB_SUMMARY}ã€ã€‚")
        return pd.DataFrame(), None, []
    vals = _safe_get_all_values(ws, "UNFORMATTED_VALUE", "SERIAL_NUMBER")
    if not vals:
        st.warning("ã€è¿å•å…¨é“¾è·¯æ±‡æ€»ã€ä¸ºç©ºã€‚")
        return pd.DataFrame(), ws, []
    header_raw = vals[0]
    df = pd.DataFrame(vals[1:], columns=header_raw) if len(vals) > 1 else pd.DataFrame(columns=header_raw)

    def pick(colnames, cands):
        for c in cands:
            if c in colnames:
                return c
        return None

    col_wb   = pick(df.columns, ["è¿å•å·","Waybill"])
    col_wh   = pick(df.columns, ["ä»“åº“ä»£ç ","ä»“åº“"])
    col_trk  = pick(df.columns, ["å‘èµ°å¡è½¦å·","å‘èµ°è½¦å·","å‘èµ°å¡è½¦","å¡è½¦å·","TruckNo","Truck"])
    col_ship = pick(df.columns, ["å‘èµ°æ—¥æœŸ","å‘è´§æ—¥æœŸ","å‡ºä»“æ—¥æœŸ"])
    col_eta  = pick(df.columns, ["åˆ°ä»“æ—¥æœŸ","åˆ°ä»“æ—¥","åˆ°ä»“(wh)"])

    if col_wb   is None: df["è¿å•å·"]   = ""; col_wb   = "è¿å•å·"
    if col_wh   is None: df["ä»“åº“ä»£ç "] = ""; col_wh   = "ä»“åº“ä»£ç "
    if col_trk  is None: df["å‘èµ°å¡è½¦å·"] = ""; col_trk  = "å‘èµ°å¡è½¦å·"
    if col_ship is None: df["å‘èµ°æ—¥æœŸ"]  = ""; col_ship = "å‘èµ°æ—¥æœŸ"
    if col_eta  is None: df["åˆ°ä»“æ—¥æœŸ"]  = ""; col_eta  = "åˆ°ä»“æ—¥æœŸ"

    df_work = df.rename(columns={
        col_wb: "è¿å•å·",
        col_wh: "ä»“åº“ä»£ç ",
        col_trk: "å‘èµ°å¡è½¦å·",
        col_ship: "å‘èµ°æ—¥æœŸ",
        col_eta: "åˆ°ä»“æ—¥æœŸ",
    }).copy()

    df_work["_rowno"] = np.arange(2, 2 + len(df_work))
    df_work["_å‘èµ°æ—¥æœŸ_dt"] = df_work["å‘èµ°æ—¥æœŸ"].apply(_parse_sheet_value_to_date)
    df_work["_åˆ°ä»“æ—¥æœŸ_dt"] = df_work["åˆ°ä»“æ—¥æœŸ"].apply(_parse_sheet_value_to_date)

    df_work["ä»“åº“ä»£ç "] = df_work["ä»“åº“ä»£ç "].astype(str).str.strip()
    df_work["å‘èµ°å¡è½¦å·"] = df_work["å‘èµ°å¡è½¦å·"].astype(str).str.strip()

    return df_work, ws, header_raw

@st.cache_data(ttl=300)
def load_pallet_detail_df(arrivals_df: pd.DataFrame | None = None,
                          bol_cost_df: pd.DataFrame | None = None,
                          _bust=0,
                          refresh_token: int = 0) -> pd.DataFrame:
    ws = client.open(SHEET_PALLET_DETAIL).sheet1
    vals = _safe_get_all_values(ws, "UNFORMATTED_VALUE", "SERIAL_NUMBER")
    if not vals:
        return pd.DataFrame()

    header = _norm_header(vals[0])
    df = pd.DataFrame(vals[1:], columns=header)

    if "æ‰˜ç›˜å·" not in df.columns:
        for cand in ["æ‰˜ç›˜ID","æ‰˜ç›˜ç¼–å·","PalletID","PalletNo","palletid","palletno"]:
            if cand in df.columns:
                df = df.rename(columns={cand: "æ‰˜ç›˜å·"})
                break
    if "æ‰˜ç›˜å·" not in df.columns:
        df["æ‰˜ç›˜å·"] = pd.NA
    if "ä»“åº“ä»£ç " not in df.columns:
        df["ä»“åº“ä»£ç "] = pd.NA
    if "è¿å•å·" not in df.columns:
        for cand in ["Waybill","waybill","è¿å•ç¼–å·"]:
            if cand in df.columns:
                df = df.rename(columns={cand: "è¿å•å·"})
                break
    if "è¿å•å·" not in df.columns:
        df["è¿å•å·"] = pd.NA

    df["æ‰˜ç›˜å·"]   = df["æ‰˜ç›˜å·"].astype(str).str.strip()
    df["ä»“åº“ä»£ç "] = df["ä»“åº“ä»£ç "].astype(str).str.strip()
    df["è¿å•å·"]   = df["è¿å•å·"].apply(_norm_waybill_str)

    weight_col = None
    for cand in ["æ‰˜ç›˜é‡é‡","æ‰˜ç›˜é‡","æ”¶è´¹é‡","æ‰˜ç›˜æ”¶è´¹é‡","è®¡è´¹é‡","è®¡è´¹é‡é‡","é‡é‡"]:
        if cand in df.columns:
            weight_col = cand
            break
    if weight_col is None:
        df["æ‰˜ç›˜é‡é‡"] = pd.NA
        weight_col = "æ‰˜ç›˜é‡é‡"
    df[weight_col] = pd.to_numeric(df[weight_col], errors="coerce")

    len_col = next((c for c in ["æ‰˜ç›˜é•¿","é•¿","é•¿åº¦","Length","length","L"] if c in df.columns), None)
    wid_col = next((c for c in ["æ‰˜ç›˜å®½","å®½","å®½åº¦","Width","width","W"] if c in df.columns), None)
    hei_col = next((c for c in ["æ‰˜ç›˜é«˜","é«˜","é«˜åº¦","Height","height","H"] if c in df.columns), None)

    qty_col = next((c for c in [
        "ç®±æ•°","ç®±","ä»¶æ•°","ç®±ä»¶æ•°","Packages","Package","Cartons","Carton",
        "Qty","QTY","æ•°é‡"
    ] if c in df.columns), None)
    if qty_col is None:
        df["ç®±æ•°"] = pd.NA
        qty_col = "ç®±æ•°"
    df[qty_col] = pd.to_numeric(df[qty_col], errors="coerce")

    INCH_TO_M = 0.0254
    def _cbm_row(r):
        if not all([len_col, wid_col, hei_col]):
            return None
        try:
            L = float(pd.to_numeric(r.get(len_col), errors="coerce"))
            W = float(pd.to_numeric(r.get(wid_col), errors="coerce"))
            H = float(pd.to_numeric(r.get(hei_col), errors="coerce"))
            if L > 0 and W > 0 and H > 0:
                return (L * W * H) * (INCH_TO_M ** 3)
        except Exception:
            pass
        return None
    df["_cbm_row"] = df.apply(_cbm_row, axis=1)

    def _first_valid_num(s):
        s_num = pd.to_numeric(s, errors="coerce").dropna()
        return float(s_num.iloc[0]) if len(s_num) > 0 else None

    def _wb_list(s):
        vals = [x for x in s if isinstance(x, str) and x.strip()]
        return vals

    def _first_nonblank_str_local(s):
        for x in s:
            if not _is_blank(x): return str(x).strip()
        return ""

    create_date_col = next((c for c in ["æ‰˜ç›˜åˆ›å»ºæ—¥æœŸ","åˆ›å»ºæ—¥æœŸ","PalletCreateDate","CreateDate"] if c in df.columns), None)
    create_time_col = next((c for c in ["æ‰˜ç›˜åˆ›å»ºæ—¶é—´","åˆ›å»ºæ—¶é—´","PalletCreateTime","CreateTime"] if c in df.columns), None)
    if create_date_col is None:
        df["æ‰˜ç›˜åˆ›å»ºæ—¥æœŸ"] = ""
        create_date_col = "æ‰˜ç›˜åˆ›å»ºæ—¥æœŸ"
    if create_time_col is None:
        df["æ‰˜ç›˜åˆ›å»ºæ—¶é—´"] = ""
        create_time_col = "æ‰˜ç›˜åˆ›å»ºæ—¶é—´"

    agg_dict = {
        "æ‰˜ç›˜é‡é‡": (weight_col, _first_valid_num),
        "æ‰˜ç›˜ä½“ç§¯": ("_cbm_row", _first_valid_num),
        "è¿å•æ¸…å•_list": ("è¿å•å·", _wb_list),
        "æ‰˜ç›˜åˆ›å»ºæ—¥æœŸ_raw": (create_date_col, _first_nonblank_str_local),
        "æ‰˜ç›˜åˆ›å»ºæ—¶é—´_raw": (create_time_col, _first_nonblank_str_local),
    }
    if len_col: agg_dict["æ‰˜ç›˜é•¿in"] = (len_col, _first_valid_num)
    if wid_col: agg_dict["æ‰˜ç›˜å®½in"] = (wid_col, _first_valid_num)
    if hei_col: agg_dict["æ‰˜ç›˜é«˜in"] = (hei_col, _first_valid_num)

    base = (
        df.groupby(["æ‰˜ç›˜å·", "ä»“åº“ä»£ç "], as_index=False, dropna=False)
          .agg(**agg_dict)
    )

    arrivals = arrivals_df if arrivals_df is not None else load_arrivals_df(_bust=_get_bust("arrivals"))

    df_join = df.merge(
        arrivals[["è¿å•å·", "ETA/ATA", "ETD/ATD", "å¯¹å®¢æ‰¿è¯ºé€ä»“æ—¶é—´", "_ETAATA_date"]],
        on="è¿å•å·", how="left"
    )

    bol_cust_df = bol_cost_df if bol_cost_df is not None else load_bol_waybill_costs(_bust=_get_bust("bol_detail"))
    cust_map = {}
    if bol_cust_df is not None and not bol_cust_df.empty and "è¿å•å·" in bol_cust_df.columns and "å®¢æˆ·å•å·" in bol_cust_df.columns:
        for _, rr in bol_cust_df.iterrows():
            wb = _norm_waybill_str(rr.get("è¿å•å·", ""))
            cust = str(rr.get("å®¢æˆ·å•å·", "")).strip()
            if wb and cust:
                cust_map[wb] = cust

    pickup_map = load_bol_pickup_map(_bust=_get_bust("bol_detail"))

    pallets = []
    for _, brow in base.iterrows():
        pid, wh = brow["æ‰˜ç›˜å·"], brow["ä»“åº“ä»£ç "]
        if _is_blank(pid):
            continue

        p_wt  = brow.get("æ‰˜ç›˜é‡é‡", None)
        p_vol = brow.get("æ‰˜ç›˜ä½“ç§¯", None)

        waybills = brow.get("è¿å•æ¸…å•_list", []) or []
        waybills_disp = []
        for wb in waybills:
            wb_norm = _norm_waybill_str(wb)
            cust = cust_map.get(wb_norm, "")
            disp = f"{wb}({cust})" if cust else f"{wb}"
            disp = _normalize_ip_list_in_parens(disp)
            waybills_disp.append(disp)

        create_date_str, create_time_str = _split_dt_to_date_time_str(
            brow.get("æ‰˜ç›˜åˆ›å»ºæ—¥æœŸ_raw", ""),
            brow.get("æ‰˜ç›˜åˆ›å»ºæ—¶é—´_raw", "")
        )

        sub_qty = df[(df["æ‰˜ç›˜å·"] == pid) & (df["ä»“åº“ä»£ç "] == wh)].copy()
        sub_qty["è¿å•å·_norm"] = sub_qty["è¿å•å·"].map(_norm_waybill_str)
        qty_map = (
            sub_qty.groupby("è¿å•å·_norm")[qty_col]
                   .sum(min_count=1)
                   .to_dict()
        )
        waybills_disp_qty = []
        for wb in waybills:
            wb_norm = _norm_waybill_str(wb)
            q = qty_map.get(wb_norm, None)
            if q is None or pd.isna(q):
                q_str = "-"
            else:
                q_str = str(int(q)) if abs(q - round(q)) < 1e-9 else f"{q:.2f}"
            waybills_disp_qty.append(f"{wb}({q_str})")

        sub = df_join[(df_join["æ‰˜ç›˜å·"] == pid) & (df_join["ä»“åº“ä»£ç "] == wh)]
        lines_etaata, lines_etdatd, promised = [], [], []
        diffs_days = []
        for _, r in sub.iterrows():
            wb = r.get("è¿å•å·", "")
            etaata_s = r.get("ETA/ATA", pd.NA)
            etdatd_s = r.get("ETD/ATD", "")
            promise  = r.get("å¯¹å®¢æ‰¿è¯ºé€ä»“æ—¶é—´", "")
            anchor   = r.get("_ETAATA_date", None)
            lines_etaata.append(f"{wb}: ETA/ATA {etaata_s if not _is_blank(etaata_s) else '-'}")
            lines_etdatd.append(f"{wb}: {'' if _is_blank(etdatd_s) else str(etdatd_s)}")
            if not _is_blank(promise):
                diffs_days.append(_promise_diff_days_str(str(promise).strip(), anchor or date.today()))
                promised.append(str(promise).strip())

        readable_etaata = " ; ".join(lines_etaata) if lines_etaata else ""
        readable_etdatd = " ; ".join(lines_etdatd) if lines_etdatd else ""
        promised_set = list(dict.fromkeys([p for p in promised if p]))
        promised_str = " , ".join(promised_set)

        diff_days_str = ""
        if diffs_days:
            def keyfn(s):
                try:
                    a, _ = s.split("-", 1)
                    return int(a)
                except Exception:
                    return 10**9
            diff_days_str = sorted(diffs_days, key=keyfn)[0]

        L_in = brow.get("æ‰˜ç›˜é•¿in", None)
        W_in = brow.get("æ‰˜ç›˜å®½in", None)
        H_in = brow.get("æ‰˜ç›˜é«˜in", None)

        pickup_list = []
        pickup_list_disp = []
        for wb in waybills:
            wb_norm = _norm_waybill_str(wb)
            pk = pickup_map.get(wb_norm, "")
            if pk and str(pk).strip():
                pickup_list.append(str(pk).strip())
                pickup_list_disp.append(f"{wb}({pk})")
            else:
                pickup_list_disp.append(f"{wb}(-)")

        if pickup_list:
            uniq = sorted(set(pickup_list))
            pallet_pickup = uniq[0] if len(uniq) == 1 else "ï¼ˆå¤šè‡ªæä»“ï¼‰"
        else:
            pallet_pickup = ""

        pallets.append({
            "æ‰˜ç›˜å·": pid,
            "ä»“åº“ä»£ç ": wh,
            "è‡ªæä»“åº“(æŒ‰æ‰˜ç›˜)": pallet_pickup,
            "æ‰˜ç›˜é‡é‡": float(p_wt) if pd.notna(p_wt) else None,
            "æ‰˜ç›˜ä½“ç§¯": float(p_vol) if p_vol is not None else None,
            "é•¿(in)": round(float(L_in), 2) if pd.notna(L_in) else None,
            "å®½(in)": round(float(W_in), 2) if pd.notna(W_in) else None,
            "é«˜(in)": round(float(H_in), 2) if pd.notna(H_in) else None,
            "æ‰˜ç›˜åˆ›å»ºæ—¥æœŸ": create_date_str,
            "æ‰˜ç›˜åˆ›å»ºæ—¶é—´": create_time_str,
            "è¿å•æ•°é‡": len(waybills),
            "è¿å•æ¸…å•": ", ".join(waybills_disp) if waybills_disp else "",
            "è¿å•ç®±æ•°": ", ".join(waybills_disp_qty) if waybills_disp_qty else "",
            "è‡ªæä»“åº“(æŒ‰è¿å•)": ", ".join(pickup_list_disp) if pickup_list_disp else "",
            "å¯¹å®¢æ‰¿è¯ºé€ä»“æ—¶é—´": promised_str,
            "é€ä»“æ—¶æ®µå·®å€¼(å¤©)": diff_days_str,
            "ETA/ATA(æŒ‰è¿å•)": readable_etaata,
            "ETD/ATD(æŒ‰è¿å•)": readable_etdatd,
        })

    out = pd.DataFrame(pallets)
    if out.empty:
        return out
    out = out[out["æ‰˜ç›˜å·"].astype(str).str.strip() != ""].copy()

    for c in ["æ‰˜ç›˜ä½“ç§¯","æ‰˜ç›˜é‡é‡","é•¿(in)","å®½(in)","é«˜(in)"]:
        if c in out.columns:
            out[c] = pd.to_numeric(out[c], errors="coerce")
    out["æ‰˜ç›˜ä½“ç§¯"] = out["æ‰˜ç›˜ä½“ç§¯"].round(2)
    out["é•¿(in)"] = out["é•¿(in)"].round(2)
    out["å®½(in)"] = out["å®½(in)"].round(2)
    out["é«˜(in)"] = out["é«˜(in)"].round(2)

    return out

@st.cache_data(ttl=30)
def load_shipped_pallet_ids(_bust=0, sheet_sig=None) -> set[str]:
    try:
        ws = client.open(SHEET_SHIP_TRACKING).sheet1
    except SpreadsheetNotFound:
        return set()
    vals = _safe_get_all_values(ws, "UNFORMATTED_VALUE", "SERIAL_NUMBER")
    if not vals or not vals[0]:
        return set()
    header = list(vals[0])
    if "æ‰˜ç›˜å·" not in header:
        return set()
    col_idx = header.index("æ‰˜ç›˜å·")
    pallet_ids = [row[col_idx] for row in vals[1:] if len(row) > col_idx]
    def _norm_pid(s):
        return str(s).strip().upper() if s and str(s).strip() else ""
    return { _norm_pid(pid) for pid in pallet_ids if _norm_pid(pid) }

@st.cache_data(ttl=300)
def load_bol_waybill_costs(_bust=0) -> pd.DataFrame:
    try:
        ws = client.open(SHEET_BOL_DETAIL).sheet1
    except SpreadsheetNotFound:
        return pd.DataFrame(columns=["è¿å•å·","å®¢æˆ·å•å·","åˆ°è‡ªæä»“åº“æ—¥æœŸ","åˆ°è‡ªæä»“åº“å¡è½¦å·","åˆ°è‡ªæä»“åº“è´¹ç”¨"])
    vals = _safe_get_all_values(ws, "UNFORMATTED_VALUE", "SERIAL_NUMBER")
    if not vals or not vals[0]:
        return pd.DataFrame(columns=["è¿å•å·","å®¢æˆ·å•å·","åˆ°è‡ªæä»“åº“æ—¥æœŸ","åˆ°è‡ªæä»“åº“å¡è½¦å·","åˆ°è‡ªæä»“åº“è´¹ç”¨"])
    raw_header = list(vals[0])
    df = pd.DataFrame(vals[1:], columns=raw_header) if len(vals) > 1 else pd.DataFrame(columns=raw_header)
    def norm(s: str) -> str:
        return str(s).replace("\u00A0"," ").replace("\n","").replace(" ","").strip().lower()
    aliases = {
        "wb":   ["è¿å•å·","Waybill","waybill","è¿å•ç¼–å·","æå•å·","å•å·"],
        "cust": ["å®¢æˆ·å•å·","å®¢æˆ·PO","å®¢æˆ·POå·","å®¢æˆ·å‚è€ƒå·","CustomerPO","CustomerRef","Reference","Ref","å‚è€ƒå·"],
        "truck":["åˆ°è‡ªæä»“åº“å¡è½¦å·","åˆ°è‡ªæä»“å¡è½¦å·","åˆ°è‡ªæä»“è½¦å·","BOLå¡è½¦å·","BOLå¡è½¦","å¡è½¦å•å·","å¡è½¦å·","TruckNo","Truck","truckno","truck"],
        "cost": ["åˆ°è‡ªæä»“åº“è´¹ç”¨","åˆ°è‡ªæä»“è´¹ç”¨","è‡ªæè´¹ç”¨","BOLè´¹ç”¨","Amount","amount","Cost","cost","è´¹ç”¨","åˆ†æ‘Šè´¹ç”¨"],
        "date": ["åˆ°è‡ªæä»“åº“æ—¥æœŸ","åˆ°è‡ªæä»“æ—¥æœŸ","è‡ªææ—¥æœŸ","BOLæ—¥æœŸ","æ—¥æœŸ","Date","date","ETA(åˆ°è‡ªæä»“)","ETAåˆ°è‡ªæä»“åº“","åˆ°è‡ªæä»“åº“ETA"],
    }
    def pick_col(cands: list[str]) -> str | None:
        wants = {norm(x) for x in cands}
        for h in raw_header:
            if norm(h) in wants:
                return h
        return None
    col_wb    = pick_col(aliases["wb"])
    col_cust  = pick_col(aliases["cust"])
    col_truck = pick_col(aliases["truck"])
    col_cost  = pick_col(aliases["cost"])
    col_date  = pick_col(aliases["date"])
    if not col_wb:
        return pd.DataFrame(columns=["è¿å•å·","å®¢æˆ·å•å·","åˆ°è‡ªæä»“åº“æ—¥æœŸ","åˆ°è‡ªæä»“åº“å¡è½¦å·","åˆ°è‡ªæä»“åº“è´¹ç”¨"])
    need_cols = [c for c in [col_wb, col_cust, col_truck, col_cost, col_date] if c]
    df = df[need_cols].copy()
    rename_map = {}
    if col_wb:    rename_map[col_wb]    = "è¿å•å·"
    if col_cust:  rename_map[col_cust]  = "å®¢æˆ·å•å·"
    if col_truck: rename_map[col_truck] = "åˆ°è‡ªæä»“åº“å¡è½¦å·"
    if col_cost:  rename_map[col_cost]  = "åˆ°è‡ªæä»“åº“è´¹ç”¨"
    if col_date:  rename_map[col_date]  = "åˆ°è‡ªæä»“åº“æ—¥æœŸ"
    df.rename(columns=rename_map, inplace=True)

    if "è¿å•å·" in df.columns:
        df["è¿å•å·"] = df["è¿å•å·"].map(_norm_waybill_str)
    if "å®¢æˆ·å•å·" in df.columns:
        df["å®¢æˆ·å•å·"] = df["å®¢æˆ·å•å·"].astype(str).str.strip()
    if "åˆ°è‡ªæä»“åº“å¡è½¦å·" in df.columns:
        df["åˆ°è‡ªæä»“åº“å¡è½¦å·"] = df["åˆ°è‡ªæä»“åº“å¡è½¦å·"].astype(str).str.strip()

    if "åˆ°è‡ªæä»“åº“è´¹ç”¨" in df.columns:
        def _to_num_safe_local(x):
            try:
                s = str(x).strip().replace(",", "")
                s = re.sub(r"[^\d\.\-]", "", s)
                return float(s)
            except Exception:
                return None
        df["åˆ°è‡ªæä»“åº“è´¹ç”¨"] = df["åˆ°è‡ªæä»“åº“è´¹ç”¨"].map(_to_num_safe_local)

    if "åˆ°è‡ªæä»“åº“æ—¥æœŸ" in df.columns:
        df["_date_tmp"] = df["åˆ°è‡ªæä»“åº“æ—¥æœŸ"].map(_parse_sheet_value_to_date)
        df["åˆ°è‡ªæä»“åº“æ—¥æœŸ"] = df["_date_tmp"].map(lambda d: d.strftime("%Y-%m-%d") if isinstance(d, date) else pd.NA)
        df.drop(columns=["_date_tmp"], inplace=True, errors="ignore")

    df = df[df["è¿å•å·"].astype(str).str.strip() != ""]
    if not df.empty:
        df = df.drop_duplicates(subset=["è¿å•å·"], keep="last")

    for c in ["å®¢æˆ·å•å·","åˆ°è‡ªæä»“åº“æ—¥æœŸ","åˆ°è‡ªæä»“åº“å¡è½¦å·","åˆ°è‡ªæä»“åº“è´¹ç”¨"]:
        if c not in df.columns:
            df[c] = pd.NA
    df["åˆ°è‡ªæä»“åº“è´¹ç”¨"] = pd.to_numeric(df["åˆ°è‡ªæä»“åº“è´¹ç”¨"], errors="coerce").round(2)
    return df[["è¿å•å·","å®¢æˆ·å•å·","åˆ°è‡ªæä»“åº“æ—¥æœŸ","åˆ°è‡ªæä»“åº“å¡è½¦å·","åˆ°è‡ªæä»“åº“è´¹ç”¨"]]

@st.cache_data(ttl=30)
def load_ship_tracking_raw(_bust=0, sheet_sig=None) -> pd.DataFrame:
    try:
        ws = client.open(SHEET_SHIP_TRACKING).sheet1
    except SpreadsheetNotFound:
        return pd.DataFrame(columns=["æ‰˜ç›˜å·","è¿å•æ¸…å•","å¡è½¦å•å·","åˆ†æ‘Šè´¹ç”¨","æ—¥æœŸ","è‡ªæä»“åº“(æŒ‰æ‰˜ç›˜)"])
    vals = _safe_get_all_values(ws, "UNFORMATTED_VALUE", "SERIAL_NUMBER")
    if not vals or not vals[0]:
        return pd.DataFrame(columns=["æ‰˜ç›˜å·","è¿å•æ¸…å•","å¡è½¦å•å·","åˆ†æ‘Šè´¹ç”¨","æ—¥æœŸ","è‡ªæä»“åº“(æŒ‰æ‰˜ç›˜)"])
    raw_header = list(vals[0])
    df = pd.DataFrame(vals[1:], columns=raw_header) if len(vals) > 1 else pd.DataFrame(columns=raw_header)
    def norm(s: str) -> str:
        return str(s).replace("\u00A0"," ").replace("\n","").replace(" ","").strip().lower()
    aliases = {
        "æ‰˜ç›˜å·": ["æ‰˜ç›˜å·","æ‰˜ç›˜ç¼–å·","PalletID","Pallet","æ‰˜ç›˜"],
        "è¿å•æ¸…å•": ["è¿å•æ¸…å•","Waybills","WaybillList","è¿å•å·","Waybill"],
        "å¡è½¦å•å·": ["å¡è½¦å•å·","TruckNo","Truck","å¡è½¦å·"],
        "åˆ†æ‘Šè´¹ç”¨": ["åˆ†æ‘Šè´¹ç”¨","è´¹ç”¨","Cost","Amount"],
        "æ—¥æœŸ":   ["æ—¥æœŸ","Date","å‘è´§æ—¥æœŸ","UploadDate"],
        "è‡ªæä»“åº“(æŒ‰æ‰˜ç›˜)": ["è‡ªæä»“åº“(æŒ‰æ‰˜ç›˜)","è‡ªæä»“åº“","ä»“åº“","PickupWarehouse"]
    }
    def pick_col(cands: list[str]) -> str | None:
        wants = {norm(x) for x in cands}
        for h in raw_header:
            if norm(h) in wants:
                return h
        return None
    rename_map = {}
    for key, cands in aliases.items():
        c = pick_col(cands)
        if c:
            rename_map[c] = key
    df = df.rename(columns=rename_map)
    for c in ["æ‰˜ç›˜å·","è¿å•æ¸…å•","å¡è½¦å•å·","åˆ†æ‘Šè´¹ç”¨","æ—¥æœŸ","è‡ªæä»“åº“(æŒ‰æ‰˜ç›˜)"]:
        if c not in df.columns:
            df[c] = pd.NA
    df["æ‰˜ç›˜å·"]   = df["æ‰˜ç›˜å·"].astype(str).str.strip()
    df["å¡è½¦å•å·"] = df["å¡è½¦å•å·"].astype(str).str.strip()
    df["è‡ªæä»“åº“(æŒ‰æ‰˜ç›˜)"] = df["è‡ªæä»“åº“(æŒ‰æ‰˜ç›˜)"].astype(str).str.strip()
    df["åˆ†æ‘Šè´¹ç”¨"] = pd.to_numeric(df["åˆ†æ‘Šè´¹ç”¨"], errors="coerce").round(2)
    df["_day_obj"] = df["æ—¥æœŸ"].apply(_parse_sheet_value_to_date)
    df["æ—¥æœŸ"] = df["_day_obj"].apply(lambda d: d.strftime("%Y-%m-%d") if isinstance(d, date) else "")
    df.drop(columns=["_day_obj"], inplace=True, errors="ignore")
    df = df[df["æ‰˜ç›˜å·"].astype(str).str.strip() != ""]
    return df[["æ‰˜ç›˜å·","è¿å•æ¸…å•","å¡è½¦å•å·","åˆ†æ‘Šè´¹ç”¨","æ—¥æœŸ","è‡ªæä»“åº“(æŒ‰æ‰˜ç›˜)"]]

@st.cache_data(ttl=300)
def load_customer_refs_from_arrivals(_bust=0):
    try:
        ws = client.open(SHEET_ARRIVALS_NAME).sheet1
    except SpreadsheetNotFound:
        return pd.DataFrame(columns=["è¿å•å·","å®¢æˆ·å•å·"])
    vals = _safe_get_all_values(ws)
    if not vals:
        return pd.DataFrame(columns=["è¿å•å·","å®¢æˆ·å•å·"])
    header = _norm_header(vals[0])
    df = pd.DataFrame(vals[1:], columns=header)
    cust_col = next((c for c in ["å®¢æˆ·å•å·","å®¢æˆ·PO","å®¢æˆ·POå·","å®¢æˆ·å‚è€ƒå·","CustomerPO","CustomerRef","Reference"] if c in df.columns), None)
    wb_col   = next((c for c in ["è¿å•å·","Waybill","waybill"] if c in df.columns), None)
    if not cust_col or not wb_col:
        return pd.DataFrame(columns=["è¿å•å·","å®¢æˆ·å•å·"])
    out = df[[wb_col, cust_col]].copy()
    out[wb_col] = out[wb_col].apply(_norm_waybill_str)
    out[cust_col] = out[cust_col].astype(str).str.strip()
    out = out.rename(columns={wb_col:"è¿å•å·", cust_col:"å®¢æˆ·å•å·"})
    out = out[out["è¿å•å·"]!=""].drop_duplicates(subset=["è¿å•å·"])
    return out[["è¿å•å·","å®¢æˆ·å•å·"]]

@st.cache_data(ttl=300)
def load_customer_refs_from_pallet(_bust=0):
    try:
        ws = client.open(SHEET_PALLET_DETAIL).sheet1
    except SpreadsheetNotFound:
        return pd.DataFrame(columns=["è¿å•å·","å®¢æˆ·å•å·"])
    vals = _safe_get_all_values(ws)
    if not vals:
        return pd.DataFrame(columns=["è¿å•å·","å®¢æˆ·å•å·"])
    header = _norm_header(vals[0])
    df = pd.DataFrame(vals[1:], columns=header)
    cust_col = next((c for c in ["å®¢æˆ·å•å·","å®¢æˆ·PO","å®¢æˆ·POå·","å®¢æˆ·å‚è€ƒå·","CustomerPO","CustomerRef","Reference"] if c in df.columns), None)
    wb_col   = next((c for c in ["è¿å•å·","Waybill","waybill"] if c in df.columns), None)
    if not cust_col or not wb_col:
        return pd.DataFrame(columns=["è¿å•å·","å®¢æˆ·å•å·"])
    out = df[[wb_col, cust_col]].copy()
    out[wb_col] = out[wb_col].apply(_norm_waybill_str)
    out[cust_col] = out[cust_col].astype(str).str.strip()
    out = out.rename(columns={wb_col:"è¿å•å·", cust_col:"å®¢æˆ·å•å·"})
    out = out[out["è¿å•å·"]!=""].drop_duplicates(subset=["è¿å•å·"])
    return out[["è¿å•å·","å®¢æˆ·å•å·"]]

# ===================== è¿å•å¢é‡æ„å»ºï¼ˆä¿®æ”¹ç‚¹ï¼šåªè¯†åˆ« USSHï¼Œå®¢æˆ·å•å·ä»…æ¥è‡ªã€åˆ°ä»“æ•°æ®è¡¨ã€ï¼‰ =====================
def build_waybill_delta(track_override: pd.DataFrame | None = None):
    arrivals = load_arrivals_df(_bust=_get_bust("arrivals"))
    bol      = load_bol_waybill_costs(_bust=_get_bust("bol_detail"))

    ship_track_sig = _sheet_row_sig(SHEET_SHIP_TRACKING, _bust=_get_bust("ship_tracking"))
    track = load_ship_tracking_raw(
        _bust=_get_bust("ship_tracking"),
        sheet_sig=ship_track_sig
    )

    if track_override is None:
        track_override = st.session_state.get("_track_override", None)
    if isinstance(track_override, pd.DataFrame) and not track_override.empty:
        need_cols = ["æ‰˜ç›˜å·","è¿å•æ¸…å•","å¡è½¦å•å·","åˆ†æ‘Šè´¹ç”¨","æ—¥æœŸ"]
        track_override = track_override[[c for c in need_cols if c in track_override.columns]].copy()
        track = pd.concat([track, track_override], ignore_index=True)

    if track is None or track.empty:
        return pd.DataFrame(columns=[
            "è¿å•å·","å®¢æˆ·å•å·","ä»“åº“ä»£ç ","æ”¶è´¹é‡","ä½“ç§¯",
            "å‘å‡º(ETD/ATD)","åˆ°æ¸¯(ETA/ATA)",
            "åˆ°è‡ªæä»“åº“æ—¥æœŸ","å‘èµ°æ—¥æœŸ","åˆ°ä»“æ—¥æœŸ",
            "åˆ°è‡ªæä»“åº“å¡è½¦å·","åˆ°è‡ªæä»“åº“è´¹ç”¨",
            "å‘èµ°å¡è½¦å·","å‘èµ°è´¹ç”¨","è‡ªæä»“åº“"
        ])

    def _norm_pid(s):
        return str(s).strip().upper() if pd.notna(s) else ""
    def _norm_trk(s):
        s = str(s).strip().upper() if pd.notna(s) else ""
        return s.replace(" ", "").replace("-", "")
    def _norm_day(s):
        dt = _parse_sheet_value_to_date(s)
        return _fmt_date(dt) if dt else ""

    track = track.copy()
    track["_pid_k"] = track.get("æ‰˜ç›˜å·","").map(_norm_pid)
    track["_trk_k"] = track.get("å¡è½¦å•å·","").map(_norm_trk)
    track["_day_k"] = track.get("æ—¥æœŸ","").map(_norm_day)

    track = track[~track.duplicated(subset=["_pid_k","_trk_k","_day_k"], keep="last")].copy()
    track["_has_day"] = track["_day_k"].ne("")
    track = (
        track.sort_values(["_pid_k","_trk_k","_has_day"], ascending=[True, True, False])
             .drop_duplicates(subset=["_pid_k","_trk_k"], keep="first")
             .drop(columns=["_has_day"])
    )
    if "åˆ†æ‘Šè´¹ç”¨" in track.columns:
        track["_cost2"] = pd.to_numeric(track["åˆ†æ‘Šè´¹ç”¨"], errors="coerce").round(2)
        dup2 = track["_day_k"].eq("") & track.duplicated(subset=["_pid_k","_trk_k","_cost2"], keep="last")
        track = track[~dup2].copy()

    # âœ… ä»…ç”¨ä¸¥æ ¼ USSH æå–
    wb_lists = []
    for _, r in track.iterrows():
        wb_list = _extract_wb_ushh_only(r.get("è¿å•æ¸…å•",""))
        wb_lists.append(wb_list)
    track["_wb_list"] = wb_lists

    wb_from_track = set()
    for lst in track["_wb_list"]:
        if lst:
            wb_from_track.update(lst)

    if not wb_from_track:
        return pd.DataFrame(columns=[
            "è¿å•å·","å®¢æˆ·å•å·","ä»“åº“ä»£ç ","æ”¶è´¹é‡","ä½“ç§¯",
            "å‘å‡º(ETD/ATD)","åˆ°æ¸¯(ETA/ATA)",
            "åˆ°è‡ªæä»“åº“æ—¥æœŸ","å‘èµ°æ—¥æœŸ","åˆ°ä»“æ—¥æœŸ",
            "åˆ°è‡ªæä»“åº“å¡è½¦å·","åˆ°è‡ªæä»“åº“è´¹ç”¨",
            "å‘èµ°å¡è½¦å·","å‘èµ°è´¹ç”¨","è‡ªæä»“åº“"
        ])

    if not arrivals.empty:
        arrivals = arrivals[arrivals["è¿å•å·"].isin(wb_from_track)].copy()
    if not bol.empty:
        bol = bol[bol["è¿å•å·"].isin(wb_from_track)].copy()

    weight_map = dict(zip(
        arrivals["è¿å•å·"],
        pd.to_numeric(arrivals["æ”¶è´¹é‡"], errors="coerce")
    ))

    pickup_map = load_bol_pickup_map(_bust=_get_bust("bol_detail"))

    wb2_cost: dict[str, float] = {}
    wb2_trucks: dict[str, set] = {}
    wb2_date: dict[str, date] = {}

    wb_weight_cache: dict[str, float | None] = {}
    def _wb_weight(wb: str):
        if wb not in wb_weight_cache:
            wb_weight_cache[wb] = weight_map.get(wb, None)
        return wb_weight_cache[wb]

    for _, r in track.iterrows():
        waybills = [wb for wb in (r.get("_wb_list") or []) if wb in wb_from_track]
        if not waybills:
            continue
        pallet_cost = _to_num_safe(r.get("åˆ†æ‘Šè´¹ç”¨"))
        truck_no    = r.get("å¡è½¦å•å·", "")
        dt_str      = r.get("æ—¥æœŸ", None)
        dt_obj      = _parse_sheet_value_to_date(dt_str) if not _is_blank(dt_str) else None

        total_w = 0.0
        weights = []
        for wb in waybills:
            w = _wb_weight(wb)
            if w and w > 0:
                weights.append(w); total_w += w
            else:
                weights.append(None)

        if total_w > 0:
            shares = [(w/total_w if (w and w > 0) else 0.0) for w in weights]
        else:
            shares = [1.0/len(waybills)] * len(waybills)

        if pallet_cost is not None:
            for wb, s in zip(waybills, shares):
                wb2_cost[wb] = wb2_cost.get(wb, 0.0) + pallet_cost * s

        if str(truck_no).strip():
            for wb in waybills:
                wb2_trucks.setdefault(wb, set()).add(str(truck_no).strip())

        if dt_obj:
            for wb in waybills:
                if (wb not in wb2_date) or (dt_obj < wb2_date[wb]):
                    wb2_date[wb] = dt_obj

    total_from_track = pd.to_numeric(track.get("åˆ†æ‘Šè´¹ç”¨"), errors="coerce").fillna(0).sum()
    total_to_waybill = sum(wb2_cost.values())
    diff_total = round(total_from_track - total_to_waybill, 2)
    st.write(f"ğŸ§® è´¹ç”¨æ£€æŸ¥ï¼šå‘è´§è¿½è¸ªåˆè®¡={total_from_track:.2f}ï¼Œå·²åˆ†åˆ°è¿å•={total_to_waybill:.2f}ï¼Œå·®é¢={diff_total:.2f}")
    if "_trk_k" in track.columns:
        by_truck = (track
            .assign(_cost = pd.to_numeric(track["åˆ†æ‘Šè´¹ç”¨"], errors="coerce").fillna(0))
            .groupby("_trk_k")["_cost"].sum())
        st.write("å„å¡è½¦åœ¨ã€å‘è´§è¿½è¸ªã€é‡Œçš„åˆè®¡ï¼š", by_truck.to_dict())

    out = pd.DataFrame({"è¿å•å·": sorted(wb_from_track)})
    out["è‡ªæä»“åº“"] = out["è¿å•å·"].map(lambda wb: pickup_map.get(wb, pd.NA))

    # âœ… åˆ°ä»“æ•°æ®ï¼ˆå« å®¢æˆ·å•å·ï¼‰å¯¹é½ï¼ˆå®¢æˆ·å•å·åªæ¥è‡ªè¿™é‡Œï¼‰
    if not arrivals.empty:
        arr2 = arrivals[["è¿å•å·","ä»“åº“ä»£ç ","æ”¶è´¹é‡","ä½“ç§¯","ETD/ATD","ETA/ATA","é¢„è®¡åˆ°ä»“æ—¶é—´ï¼ˆæ—¥ï¼‰","å®¢æˆ·å•å·"]].copy()
        arr2 = arr2.rename(columns={
            "ETD/ATD": "å‘å‡º(ETD/ATD)",
            "ETA/ATA": "åˆ°æ¸¯(ETA/ATA)",
            "é¢„è®¡åˆ°ä»“æ—¶é—´ï¼ˆæ—¥ï¼‰": "åˆ°ä»“æ—¥æœŸ"
        })
        out = out.merge(arr2, on="è¿å•å·", how="left")
    else:
        out["ä»“åº“ä»£ç "] = pd.NA
        out["æ”¶è´¹é‡"] = pd.NA
        out["ä½“ç§¯"]   = pd.NA
        out["å‘å‡º(ETD/ATD)"] = pd.NA
        out["åˆ°æ¸¯(ETA/ATA)"] = pd.NA
        out["åˆ°ä»“æ—¥æœŸ"]       = pd.NA
        out["å®¢æˆ·å•å·"]       = pd.NA

    if not bol.empty:
        out = out.merge(bol[["è¿å•å·","åˆ°è‡ªæä»“åº“æ—¥æœŸ","åˆ°è‡ªæä»“åº“å¡è½¦å·","åˆ°è‡ªæä»“åº“è´¹ç”¨"]], on="è¿å•å·", how="left")
    else:
        for c in ["åˆ°è‡ªæä»“åº“æ—¥æœŸ","åˆ°è‡ªæä»“åº“å¡è½¦å·","åˆ°è‡ªæä»“åº“è´¹ç”¨"]:
            out[c] = pd.NA

    out["å‘èµ°è´¹ç”¨"]   = out["è¿å•å·"].map(lambda wb: round(wb2_cost.get(wb, 0.0), 2) if wb in wb2_cost else pd.NA)
    out["å‘èµ°å¡è½¦å·"] = out["è¿å•å·"].map(lambda wb: ", ".join(sorted(wb2_trucks.get(wb, []))) if wb in wb2_trucks else pd.NA)
    def _safe_fmt_date(d):
        return _fmt_date(d) if isinstance(d, date) else pd.NA
    out["å‘èµ°æ—¥æœŸ"] = out["è¿å•å·"].map(lambda wb: _safe_fmt_date(wb2_date.get(wb)))

    out["æ”¶è´¹é‡"]        = pd.to_numeric(out["æ”¶è´¹é‡"], errors="coerce")
    out["ä½“ç§¯"]          = pd.to_numeric(out["ä½“ç§¯"], errors="coerce").round(2)
    out["åˆ°è‡ªæä»“åº“è´¹ç”¨"]  = pd.to_numeric(out["åˆ°è‡ªæä»“åº“è´¹ç”¨"], errors="coerce").round(2)
    out["å‘èµ°è´¹ç”¨"]       = pd.to_numeric(out["å‘èµ°è´¹ç”¨"], errors="coerce").round(2)
    out["ç¾ä»“å¤‡è´§å®Œæˆæ—¥æœŸ"] = out["åˆ°è‡ªæä»“åº“æ—¥æœŸ"]

    final_cols = [
        "è¿å•å·","å®¢æˆ·å•å·","ä»“åº“ä»£ç ","è‡ªæä»“åº“","æ”¶è´¹é‡","ä½“ç§¯",
        "å‘å‡º(ETD/ATD)","åˆ°æ¸¯(ETA/ATA)","ç¾ä»“å¤‡è´§å®Œæˆæ—¥æœŸ",
        "åˆ°è‡ªæä»“åº“æ—¥æœŸ","å‘èµ°æ—¥æœŸ","åˆ°ä»“æ—¥æœŸ",
        "åˆ°è‡ªæä»“åº“å¡è½¦å·","åˆ°è‡ªæä»“åº“è´¹ç”¨",
        "å‘èµ°å¡è½¦å·","å‘èµ°è´¹ç”¨"
    ]
    for c in final_cols:
        if c not in out.columns:
            out[c] = pd.NA
    return out[final_cols]

MANAGED_COLS = [
    "è¿å•å·","å®¢æˆ·å•å·",
    "å‘å‡º(ETD/ATD)","åˆ°æ¸¯(ETA/ATA)","ç¾ä»“å¤‡è´§å®Œæˆæ—¥æœŸ",
    "åˆ°è‡ªæä»“åº“æ—¥æœŸ","å‘èµ°æ—¥æœŸ","åˆ°ä»“æ—¥æœŸ",
    "åˆ°è‡ªæä»“åº“å¡è½¦å·","åˆ°è‡ªæä»“åº“è´¹ç”¨",
    "å‘èµ°å¡è½¦å·","å‘èµ°è´¹ç”¨",
    "ä»“åº“ä»£ç ","è‡ªæä»“åº“","æ”¶è´¹é‡","ä½“ç§¯",
    "æ‰¹æ¬¡ID","ä¸Šä¼ æ—¶é—´"
]

def _is_effective(v):
    if v is None: return False
    if isinstance(v, float) and pd.isna(v): return False
    if isinstance(v, str) and v.strip() == "": return False
    return True

def _to_jsonable_cell(v):
    try:
        if v is None or pd.isna(v):
            return ""
    except Exception:
        if v is None:
            return ""
    if isinstance(v, (np.integer,)):
        return int(v)
    if isinstance(v, (np.floating,)):
        return "" if np.isnan(v) or np.isinf(v) else float(v)
    if isinstance(v, float):
        return "" if (math.isnan(v) or math.isinf(v)) else v
    return v if isinstance(v, (str, int, float, bool)) else str(v)

def upsert_waybill_summary_partial(df_delta: pd.DataFrame):
    WRITE_POLICY = {
        "åˆ°ä»“æ—¥æœŸ": "blank_only",
        "å‘èµ°æ—¥æœŸ": "blank_only",
        "ç¾ä»“å¤‡è´§å®Œæˆæ—¥æœŸ": "blank_only",
        "åˆ°è‡ªæä»“åº“æ—¥æœŸ": "blank_only",
        "åˆ°è‡ªæä»“åº“è´¹ç”¨": "blank_only",
        "å‘èµ°è´¹ç”¨": "blank_only",
        "åˆ°è‡ªæä»“åº“å¡è½¦å·": "merge_set",
        "å‘èµ°å¡è½¦å·": "merge_set",
        "ä»“åº“ä»£ç ": "blank_only",
        "å®¢æˆ·å•å·": "blank_only",
        "è‡ªæä»“åº“": "blank_only",
        "æ‰¹æ¬¡ID": "blank_only",
        "ä¸Šä¼ æ—¶é—´": "blank_only",
    }
    MERGE_SEP = ","

    def _cell_blank(x):
        return (x is None) or (isinstance(x, float) and pd.isna(x)) or (isinstance(x, str) and x.strip() == "")

    def _merge_set(old, new):
        def toks(s):
            if _cell_blank(s): return []
            parts = re.split(r"[,\ï¼Œ;\ï¼›\|/ ]+", str(s).strip())
            return [p for p in parts if p]
        seen = []
        for t in (toks(old) + toks(new)):
            if t not in seen:
                seen.append(t)
        return MERGE_SEP.join(seen)

    def _is_iso_date(s: str) -> bool:
        try:
            if not isinstance(s, str):
                return False
            pd.to_datetime(s, format="%Y-%m-%d", errors="raise")
            return True
        except Exception:
            return False

    try:
        ws = client.open(SHEET_WB_SUMMARY).sheet1
    except SpreadsheetNotFound:
        st.error(f"æ‰¾ä¸åˆ°å·¥ä½œè¡¨ã€Œ{SHEET_WB_SUMMARY}ã€ã€‚è¯·å…ˆåˆ›å»ºå¹¶åœ¨ç¬¬1è¡Œå†™å…¥è¡¨å¤´ï¼ˆè‡³å°‘åŒ…å«ï¼šè¿å•å·ï¼‰ã€‚")
        return False

    vals = _safe_get_all_values(ws, "UNFORMATTED_VALUE", "SERIAL_NUMBER")
    if not vals or not vals[0]:
        st.error("ã€è¿å•å…¨é“¾è·¯æ±‡æ€»ã€ä¸ºç©ºä¸”æ— è¡¨å¤´ã€‚è¯·å…ˆåœ¨ç¬¬ä¸€è¡Œå†™å¥½è¡¨å¤´ï¼ˆè‡³å°‘åŒ…å«ï¼šè¿å•å·ï¼‰ã€‚")
        return False

    header = list(vals[0])
    if "è¿å•å·" not in header:
        st.error("ã€è¿å•å…¨é“¾è·¯æ±‡æ€»ã€ç¼ºå°‘â€œè¿å•å·â€è¡¨å¤´ï¼Œæ— æ³•æ›´æ–°ã€‚")
        return False

    df_delta = df_delta.copy()
    if "è¿å•å·" not in df_delta.columns:
        st.error("å¢é‡æ•°æ®ç¼ºå°‘â€œè¿å•å·â€ã€‚")
        return False
    df_delta["è¿å•å·"] = df_delta["è¿å•å·"].map(_norm_waybill_str)

    missing_cols = [c for c in MANAGED_COLS if c not in header]
    if missing_cols:
        ws.update(f"{ws.title}!1:1", [header + missing_cols], value_input_option="USER_ENTERED")
        header = header + missing_cols

    exist_df = pd.DataFrame(vals[1:], columns=header) if len(vals) > 1 else pd.DataFrame(columns=header)
    if "è¿å•å·" not in exist_df.columns:
        exist_df["è¿å•å·"] = ""
    exist_df["è¿å•å·"] = exist_df["è¿å•å·"].map(_norm_waybill_str)
    exist_df["_rowno"] = np.arange(2, 2 + len(exist_df))

    idx_exist = exist_df.set_index("è¿å•å·", drop=False)
    idx_delta = df_delta.set_index("è¿å•å·", drop=False)

    common  = idx_delta.index.intersection(idx_exist.index)
    new_ids = list(idx_delta.index.difference(idx_exist.index))

    updates = []
    for col in MANAGED_COLS:
        if col == "è¿å•å·":
            continue
        if col not in header or col not in idx_delta.columns:
            continue
        col_idx = header.index(col) + 1
        rows_payload = []

        policy = WRITE_POLICY.get(col, "default")
        is_date_col = col in ["åˆ°ä»“æ—¥æœŸ","å‘èµ°æ—¥æœŸ","ç¾ä»“å¤‡è´§å®Œæˆæ—¥æœŸ","åˆ°è‡ªæä»“åº“æ—¥æœŸ"]

        for wb in common:
            new_v = idx_delta.loc[wb, col]
            if is_date_col:
                if not (isinstance(new_v, str) and len(new_v) == 10 and new_v.count("-")==2):
                    continue
            else:
                if not _is_effective(new_v):
                    continue

            rno = int(idx_exist.loc[wb, "_rowno"])
            old_v = idx_exist.loc[wb, col] if col in idx_exist.columns else ""

            if policy == "blank_only":
                if not (old_v is None or (isinstance(old_v, float) and pd.isna(old_v)) or (isinstance(old_v, str) and old_v.strip() == "")):
                    continue
                write_v = new_v
            elif policy == "merge_set":
                write_v = _merge_set(old_v, new_v)
            else:
                write_v = new_v

            rows_payload.append((rno, [_to_jsonable_cell(write_v)]))

        if not rows_payload:
            continue

        rows_payload.sort(key=lambda x: x[0])
        updates.extend(_pack_ranges_for_col(ws.title, col_idx, rows_payload))

    if updates:
        spreadsheet_id = ws.spreadsheet.id
        batch_sz = 300
        for i in range(0, len(updates), batch_sz):
            sub = updates[i:i + batch_sz]
            body = {"valueInputOption": "USER_ENTERED", "data": sub}
            sheets_service.spreadsheets().values().batchUpdate(
                spreadsheetId=spreadsheet_id,
                body=body
            ).execute()

    if new_ids:
        cols_out = [c for c in header if c in MANAGED_COLS]
        if "è¿å•å·" not in cols_out:
            cols_out = ["è¿å•å·"] + cols_out

        new_rows = []
        for wb in new_ids:
            row_dict = {c: "" for c in header}
            row_dict["è¿å•å·"] = wb
            for c in MANAGED_COLS:
                if c == "è¿å•å·" or c not in header:
                    continue
                if c in idx_delta.columns:
                    v = idx_delta.loc[wb, c]
                    if _is_effective(v):
                        row_dict[c] = _to_jsonable_cell(v)
            new_rows.append([row_dict.get(c, "") for c in header])

        if new_rows:
            ws.append_rows(new_rows, value_input_option="USER_ENTERED")

    return True

# ========= UI =========
st.title("ğŸšš å‘è´§è°ƒåº¦")

st.markdown("""
    <style>
    div.stButton > button[kind="secondary"] {
        font-size: 28px !important;
        font-weight: 900 !important;
        padding: 0.8em 1.6em !important;
        background-color: #ff9800 !important;
        color: white !important;
        border-radius: 10px !important;
        border: 3px solid #e65100 !important;
    }
    </style>
""", unsafe_allow_html=True)

tab1, tab2 = st.tabs(["æŒ‰æ‰˜ç›˜å‘è´§","æŒ‰å¡è½¦å›å¡«åˆ°ä»“æ—¥æœŸ"])

with tab1:
    c1,_ = st.columns([1,6])
    with c1:
        if st.button("ğŸ”„ åˆ·æ–°æ•°æ®", key="btn_refresh_all"):
            for k in ["pallet_detail", "ship_tracking", "arrivals", "bol_detail", "wb_summary"]:
                _bust(k)
            for k in ["sel_locked", "locked_df", "_last_upload_pallets", "_last_upload_truck", "_last_upload_at", "all_snapshot_df", "_track_override"]:
                if k in st.session_state:
                    del st.session_state[k]
            st.rerun()

    arrivals_df = load_arrivals_df(_bust=_get_bust("arrivals"))
    bol_df      = load_bol_waybill_costs(_bust=_get_bust("bol_detail"))
    pallet_df   = load_pallet_detail_df(
        arrivals_df=arrivals_df,
        bol_cost_df=bol_df,
        _bust=_get_bust("pallet_detail"),
        refresh_token=(
            _get_bust("pallet_detail")
            + _get_bust("ship_tracking")
            + _get_bust("arrivals")
            + _get_bust("bol_detail")
        )
    )

    if pallet_df.empty:
        st.warning("æœªä»ã€æ‰˜ç›˜æ˜ç»†è¡¨ã€è¯»å–åˆ°æ•°æ®ï¼Œè¯·æ£€æŸ¥è¡¨å/æƒé™/è¡¨å¤´ã€‚")
        st.stop()

    ship_track_sig = _sheet_row_sig(SHEET_SHIP_TRACKING, _bust=_get_bust("ship_tracking"))
    shipped_pallets_raw = load_shipped_pallet_ids(
        _bust=_get_bust("ship_tracking"),
        sheet_sig=ship_track_sig
    )
    shipped_pallets_norm = {str(x).strip().upper() for x in shipped_pallets_raw}

    pallet_df["æ‰˜ç›˜å·_norm"] = pallet_df["æ‰˜ç›˜å·"].astype(str).str.strip().str.upper()
    pallet_df = pallet_df[~pallet_df["æ‰˜ç›˜å·_norm"].isin(shipped_pallets_norm)]

    if pallet_df.empty:
        st.info("å½“å‰å¯å‘è´§çš„æ‰˜ç›˜ä¸ºç©ºï¼ˆå¯èƒ½éƒ½å·²è®°å½•åœ¨ã€å‘è´§è¿½è¸ªã€ï¼‰ã€‚")
        st.stop()

    pk_opts = ["ï¼ˆå…¨éƒ¨ï¼‰"] + sorted([x for x in pallet_df["è‡ªæä»“åº“(æŒ‰æ‰˜ç›˜)"].dropna().astype(str).unique() if x.strip()])
    pickup_pick = st.selectbox("é€‰æ‹©è‡ªæä»“åº“ï¼ˆå¯é€‰ï¼‰", options=pk_opts, key="pickup_pallet")
    if pickup_pick != "ï¼ˆå…¨éƒ¨ï¼‰":
        pallet_df = pallet_df[pallet_df["è‡ªæä»“åº“(æŒ‰æ‰˜ç›˜)"] == pickup_pick]   

    wh_opts = ["ï¼ˆå…¨éƒ¨ï¼‰"] + sorted([w for w in pallet_df["ä»“åº“ä»£ç "].dropna().unique() if str(w).strip()])
    wh_pick = st.selectbox("é€‰æ‹©ä»“åº“ä»£ç ï¼ˆå¯é€‰ï¼‰", options=wh_opts, key="wh_pallet")
    if wh_pick != "ï¼ˆå…¨éƒ¨ï¼‰":
        pallet_df = pallet_df[pallet_df["ä»“åº“ä»£ç "]==wh_pick]

    show_cols = [
        "æ‰˜ç›˜å·","ä»“åº“ä»£ç ","è‡ªæä»“åº“(æŒ‰æ‰˜ç›˜)","æ‰˜ç›˜é‡é‡","é•¿(in)","å®½(in)","é«˜(in)","æ‰˜ç›˜ä½“ç§¯",
        "æ‰˜ç›˜åˆ›å»ºæ—¥æœŸ","æ‰˜ç›˜åˆ›å»ºæ—¶é—´",
        "è¿å•æ•°é‡","è¿å•æ¸…å•","è¿å•ç®±æ•°","è‡ªæä»“åº“(æŒ‰è¿å•)",
        "å¯¹å®¢æ‰¿è¯ºé€ä»“æ—¶é—´","é€ä»“æ—¶æ®µå·®å€¼(å¤©)",
        "ETA/ATA(æŒ‰è¿å•)","ETD/ATD(æŒ‰è¿å•)"
    ]
    for c in show_cols:
        if c not in pallet_df.columns:
            pallet_df[c] = ""

    disp_df = pallet_df.copy().reset_index(drop=True)
    for c in ["æ‰˜ç›˜ä½“ç§¯","æ‰˜ç›˜é‡é‡","é•¿(in)","å®½(in)","é«˜(in)"]:
        disp_df[c] = pd.to_numeric(disp_df.get(c, pd.Series()), errors="coerce")
    disp_df["æ‰˜ç›˜ä½“ç§¯"] = disp_df["æ‰˜ç›˜ä½“ç§¯"].round(2)
    disp_df["é•¿(in)"] = disp_df["é•¿(in)"].round(2)
    disp_df["å®½(in)"] = disp_df["å®½(in)"].round(2)
    disp_df["é«˜(in)"] = disp_df["é«˜(in)"].round(2)

    if "é€‰æ‹©" not in disp_df.columns:
        disp_df["é€‰æ‹©"] = False
    cols_order = ["é€‰æ‹©"] + show_cols

    if "sel_locked" not in st.session_state:
        st.session_state.sel_locked = False
    if "locked_df" not in st.session_state:
        st.session_state.locked_df = pd.DataFrame()

    if not st.session_state.sel_locked:
        with st.form("pick_pallets_form", clear_on_submit=False):
            edited_pal = st.data_editor(
                disp_df[cols_order],
                hide_index=True,
                use_container_width=True,
                height=500,
                column_config={"é€‰æ‹©": st.column_config.CheckboxColumn("é€‰æ‹©")},
                disabled=[c for c in show_cols],
                key="pallet_select_editor"
            )
            submitted = st.form_submit_button("ğŸ”’ é”å®šé€‰æ‹©å¹¶è¿›å…¥è®¡ç®—")
        if submitted:
            selected_pal = edited_pal[edited_pal["é€‰æ‹©"]==True].copy()
            if len(selected_pal) == 0:
                st.warning("è¯·è‡³å°‘å‹¾é€‰ä¸€ä¸ªæ‰˜ç›˜å†ç‚¹å‡»ã€é”å®šé€‰æ‹©å¹¶è¿›å…¥è®¡ç®—ã€ã€‚")
                st.stop()
            st.session_state.locked_df = selected_pal.reset_index(drop=True)
            st.session_state.all_snapshot_df = disp_df[cols_order].copy()
            st.session_state.sel_locked = True
            st.rerun()

    if st.session_state.sel_locked:
        st.success("âœ… å·²é”å®šæ‰˜ç›˜é€‰æ‹©")
        if st.button("ğŸ”“ é‡æ–°é€‰æ‹©"):
            st.session_state.sel_locked = False
            st.session_state.locked_df = pd.DataFrame()
            st.rerun()

        selected_pal = st.session_state.locked_df.copy()
        locked_ids = set(selected_pal["æ‰˜ç›˜å·"].astype(str))
        others_df = disp_df[~disp_df["æ‰˜ç›˜å·"].astype(str).isin(locked_ids)].copy()
        if "é€‰æ‹©" in others_df.columns:
            others_df["é€‰æ‹©"] = False

        left, right = st.columns([2, 2], gap="large")
        with left:
            st.markdown("**ğŸ“¦ å·²é”å®šæ‰˜ç›˜ï¼ˆç”¨äºè®¡ç®—ï¼‰**")
            st.dataframe(
                selected_pal[cols_order],
                use_container_width=True,
                height=320
            )
            st.caption(f"å·²é”å®šæ•°é‡ï¼š{len(selected_pal)}")
        with right:
            st.markdown("**ğŸ—‚ å…¶ä»–æ‰˜ç›˜ï¼ˆæœªé”å®šï¼Œä»…æŸ¥çœ‹ï¼‰**")
            with st.expander("å±•å¼€æŸ¥çœ‹æœªé”å®šæ‰˜ç›˜ï¼ˆç‚¹å‡»å±•å¼€/æŠ˜å ï¼‰", expanded=False):
                st.dataframe(
                    others_df[cols_order],
                    use_container_width=True,
                    height=320
                )
                st.caption(f"æœªé”å®šæ•°é‡ï¼š{len(others_df)}")

        sel_count = int(len(selected_pal))
        sel_vol_sum = pd.to_numeric(selected_pal.get("æ‰˜ç›˜ä½“ç§¯", pd.Series()), errors="coerce").sum()
        m1, m2 = st.columns(2)
        with m1: st.metric("å·²é€‰æ‹©æ‰˜ç›˜æ•°", sel_count)
        with m2: st.metric("é€‰ä¸­ä½“ç§¯åˆè®¡ï¼ˆCBMï¼‰", round(float(sel_vol_sum or 0.0), 2))

        if sel_count == 0:
            st.info("å½“å‰æ²¡æœ‰é”å®šçš„æ‰˜ç›˜ã€‚ç‚¹å‡»ã€é‡æ–°é€‰æ‹©ã€è¿”å›ã€‚")
            st.stop()

        st.subheader("ğŸ§¾ è½¦æ¬¡ä¿¡æ¯ï¼ˆæ‰˜ç›˜ç»´åº¦åˆ†æ‘Šï¼‰")
        cc1, cc2, cc3 = st.columns([2,2,2])
        with cc1:
            pallet_truck_no = st.text_input("å¡è½¦å•å·ï¼ˆå¿…å¡«ï¼‰", key="pallet_truck_no")
        with cc2:
            pallet_total_cost = st.number_input("æœ¬è½¦æ€»è´¹ç”¨ï¼ˆå¿…å¡«ï¼‰", min_value=0.0, step=1.0, format="%.2f", key="pallet_total_cost")
        with cc3:
            ship_date_input = st.date_input("å‘è´§æ—¥æœŸï¼ˆé»˜è®¤ä»Šå¤©ï¼‰", value=date.today(), key="pallet_ship_date")

        if not pallet_truck_no or pallet_total_cost <= 0:
            st.info("è¯·å¡«å†™å¡è½¦å•å·ä¸æœ¬è½¦æ€»è´¹ç”¨ã€‚")
            st.stop()

        selected_pal["æ‰˜ç›˜é‡é‡"] = pd.to_numeric(selected_pal["æ‰˜ç›˜é‡é‡"], errors="coerce")
        weights = selected_pal["æ‰˜ç›˜é‡é‡"]
        if weights.isna().any() or (weights.dropna() <= 0).any():
            st.error("æ‰€é€‰æ‰˜ç›˜å­˜åœ¨ç¼ºå¤±æˆ–éæ­£çš„ã€æ‰˜ç›˜é‡é‡ã€ï¼Œæ— æ³•åˆ†æ‘Šã€‚è¯·å…ˆåœ¨ã€æ‰˜ç›˜æ˜ç»†è¡¨ã€ä¿®æ­£ã€‚")
            st.stop()

        wt_sum = float(weights.sum())
        if wt_sum <= 0:
            st.error("æ€»æ‰˜ç›˜é‡é‡ä¸º 0ï¼Œæ— æ³•åˆ†æ‘Šã€‚")
            st.stop()

        selected_pal["åˆ†æ‘Šæ¯”ä¾‹"] = weights / wt_sum
        selected_pal["åˆ†æ‘Šè´¹ç”¨_raw"] = selected_pal["åˆ†æ‘Šæ¯”ä¾‹"] * float(pallet_total_cost)
        selected_pal["åˆ†æ‘Šè´¹ç”¨"] = selected_pal["åˆ†æ‘Šè´¹ç”¨_raw"].round(2)
        diff_cost = round(float(pallet_total_cost) - selected_pal["åˆ†æ‘Šè´¹ç”¨"].sum(), 2)

        if abs(diff_cost) >= 0.01:
            idx = selected_pal["åˆ†æ‘Šè´¹ç”¨"].idxmax()
            selected_pal.loc[idx, "åˆ†æ‘Šè´¹ç”¨"] = round(
                selected_pal.loc[idx, "åˆ†æ‘Šè´¹ç”¨"] + diff_cost, 2
            )

        upload_df = selected_pal.copy()
        upload_df["å¡è½¦å•å·"] = pallet_truck_no
        upload_df["æ€»è´¹ç”¨"] = round(float(pallet_total_cost), 2)
        upload_df["åˆ†æ‘Šæ¯”ä¾‹"] = (upload_df["åˆ†æ‘Šæ¯”ä¾‹"]*100).round(2).astype(str) + "%"
        upload_df["åˆ†æ‘Šè´¹ç”¨"] = upload_df["åˆ†æ‘Šè´¹ç”¨"].map(lambda x: f"{x:.2f}")
        upload_df["æ€»è´¹ç”¨"] = upload_df["æ€»è´¹ç”¨"].map(lambda x: f"{x:.2f}")
        upload_df["æ‰˜ç›˜ä½“ç§¯"] = pd.to_numeric(upload_df.get("æ‰˜ç›˜ä½“ç§¯", pd.Series()), errors="coerce").round(2)
        upload_df["ä¸Šä¼ å‘è´§æ—¥æœŸï¼ˆé¢„è§ˆï¼‰"] = ship_date_input.strftime("%Y-%m-%d")

        preview_cols_pal = [
            "å¡è½¦å•å·","ä¸Šä¼ å‘è´§æ—¥æœŸï¼ˆé¢„è§ˆï¼‰","ä»“åº“ä»£ç ","æ‰˜ç›˜å·","æ‰˜ç›˜é‡é‡","é•¿(in)","å®½(in)","é«˜(in)","æ‰˜ç›˜ä½“ç§¯",
            "æ‰˜ç›˜åˆ›å»ºæ—¥æœŸ","æ‰˜ç›˜åˆ›å»ºæ—¶é—´",
            "è¿å•æ•°é‡","è¿å•æ¸…å•",
            "å¯¹å®¢æ‰¿è¯ºé€ä»“æ—¶é—´","é€ä»“æ—¶æ®µå·®å€¼(å¤©)",
            "ETA/ATA(æŒ‰è¿å•)","ETD/ATD(æŒ‰è¿å•)",
            "åˆ†æ‘Šæ¯”ä¾‹","åˆ†æ‘Šè´¹ç”¨","æ€»è´¹ç”¨"
        ]
        for c in preview_cols_pal:
            if c not in upload_df.columns:
                upload_df[c] = ""

        st.subheader("âœ… ä¸Šä¼ é¢„è§ˆï¼ˆæ‰˜ç›˜ â†’ å‘è´§è¿½è¸ªï¼‰")
        st.dataframe(upload_df[preview_cols_pal], use_container_width=True, height=360)

        st.markdown("""
        **åˆ†æ‘Šæ¯”ä¾‹** = æ‰˜ç›˜é‡é‡ Ã· æ‰€é€‰æ‰˜ç›˜æ€»é‡é‡  
        **åˆ†æ‘Šè´¹ç”¨** = åˆ†æ‘Šæ¯”ä¾‹ Ã— æœ¬è½¦æ€»è´¹ç”¨ï¼ˆè‡ªåŠ¨ç”¨æœ€å¤§é¡¹å¸æ”¶å‡ åˆ†é’±å·®é¢ï¼‰
        """)

        if st.button("ğŸ“¤ ä¸Šä¼ åˆ°ã€å‘è´§è¿½è¸ªã€", key="btn_upload_pallet_upload_only"):
            try:
                ss = client.open(SHEET_SHIP_TRACKING); ws_track = ss.sheet1
            except SpreadsheetNotFound:
                st.error(f"æ‰¾ä¸åˆ°å·¥ä½œè¡¨ã€Œ{SHEET_SHIP_TRACKING}ã€ã€‚è¯·å…ˆåœ¨ Google Drive ä¸­åˆ›å»ºï¼Œå¹¶è®¾ç½®ç¬¬ä¸€è¡Œè¡¨å¤´ã€‚")
                st.stop()

            exist = _safe_get_all_values(ws_track)
            if not exist:
                st.error("ç›®æ ‡è¡¨ä¸ºç©ºä¸”æ— è¡¨å¤´ã€‚è¯·å…ˆåœ¨ç¬¬ä¸€è¡Œå†™å¥½è¡¨å¤´ï¼ˆæ ‡é¢˜è¡Œï¼‰ã€‚")
                st.stop()

            header_raw = exist[0]
            header_norm = _norm_header(header_raw)
            header_norm_lower = [h.lower() for h in header_norm]
            need_ok = any(n in header_norm for n in ["æ‰˜ç›˜å·","æ‰˜ç›˜ç¼–å·"]) or \
                    any(n in header_norm_lower for n in ["palletid","palletno","palletç¼–å·"])
            if not need_ok:
                st.error("ã€å‘è´§è¿½è¸ªã€ç¼ºå°‘â€œæ‰˜ç›˜å·â€åˆ—ï¼ˆæˆ–ç­‰ä»·åˆ—å¦‚ PalletID/PalletNoï¼‰ã€‚è¯·å…ˆåœ¨ç›®æ ‡è¡¨å¢åŠ è¯¥åˆ—ã€‚")
                st.stop()

            tmp = upload_df.copy()
            _ship_date_str = ship_date_input.strftime("%Y-%m-%d")

            _date_header_candidates = ["æ—¥æœŸ", "å‘è´§æ—¥æœŸ", "å‡ºä»“æ—¥æœŸ", "Date", "ShipDate"]
            date_col_to_use = None
            for cand in _date_header_candidates:
                if cand in header_raw:
                    date_col_to_use = cand
                    break
            if date_col_to_use is not None:
                tmp[date_col_to_use] = _ship_date_str

            _pickup_header_candidates = ["è‡ªæä»“åº“", "è‡ªæä»“", "Pickup", "pickup"]
            pickup_col_to_use = None
            for cand in _pickup_header_candidates:
                if cand in header_raw:
                    pickup_col_to_use = cand
                    break
            if pickup_col_to_use is not None:
                tmp[pickup_col_to_use] = upload_df.get("è‡ªæä»“åº“(æŒ‰æ‰˜ç›˜)", "").fillna("")

            def _norm_hdr(s: str) -> str:
                return str(s).replace("\u00A0"," ").replace("\n","").replace(" ","").strip().lower()
            _pid_candidates = ["æ‰˜ç›˜å·","æ‰˜ç›˜ç¼–å·","æ‰˜ç›˜id","æ‰˜ç›˜#",
                               "PalletID","PalletNo","palletid","palletno","palletç¼–å·","pallet#","pallet"]
            cand_norm_set = {_norm_hdr(x) for x in _pid_candidates}

            pid_col_to_use = None
            for h in header_raw:
                if _norm_hdr(h) in cand_norm_set:
                    pid_col_to_use = h
                    break
            if pid_col_to_use is None:
                st.error("ã€å‘è´§è¿½è¸ªã€ç¼ºå°‘â€œæ‰˜ç›˜å·â€åˆ—ï¼ˆæˆ–ç­‰ä»·åˆ—ï¼‰ã€‚è¯·å…ˆåœ¨ç›®æ ‡è¡¨å¢åŠ è¯¥åˆ—ã€‚")
                st.stop()

            tmp[pid_col_to_use] = upload_df["æ‰˜ç›˜å·"].astype(str).str.strip()

            for col in header_raw:
                if col not in tmp.columns:
                    tmp[col] = ""
            rows = tmp.reindex(columns=header_raw).fillna("").values.tolist()

            ws_track.append_rows(rows, value_input_option="USER_ENTERED")
            st.success(f"âœ… å·²ä¸Šä¼  {len(rows)} æ¡åˆ°ã€{SHEET_SHIP_TRACKING}ã€ã€‚å¡è½¦å•å·ï¼š{pallet_truck_no}")

            _bust("ship_tracking")
            _ = load_ship_tracking_raw(_bust=_get_bust("ship_tracking"))

            st.session_state["_last_upload_pallets"] = set(upload_df["æ‰˜ç›˜å·"].astype(str).str.strip())
            st.session_state["_last_upload_truck"] = str(pallet_truck_no).strip()
            st.session_state["_last_upload_at"] = datetime.now()

            override = upload_df[[
                "æ‰˜ç›˜å·","è¿å•æ¸…å•","è‡ªæä»“åº“(æŒ‰æ‰˜ç›˜)","åˆ†æ‘Šè´¹ç”¨","ä¸Šä¼ å‘è´§æ—¥æœŸï¼ˆé¢„è§ˆï¼‰","å¡è½¦å•å·"
            ]].copy()
            override = override.rename(columns={"ä¸Šä¼ å‘è´§æ—¥æœŸï¼ˆé¢„è§ˆï¼‰": "æ—¥æœŸ"})
            def _to_float_safe(v):
                try:
                    return float(str(v).strip())
                except Exception:
                    return None
            override["æ‰˜ç›˜å·"]   = override["æ‰˜ç›˜å·"].astype(str).str.strip()
            override["å¡è½¦å•å·"] = override["å¡è½¦å•å·"].astype(str).str.strip()
            override["åˆ†æ‘Šè´¹ç”¨"] = override["åˆ†æ‘Šè´¹ç”¨"].map(_to_float_safe)
            override["æ—¥æœŸ"] = pd.to_datetime(override["æ—¥æœŸ"], errors="coerce").dt.strftime("%Y-%m-%d")
            override["è‡ªæä»“åº“(æŒ‰æ‰˜ç›˜)"] = override["è‡ªæä»“åº“(æŒ‰æ‰˜ç›˜)"].astype(str).str.strip()
            st.session_state["_track_override"] = override

            st.info("ä¸‹ä¸€æ­¥ï¼šç‚¹å‡»ä¸‹æ–¹â€œğŸ” æ›´æ–°åˆ°ã€è¿å•å…¨é“¾è·¯æ±‡æ€»ã€â€ã€‚")

        disable_b = not bool(st.session_state.get("_last_upload_pallets"))
        if st.button("ğŸ” æ›´æ–°åˆ°ã€è¿å•å…¨é“¾è·¯æ±‡æ€»ã€", key="btn_update_wb_summary", disabled=disable_b):
            needed_pids = st.session_state.get("_last_upload_pallets", set())

            def _wait_visibility(max_wait_s=6.0, poll_every=0.6) -> bool:
                start = time.time()
                while True:
                    track_now = load_ship_tracking_raw(_bust=_get_bust("ship_tracking"))
                    if not track_now.empty:
                        seen_pids = set(track_now.get("æ‰˜ç›˜å·","").astype(str).str.strip())
                        if needed_pids & seen_pids:
                            return True
                    if time.time() - start > max_wait_s:
                        return False
                    time.sleep(poll_every)

            visible = _wait_visibility()
            if not visible:
                st.info("æç¤ºï¼šè¿œç«¯å¯èƒ½å­˜åœ¨çŸ­æš‚ä¸€è‡´æ€§å»¶è¿Ÿï¼Œå·²ç»§ç»­å°è¯•åŒæ­¥â€¦")

            try:
                df_delta = build_waybill_delta(track_override=st.session_state.get("_track_override"))
            except Exception as e:
                st.error(f"æ„å»ºå¢é‡å¤±è´¥ï¼š{e}")
                st.stop()

            if df_delta.empty:
                time.sleep(1.2)
                _bust("ship_tracking")
                _ = load_ship_tracking_raw(_bust=_get_bust("ship_tracking"))
                try:
                    df_delta = build_waybill_delta()
                except Exception as e:
                    st.error(f"äºŒæ¬¡æ„å»ºå¢é‡å¤±è´¥ï¼š{e}")
                    st.stop()

            if df_delta.empty:
                st.warning("æ²¡æœ‰å¯æ›´æ–°çš„è¿å•ï¼šå¯èƒ½ä»åœ¨è¿œç«¯å»¶è¿Ÿï¼Œæˆ–æœ¬æ¬¡ä¸Šä¼ æœªåŒ…å«å¯è§£æçš„è¿å•å·ã€‚ç¨åå†è¯•æˆ–åˆ·æ–°ç¼“å­˜ã€‚")
            else:
                try:
                    ok = upsert_waybill_summary_partial(df_delta)
                    if ok:
                        if "_track_override" in st.session_state:
                            del st.session_state["_track_override"]
                        st.success("âœ… å·²æ›´æ–°åˆ°ã€è¿å•å…¨é“¾è·¯æ±‡æ€»ã€")
                except Exception as e:
                    st.error(f"å†™å…¥ã€è¿å•å…¨é“¾è·¯æ±‡æ€»ã€å¤±è´¥ï¼š{e}")
                    st.stop()

                if ok:
                    st.session_state["_wb_updated_at"] = time.time()
                    _bust("wb_summary")
                    _ = load_waybill_summary_df(_bust=_get_bust("wb_summary"))
                    st.success(f"âœ… å·²æ›´æ–°/æ–°å¢ {len(df_delta)} æ¡åˆ°ã€{SHEET_WB_SUMMARY}ã€ã€‚")
                    st.rerun()
                else:
                    st.warning("æœªèƒ½å†™å…¥ã€è¿å•å…¨é“¾è·¯æ±‡æ€»ã€ï¼šè¯·æ£€æŸ¥è¡¨å¤´ï¼ˆéœ€åŒ…å«â€œè¿å•å·â€ï¼‰æˆ–æƒé™ã€‚")

with tab2:
    if st.session_state.get("_wb_updated_at"):
        if time.time() - float(st.session_state["_wb_updated_at"]) < 30:
            _bust("wb_summary")
            _ = load_waybill_summary_df(_bust=_get_bust("wb_summary"))
        del st.session_state["_wb_updated_at"]

    st.subheader("ğŸšš æŒ‰å¡è½¦å›å¡«åˆ°ä»“æ—¥æœŸ")

    df_sum, ws_sum, header_raw = load_waybill_summary_df(_bust=_get_bust("wb_summary"))

    if ws_sum is None:
        st.info("æœªæ‰¾åˆ°ã€è¿å•å…¨é“¾è·¯æ±‡æ€»ã€è¡¨ã€‚è¯·å…ˆåˆ›å»ºè¯¥è¡¨ï¼ˆè‡³å°‘åŒ…å«è¡¨å¤´ã€è¿å•å·ã€ï¼‰ã€‚")
    elif df_sum.empty:
        st.info("ã€è¿å•å…¨é“¾è·¯æ±‡æ€»ã€å½“å‰ä¸ºç©ºã€‚è¯·å…ˆåœ¨ã€æŒ‰æ‰˜ç›˜å‘è´§ã€ä¸Šä¼ æ•°æ®åï¼Œå†å›åˆ°æ­¤å¤„å›å¡«ã€‚")
    else:
        st.subheader("ç­›é€‰æ¡ä»¶")

        wh_all = sorted([w for w in df_sum["ä»“åº“ä»£ç "].astype(str).unique() if w.strip()])
        wh_pick = st.multiselect("ä»“åº“ä»£ç ï¼ˆå…ˆé€‰è¿™é‡Œï¼‰", options=wh_all, placeholder="é€‰æ‹©ä¸€ä¸ªæˆ–å¤šä¸ªä»“åº“â€¦")

        if wh_pick:
            df_wh = df_sum[df_sum["ä»“åº“ä»£ç "].isin(wh_pick)].copy()
        else:
            df_wh = df_sum.copy()

        truck_opts = sorted([t for t in df_wh["å‘èµ°å¡è½¦å·"].astype(str).unique() if t.strip()])
        if truck_opts:
            trucks_pick = st.multiselect(
                "å¡è½¦å•å·ï¼ˆä»æ‰€é€‰ä»“åº“æ´¾ç”Ÿï¼‰",
                options=truck_opts,
                placeholder="é€‰æ‹©è¦æ‰¹é‡å›å¡«çš„è½¦æ¬¡â€¦"
            )
        else:
            st.info("å½“å‰ä»“åº“ä¸‹æ²¡æœ‰å¯é€‰çš„å¡è½¦å•å·ã€‚")
            trucks_pick = []

        df_for_dates = df_wh.copy()
        if trucks_pick:
            df_for_dates = df_for_dates[df_for_dates["å‘èµ°å¡è½¦å·"].astype(str).isin(trucks_pick)]

        valid_ship_dates = df_for_dates.loc[df_for_dates["_å‘èµ°æ—¥æœŸ_dt"].notna(), "_å‘èµ°æ—¥æœŸ_dt"]
        if not valid_ship_dates.empty:
            dmin, dmax = valid_ship_dates.min(), valid_ship_dates.max()
            default_start = dmin
            default_end = dmax if dmax >= dmin else dmin
            r1, r2 = st.date_input(
                "æŒ‰å‘èµ°æ—¥æœŸç­›é€‰èŒƒå›´",
                value=(default_start, default_end),
                min_value=dmin, max_value=max(dmax, dmin)
            )
        else:
            r1 = r2 = None
            st.caption("æœªæ£€ç´¢åˆ°å¯ç”¨çš„ã€å‘èµ°æ—¥æœŸã€èŒƒå›´ï¼ˆæ‰€é€‰æ¡ä»¶å¯èƒ½æ²¡æœ‰æ—¥æœŸæ•°æ®ï¼‰ã€‚")

        only_blank = st.checkbox("ä»…å¡«ç©ºç™½åˆ°ä»“æ—¥æœŸ", value=True)

        filt = pd.Series(True, index=df_sum.index)
        if wh_pick:
            filt &= df_sum["ä»“åº“ä»£ç "].isin(wh_pick)
        if trucks_pick:
            filt &= df_sum["_åˆ°ä»“æ—¥æœŸ_dt"].index.isin(df_sum[df_sum["å‘èµ°å¡è½¦å·"].astype(str).isin(trucks_pick)].index)
            filt &= df_sum["å‘èµ°å¡è½¦å·"].astype(str).isin(trucks_pick)
        if r1 and r2:
            filt &= df_sum["_å‘èµ°æ—¥æœŸ_dt"].between(r1, r2)
        if only_blank:
            filt &= df_sum["_åˆ°ä»“æ—¥æœŸ_dt"].isna()

        df_target = df_sum.loc[filt].copy()

        st.markdown(f"**åŒ¹é…åˆ° {len(df_target)} æ¡è¿å•**")
        st.dataframe(
            df_target[["è¿å•å·","ä»“åº“ä»£ç ","å‘èµ°å¡è½¦å·","å‘èµ°æ—¥æœŸ","åˆ°ä»“æ—¥æœŸ"]]
                .sort_values(["ä»“åº“ä»£ç ","å‘èµ°å¡è½¦å·","è¿å•å·"]),
            use_container_width=True, height=360
        )

        st.divider()

        today = date.today()
        fill_date = st.date_input("å¡«å……åˆ°ä»“æ—¥æœŸ", value=today)

        def _write_arrival_date(rows_idx, date_to_fill: date):
            col_idx_1based = None
            for i, h in enumerate(header_raw):
                hn = str(h).replace(" ", "")
                if hn in ["åˆ°ä»“æ—¥æœŸ", "åˆ°ä»“æ—¥", "åˆ°ä»“(wh)"]:
                    col_idx_1based = i + 1
                    break
            if col_idx_1based is None:
                st.error("ç›®æ ‡è¡¨ç¼ºå°‘ã€åˆ°ä»“æ—¥æœŸã€åˆ—ã€‚è¯·å…ˆåœ¨è¡¨å¤´æ–°å¢è¯¥åˆ—åé‡è¯•ã€‚")
                return False
            if not rows_idx:
                return True
            try:
                rows = sorted(int(r) for r in rows_idx if r is not None)
            except Exception:
                st.error("è¡Œå·åˆ—è¡¨æ ¼å¼å¼‚å¸¸ã€‚")
                return False
            if not rows:
                return True

            ranges = []
            s = p = rows[0]
            for r in rows[1:]:
                if r == p + 1:
                    p = r
                else:
                    ranges.append((s, p))
                    s = p = r
            ranges.append((s, p))

            sheet_title = ws_sum.title
            spreadsheet_id = ws_sum.spreadsheet.id
            date_str = date_to_fill.strftime("%Y-%m-%d")

            def _mk_update_for_segment(r1, r2):
                a1_start = gspread.utils.rowcol_to_a1(r1, col_idx_1based)
                a1_end   = gspread.utils.rowcol_to_a1(r2, col_idx_1based)
                num_rows = r2 - r1 + 1
                return {
                    "range": f"{sheet_title}!{a1_start}:{a1_end}",
                    "values": [[date_str] for _ in range(num_rows)]
                }

            updates = [_mk_update_for_segment(r1, r2) for (r1, r2) in ranges]

            try:
                batch_size = 500
                for i in range(0, len(updates), batch_size):
                    sub = updates[i:i + batch_size]
                    body = {"valueInputOption": "USER_ENTERED", "data": sub}
                    sheets_service.spreadsheets().values().batchUpdate(
                        spreadsheetId=spreadsheet_id,
                        body=body
                    ).execute()
                return True
            except HttpError as e:
                st.error(f"å†™å…¥å¤±è´¥ï¼ˆHTTPï¼‰ï¼š{e}")
                return False
            except Exception as e:
                st.error(f"å†™å…¥å¤±è´¥ï¼š{e}")
                return False

        left, right = st.columns([1,1])
        with left:
            st.caption("æç¤ºï¼šå…ˆé€‰ä»“åº“ï¼Œå†é€‰å¡è½¦ï¼›å¯æŒ‰å‘èµ°æ—¥æœŸèŒƒå›´è¿‡æ»¤ï¼›å‹¾é€‰â€œä»…å¡«ç©ºç™½â€é¿å…è¦†ç›–å·²æœ‰å€¼ã€‚")
        with right:
            if st.button("ğŸ“ å†™å…¥åˆ°ä»“æ—¥æœŸ", key="btn_fill_arrival_date"):
                if df_target.empty:
                    st.warning("ç­›é€‰ç»“æœä¸ºç©ºï¼›è¯·è°ƒæ•´ä»“åº“/å¡è½¦/æ—¥æœŸæ¡ä»¶ã€‚")
                else:
                    ok = _write_arrival_date(df_target["_rowno"].tolist(), fill_date)
                    if ok:
                        st.success(f"å·²æ›´æ–° {len(df_target)} è¡Œçš„ã€åˆ°ä»“æ—¥æœŸã€ä¸º {fill_date.strftime('%Y-%m-%d')}ã€‚")
                        _bust("wb_summary")
                        st.rerun()
